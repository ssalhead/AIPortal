# AI Ìè¨ÌÉà ÏïÑÌÇ§ÌÖçÏ≤ò ÏÑ§Í≥ÑÏÑú
## ÏãúÏä§ÌÖú ÏïÑÌÇ§ÌÖçÏ≤ò Î∞è Íµ¨ÌòÑ Í∞ÄÏù¥ÎìúÎùºÏù∏

---

## üìä ÏãúÏä§ÌÖú Í∞úÏöî

### ÏïÑÌÇ§ÌÖçÏ≤ò ÏÑ§Í≥Ñ ÏõêÏπô
- **Í≤ÄÏ¶ùÎêú ÏïàÏ†ïÏÑ±**: SYGenai Ïö¥ÏòÅ ÌôòÍ≤ΩÏóêÏÑú Í≤ÄÏ¶ùÎêú Ìå®ÌÑ¥ ÌôúÏö©
- **ÌôïÏû• Í∞ÄÎä•ÏÑ±**: Î™®ÎìàÌôîÎêú ÏÑ§Í≥ÑÎ°ú ÏÉàÎ°úÏö¥ Í∏∞Îä• Ï∂îÍ∞Ä Ïö©Ïù¥
- **ÏÑ±Îä• ÏµúÏ†ÅÌôî**: Redis ÏóÜÏù¥ÎèÑ Í≥†ÏÑ±Îä•ÏùÑ Î≥¥Ïû•ÌïòÎäî Ï∫êÏã± Ï†ÑÎûµ
- **ÏÇ¨Ïö©Ïûê Ï§ëÏã¨**: ÏßÅÍ¥ÄÏ†ÅÏù¥Í≥† Î∞òÏùëÏÑ± ÏûàÎäî ÏÇ¨Ïö©Ïûê Í≤ΩÌóò

### Í∏∞Ïà† Ïä§ÌÉù Í∞úÏöî
```
‚îå‚îÄ Frontend ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ React 18+ ‚Ä¢ TypeScript ‚Ä¢ TailwindCSS ‚Ä¢ Zustand ‚Ä¢ SWR      ‚îÇ
‚îú‚îÄ API Gateway ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ FastAPI ‚Ä¢ Pydantic ‚Ä¢ Uvicorn ‚Ä¢ WebSocket                  ‚îÇ
‚îú‚îÄ AI Agents ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ LangGraph ‚Ä¢ Claude (Bedrock) ‚Ä¢ Gemini (GenerativeAI)      ‚îÇ
‚îú‚îÄ Data Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ PostgreSQL ‚Ä¢ DynamoDB ‚Ä¢ OpenSearch ‚Ä¢ S3                   ‚îÇ
‚îî‚îÄ Infrastructure ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ Docker ‚Ä¢ Kubernetes ‚Ä¢ GitHub Actions ‚Ä¢ AWS/GCP           ‚îÇ
```

---

## üèóÔ∏è ÏãúÏä§ÌÖú ÏïÑÌÇ§ÌÖçÏ≤ò

### Ï†ÑÏ≤¥ ÏãúÏä§ÌÖú Íµ¨Ï°∞
```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[React UI]
        WS[WebSocket Client]
    end
    
    subgraph "API Gateway Layer"
        API[FastAPI Server]
        WSS[WebSocket Server]
        AUTH[Auth Middleware]
    end
    
    subgraph "AI Agent Layer"
        SUP[Supervisor Agent]
        ROUTE[LLM Router]
        WSA[Web Search Agent]
        RAG[Multimodal RAG Agent]
        CAN[Canvas Agent]
        T1A[Tier1 Domain Agents]
    end
    
    subgraph "Data Layer"
        PG[(PostgreSQL)]
        DDB[(DynamoDB)]
        OS[(OpenSearch)]
        S3[(S3 Storage)]
        CACHE[Cache Manager]
    end
    
    subgraph "External Services"
        BEDROCK[AWS Bedrock]
        GENAI[GCP GenerativeAI]
        MCP[MCP Servers]
    end
    
    UI --> API
    WS --> WSS
    API --> AUTH
    AUTH --> SUP
    SUP --> ROUTE
    ROUTE --> WSA
    ROUTE --> RAG
    ROUTE --> CAN
    ROUTE --> T1A
    
    WSA --> BEDROCK
    RAG --> GENAI
    T1A --> MCP
    
    SUP --> CACHE
    CACHE --> PG
    API --> DDB
    RAG --> OS
    API --> S3
```

### Îç∞Ïù¥ÌÑ∞ ÌîåÎ°úÏö∞
```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant A as API Gateway
    participant S as Supervisor
    participant W as Worker Agent
    participant L as LLM
    participant D as Database
    
    U->>F: ÏÇ¨Ïö©Ïûê ÏûÖÎ†•
    F->>A: API Request
    A->>S: ÏóêÏù¥Ï†ÑÌä∏ ÏöîÏ≤≠
    S->>S: ÏùòÎèÑ Î∂ÑÏÑù
    S->>W: Worker ÏÑ†ÌÉù
    W->>L: LLM Ìò∏Ï∂ú
    L->>W: ÏùëÎãµ
    W->>D: Í≤∞Í≥º Ï†ÄÏû•
    W->>S: ÏûëÏóÖ ÏôÑÎ£å
    S->>A: ÏùëÎãµ Ï†ÑÎã¨
    A->>F: Ïä§Ìä∏Î¶¨Î∞ç ÏùëÎãµ
    F->>U: Í≤∞Í≥º ÌëúÏãú
```

---

## ü§ñ AI ÏóêÏù¥Ï†ÑÌä∏ ÏïÑÌÇ§ÌÖçÏ≤ò

### LangGraph Í∏∞Î∞ò ÏóêÏù¥Ï†ÑÌä∏ Íµ¨Ï°∞
```python
from langgraph.graph import StateGraph
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    current_agent: str
    context: dict
    user_profile: dict
    workspace_id: str
    artifacts: list

class SupervisorAgent:
    """Ï§ëÏïô Ï°∞Ï†ï ÏóêÏù¥Ï†ÑÌä∏ - ÏùòÎèÑ Î∂ÑÏÑù Î∞è Worker Ìï†Îãπ"""
    
    def __init__(self):
        self.llm_router = OptimizedLLMRouter()
        self.workers = {
            'web_search': WebSearchWorker(),
            'multimodal_rag': MultimodalRAGWorker(),
            'canvas': CanvasWorker(),
            'regulations': RegulationsWorker(),
            'legal': LegalWorker(),
            'accounting': AccountingWorker(),
            'tax': TaxWorker()
        }
        
        # LangGraph ÏÉÅÌÉú Í∑∏ÎûòÌîÑ Íµ¨ÏÑ±
        self.workflow = StateGraph(AgentState)
        self._build_workflow()
    
    def _build_workflow(self):
        """ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í∑∏ÎûòÌîÑ Íµ¨ÏÑ±"""
        # ÎÖ∏Îìú Ï∂îÍ∞Ä
        self.workflow.add_node("supervisor", self.supervisor_node)
        self.workflow.add_node("web_search", self.workers['web_search'].execute)
        self.workflow.add_node("multimodal_rag", self.workers['multimodal_rag'].execute)
        self.workflow.add_node("canvas", self.workers['canvas'].execute)
        
        # Tier1 ÎèÑÎ©îÏù∏ ÏóêÏù¥Ï†ÑÌä∏
        for agent_type in ['regulations', 'legal', 'accounting', 'tax']:
            self.workflow.add_node(agent_type, self.workers[agent_type].execute)
        
        # Ïó£ÏßÄ Î∞è Ï°∞Í±¥Î∂Ä ÎùºÏö∞ÌåÖ
        self.workflow.set_entry_point("supervisor")
        self.workflow.add_conditional_edges(
            "supervisor",
            self.route_to_worker,
            {
                "web_search": "web_search",
                "multimodal_rag": "multimodal_rag", 
                "canvas": "canvas",
                "regulations": "regulations",
                "legal": "legal",
                "accounting": "accounting",
                "tax": "tax"
            }
        )
        
        # Ï¢ÖÎ£å Ï°∞Í±¥
        for worker in self.workers.keys():
            self.workflow.add_edge(worker, "__end__")
    
    async def supervisor_node(self, state: AgentState) -> AgentState:
        """Supervisor ÏùòÏÇ¨Í≤∞Ï†ï ÎÖ∏Îìú"""
        last_message = state["messages"][-1]
        user_profile = state.get("user_profile", {})
        
        # ÏùòÎèÑ Î∂ÑÏÑù ÌîÑÎ°¨ÌîÑÌä∏
        analysis_prompt = f"""
        ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ: {last_message['content']}
        ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑ: {user_profile}
        
        Îã§Ïùå Ï§ë Í∞ÄÏû• Ï†ÅÌï©Ìïú ÏóêÏù¥Ï†ÑÌä∏Î•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:
        - web_search: Ïã§ÏãúÍ∞Ñ Ïõπ Í≤ÄÏÉâÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞
        - multimodal_rag: ÏóÖÎ°úÎìúÎêú ÌååÏùº Í∏∞Î∞ò ÏßàÏùòÏùëÎãµ
        - canvas: ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ÏóêÏÑú ÌòëÏóÖ ÏûëÏóÖ
        - regulations: Í∑úÏ†ï Î∞è Ïª¥ÌîåÎùºÏù¥Ïñ∏Ïä§ Î¨∏Ïùò
        - legal: Î≤ïÎ¨¥ Í¥ÄÎ†® Î¨∏Ïùò
        - accounting: ÌöåÍ≥Ñ Î∞è Ïû¨Î¨¥ Î∂ÑÏÑù
        - tax: ÏÑ∏Î¨¥ Ï≤òÎ¶¨ Î∞è Ïã†Í≥†
        """
        
        # LLMÏúºÎ°ú ÏùòÎèÑ Î∂ÑÏÑù
        model = await self.llm_router.get_optimal_model("intent_analysis", len(analysis_prompt))
        response = await model.ainvoke(analysis_prompt)
        
        selected_agent = self._parse_agent_selection(response.content)
        state["current_agent"] = selected_agent
        
        return state
    
    def route_to_worker(self, state: AgentState) -> str:
        """ÏõåÏª§ ÎùºÏö∞ÌåÖ Ìï®Ïàò"""
        return state["current_agent"]
```

### LLM ÎùºÏö∞ÌåÖ ÏãúÏä§ÌÖú
```python
from enum import Enum
from typing import Optional, Dict, Any
import asyncio
from anthropic import AsyncAnthropic
from google.generativeai import GenerativeModel
import google.generativeai as genai

class ModelType(Enum):
    CLAUDE_4_SONNET = "claude-4.0-sonnet"
    CLAUDE_37_SONNET = "claude-3.7-sonnet"
    CLAUDE_35_HAIKU = "claude-3.5-haiku"
    GEMINI_25_PRO = "gemini-2.5-pro"
    GEMINI_25_FLASH = "gemini-2.5-flash"
    GEMINI_20_FLASH = "gemini-2.0-flash"

class TaskType(Enum):
    COMPLEX_ANALYSIS = "complex_analysis"
    BALANCED_REASONING = "balanced_reasoning"
    QUICK_RESPONSE = "quick_response"
    MULTIMODAL = "multimodal"
    LARGE_CONTEXT = "large_context"

class OptimizedLLMRouter:
    """ÏµúÏ†ÅÌôîÎêú LLM ÎùºÏö∞ÌåÖ ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        # ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
        self.anthropic = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        
        # Î™®Îç∏ ÏÑ±Îä• ÌäπÏÑ± Ï†ïÏùò
        self.model_specs = {
            ModelType.CLAUDE_4_SONNET: {
                "max_tokens": 200000,
                "cost_per_1k": 0.015,
                "speed_score": 7,
                "reasoning_score": 10,
                "multimodal": False
            },
            ModelType.CLAUDE_37_SONNET: {
                "max_tokens": 200000,
                "cost_per_1k": 0.010,
                "speed_score": 8,
                "reasoning_score": 9,
                "multimodal": False
            },
            ModelType.CLAUDE_35_HAIKU: {
                "max_tokens": 200000,
                "cost_per_1k": 0.005,
                "speed_score": 10,
                "reasoning_score": 7,
                "multimodal": False
            },
            ModelType.GEMINI_25_PRO: {
                "max_tokens": 2000000,
                "cost_per_1k": 0.007,
                "speed_score": 6,
                "reasoning_score": 9,
                "multimodal": True
            },
            ModelType.GEMINI_25_FLASH: {
                "max_tokens": 1000000,
                "cost_per_1k": 0.003,
                "speed_score": 9,
                "reasoning_score": 8,
                "multimodal": True
            },
            ModelType.GEMINI_20_FLASH: {
                "max_tokens": 1000000,
                "cost_per_1k": 0.002,
                "speed_score": 10,
                "reasoning_score": 7,
                "multimodal": True
            }
        }
        
        # ÏûëÏóÖÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ Îß§Ìïë
        self.task_model_mapping = {
            TaskType.COMPLEX_ANALYSIS: [
                ModelType.CLAUDE_4_SONNET,
                ModelType.GEMINI_25_PRO,
                ModelType.CLAUDE_37_SONNET
            ],
            TaskType.BALANCED_REASONING: [
                ModelType.CLAUDE_37_SONNET,
                ModelType.GEMINI_25_FLASH,
                ModelType.CLAUDE_35_HAIKU
            ],
            TaskType.QUICK_RESPONSE: [
                ModelType.CLAUDE_35_HAIKU,
                ModelType.GEMINI_20_FLASH,
                ModelType.GEMINI_25_FLASH
            ],
            TaskType.MULTIMODAL: [
                ModelType.GEMINI_25_PRO,
                ModelType.GEMINI_25_FLASH,
                ModelType.GEMINI_20_FLASH
            ],
            TaskType.LARGE_CONTEXT: [
                ModelType.GEMINI_25_PRO,
                ModelType.CLAUDE_4_SONNET,
                ModelType.CLAUDE_37_SONNET
            ]
        }
    
    async def get_optimal_model(self, task_type: str, context_length: int = 0, 
                              has_images: bool = False, priority: str = "balanced") -> Any:
        """ÏµúÏ†Å Î™®Îç∏ ÏÑ†ÌÉù Î∞è Î∞òÌôò"""
        
        # ÏûëÏóÖ Ïú†Ìòï Í≤∞Ï†ï
        if has_images:
            task_enum = TaskType.MULTIMODAL
        elif context_length > 100000:
            task_enum = TaskType.LARGE_CONTEXT
        elif task_type in ["complex_analysis", "deep_reasoning"]:
            task_enum = TaskType.COMPLEX_ANALYSIS
        elif task_type in ["quick_response", "simple_qa"]:
            task_enum = TaskType.QUICK_RESPONSE
        else:
            task_enum = TaskType.BALANCED_REASONING
        
        # ÌõÑÎ≥¥ Î™®Îç∏ Î™©Î°ù
        candidate_models = self.task_model_mapping[task_enum]
        
        # Ïö∞ÏÑ†ÏàúÏúÑÏóê Îî∞Î•∏ Î™®Îç∏ ÏÑ†ÌÉù
        selected_model = self._select_by_priority(candidate_models, priority, context_length)
        
        # Î™®Îç∏ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∞òÌôò
        return await self._get_model_client(selected_model)
    
    def _select_by_priority(self, candidates: list, priority: str, context_length: int) -> ModelType:
        """Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò Î™®Îç∏ ÏÑ†ÌÉù"""
        if priority == "cost":
            # ÎπÑÏö© ÏµúÏ†ÅÌôî
            return min(candidates, key=lambda m: self.model_specs[m]["cost_per_1k"])
        elif priority == "speed":
            # ÏÜçÎèÑ ÏµúÏ†ÅÌôî
            return max(candidates, key=lambda m: self.model_specs[m]["speed_score"])
        elif priority == "quality":
            # ÌíàÏßà ÏµúÏ†ÅÌôî
            return max(candidates, key=lambda m: self.model_specs[m]["reasoning_score"])
        else:
            # Í∑†ÌòïÏû°Ìûå ÏÑ†ÌÉù (Í∏∞Î≥∏Í∞í)
            return self._balanced_selection(candidates, context_length)
    
    def _balanced_selection(self, candidates: list, context_length: int) -> ModelType:
        """Í∑†ÌòïÏû°Ìûå Î™®Îç∏ ÏÑ†ÌÉù ÏïåÍ≥†Î¶¨Ï¶ò"""
        scores = {}
        
        for model in candidates:
            spec = self.model_specs[model]
            
            # Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Ï†ÅÌï©ÏÑ±
            context_score = 1.0 if context_length <= spec["max_tokens"] else 0.5
            
            # Ï¢ÖÌï© Ï†êÏàò Í≥ÑÏÇ∞ (ÏÜçÎèÑ, ÌíàÏßà, ÎπÑÏö©Ïùò Í∞ÄÏ§ëÌèâÍ∑†)
            total_score = (
                spec["speed_score"] * 0.3 +
                spec["reasoning_score"] * 0.4 +
                (10 - spec["cost_per_1k"] * 100) * 0.2 +  # ÎπÑÏö© Ïó≠Ïàò
                context_score * 0.1
            )
            
            scores[model] = total_score
        
        return max(scores, key=scores.get)
    
    async def _get_model_client(self, model_type: ModelType) -> Any:
        """Î™®Îç∏Î≥Ñ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∞òÌôò"""
        if model_type.value.startswith("claude"):
            return await self._get_claude_client(model_type)
        elif model_type.value.startswith("gemini"):
            return await self._get_gemini_client(model_type)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    async def _get_claude_client(self, model_type: ModelType):
        """Claude Î™®Îç∏ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏"""
        # AWS BedrockÏùÑ ÌÜµÌïú Claude Ï†ëÍ∑º
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî boto3Ïùò bedrock-runtime ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÇ¨Ïö©
        class ClaudeWrapper:
            def __init__(self, client, model_id):
                self.client = client
                self.model_id = model_id
            
            async def ainvoke(self, prompt: str, **kwargs):
                # Bedrock API Ìò∏Ï∂ú Íµ¨ÌòÑ
                pass
        
        return ClaudeWrapper(self.anthropic, model_type.value)
    
    async def _get_gemini_client(self, model_type: ModelType):
        """Gemini Î™®Îç∏ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏"""
        model_id_mapping = {
            ModelType.GEMINI_25_PRO: "gemini-2.5-pro",
            ModelType.GEMINI_25_FLASH: "gemini-2.5-flash",
            ModelType.GEMINI_20_FLASH: "gemini-2.0-flash"
        }
        
        return GenerativeModel(model_id_mapping[model_type])
```

---

## üíæ Îç∞Ïù¥ÌÑ∞ ÏïÑÌÇ§ÌÖçÏ≤ò

### PostgreSQL Ïä§ÌÇ§Îßà ÏÑ§Í≥Ñ
```sql
-- ÌôïÏû• Î∞è Í∏∞Î≥∏ ÏÑ§Ï†ï
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";  -- ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ ÏÑ±Îä• Ìñ•ÏÉÅ

-- ÏÇ¨Ïö©Ïûê Î∞è Ïù∏Ï¶ù (SYGenai Ìò∏Ìôò ÌôïÏû•)
CREATE TABLE users (
    empno VARCHAR(20) PRIMARY KEY,
    name_ko VARCHAR(100) NOT NULL,
    company VARCHAR(100),
    company_code VARCHAR(10),
    roles JSONB DEFAULT '[]'::jsonb,
    permissions JSONB DEFAULT '{}'::jsonb,
    profile_data JSONB DEFAULT '{}'::jsonb,  -- ÏûêÎèô ÏàòÏßëÎêú ÌîÑÎ°úÌååÏùº
    preferences JSONB DEFAULT '{}'::jsonb,   -- ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ÏÇ¨Ïö©Ïûê Ïù∏Îç±Ïä§
CREATE INDEX idx_users_company ON users(company_code);
CREATE INDEX idx_users_profile ON users USING GIN(profile_data);
CREATE INDEX idx_users_updated ON users(updated_at);

-- Ï∫êÏãú ÌÖåÏù¥Î∏î (Redis ÎåÄÏ≤¥)
CREATE TABLE cache_entries (
    key VARCHAR(255) PRIMARY KEY,
    data JSONB NOT NULL,
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
    tags TEXT[] DEFAULT '{}',  -- Ï∫êÏãú ÌÉúÍπÖ
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    accessed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    access_count INTEGER DEFAULT 0
);

-- Ï∫êÏãú Ïù∏Îç±Ïä§ (ÏÑ±Îä• ÏµúÏ†ÅÌôî)
CREATE INDEX idx_cache_expires ON cache_entries(expires_at);
CREATE INDEX idx_cache_tags ON cache_entries USING GIN(tags);
CREATE INDEX idx_cache_accessed ON cache_entries(accessed_at);

-- ÏûêÎèô ÎßåÎ£åÎêú Ï∫êÏãú Ï†ïÎ¶¨ Ìï®Ïàò
CREATE OR REPLACE FUNCTION cleanup_expired_cache()
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    DELETE FROM cache_entries WHERE expires_at <= NOW();
    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

-- ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Î∞è ÌòëÏóÖ
CREATE TABLE workspaces (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    owner_empno VARCHAR(20) NOT NULL REFERENCES users(empno) ON DELETE CASCADE,
    title VARCHAR(200) NOT NULL,
    description TEXT,
    settings JSONB DEFAULT '{}'::jsonb,
    collaborators JSONB DEFAULT '[]'::jsonb,
    visibility VARCHAR(20) DEFAULT 'private' CHECK (visibility IN ('private', 'team', 'public')),
    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'archived', 'deleted')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Ïù∏Îç±Ïä§
CREATE INDEX idx_workspaces_owner ON workspaces(owner_empno);
CREATE INDEX idx_workspaces_status ON workspaces(status);
CREATE INDEX idx_workspaces_updated ON workspaces(updated_at);
CREATE INDEX idx_workspaces_collaborators ON workspaces USING GIN(collaborators);

-- ÏïÑÌã∞Ìå©Ìä∏ (ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Í≤∞Í≥ºÎ¨º)
CREATE TABLE artifacts (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    workspace_id UUID NOT NULL REFERENCES workspaces(id) ON DELETE CASCADE,
    parent_id UUID REFERENCES artifacts(id),  -- Î≤ÑÏ†Ñ Í¥ÄÎ¶¨Ïö©
    type VARCHAR(50) NOT NULL,  -- 'document', 'code', 'chart', 'image', 'data'
    title VARCHAR(200),
    description TEXT,
    content JSONB NOT NULL,
    metadata JSONB DEFAULT '{}'::jsonb,
    version INTEGER DEFAULT 1,
    status VARCHAR(20) DEFAULT 'draft' CHECK (status IN ('draft', 'published', 'archived')),
    created_by VARCHAR(20) NOT NULL REFERENCES users(empno),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ÏïÑÌã∞Ìå©Ìä∏ Ïù∏Îç±Ïä§
CREATE INDEX idx_artifacts_workspace ON artifacts(workspace_id);
CREATE INDEX idx_artifacts_type ON artifacts(type);
CREATE INDEX idx_artifacts_created_by ON artifacts(created_by);
CREATE INDEX idx_artifacts_parent ON artifacts(parent_id);
CREATE INDEX idx_artifacts_updated ON artifacts(updated_at);

-- ÎåÄÌôî Î∞è Î©îÏãúÏßÄ (Í∏∞Î≥∏ Íµ¨Ï°∞)
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_empno VARCHAR(20) NOT NULL REFERENCES users(empno),
    workspace_id UUID REFERENCES workspaces(id),
    title VARCHAR(200),
    context JSONB DEFAULT '{}'::jsonb,
    agent_config JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_conversations_user ON conversations(user_empno);
CREATE INDEX idx_conversations_workspace ON conversations(workspace_id);
CREATE INDEX idx_conversations_updated ON conversations(updated_at);

CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}'::jsonb,
    agent_type VARCHAR(50),
    model_used VARCHAR(50),
    tokens_used INTEGER,
    processing_time FLOAT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_messages_conversation ON messages(conversation_id);
CREATE INDEX idx_messages_created ON messages(created_at);
CREATE INDEX idx_messages_agent ON messages(agent_type);

-- Tier1 ÏóêÏù¥Ï†ÑÌä∏ MCP ÏÑ§Ï†ï
CREATE TABLE agent_mcp_configs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL UNIQUE,  -- 'regulations', 'legal', 'accounting', 'tax'
    name VARCHAR(100) NOT NULL,
    description TEXT,
    config_data JSONB NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_by VARCHAR(20) NOT NULL REFERENCES users(empno),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_agent_configs_type ON agent_mcp_configs(agent_type);
CREATE INDEX idx_agent_configs_active ON agent_mcp_configs(is_active);

-- ÏÇ¨Ïö©ÏûêÎ≥Ñ ÏóêÏù¥Ï†ÑÌä∏ Í∂åÌïú
CREATE TABLE user_agent_permissions (
    empno VARCHAR(20) REFERENCES users(empno) ON DELETE CASCADE,
    agent_type VARCHAR(50),
    permissions JSONB DEFAULT '{}'::jsonb,
    granted_by VARCHAR(20) REFERENCES users(empno),
    granted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    expires_at TIMESTAMP WITH TIME ZONE,
    PRIMARY KEY (empno, agent_type)
);

CREATE INDEX idx_user_agent_perms_granted ON user_agent_permissions(granted_at);
CREATE INDEX idx_user_agent_perms_expires ON user_agent_permissions(expires_at);

-- ÏÉàÎØ∏ GPT Í∏∞Îä• ÏÑ§Ï†ï
CREATE TABLE user_feature_preferences (
    empno VARCHAR(20) PRIMARY KEY REFERENCES users(empno) ON DELETE CASCADE,
    enable_deep_research BOOLEAN DEFAULT true,
    enable_web_search BOOLEAN DEFAULT true,
    enable_canvas_mode BOOLEAN DEFAULT true,
    enable_file_processing BOOLEAN DEFAULT true,
    model_preference JSONB DEFAULT '{}'::jsonb,  -- ÏÇ¨Ïö©ÏûêÎ≥Ñ Î™®Îç∏ ÏÑ†Ìò∏ÎèÑ
    ui_preferences JSONB DEFAULT '{}'::jsonb,    -- UI ÏÑ§Ï†ï
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌååÏùº ÏûêÎèô ÏàòÏßë Î°úÍ∑∏
CREATE TABLE profile_collection_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    empno VARCHAR(20) NOT NULL REFERENCES users(empno) ON DELETE CASCADE,
    data_source VARCHAR(100) NOT NULL,  -- 'chat_interaction', 'file_upload', 'workspace_activity'
    collected_data JSONB NOT NULL,
    confidence_score FLOAT CHECK (confidence_score >= 0 AND confidence_score <= 1),
    processed BOOLEAN DEFAULT false,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_profile_logs_user ON profile_collection_logs(empno, created_at);
CREATE INDEX idx_profile_logs_source ON profile_collection_logs(data_source);
CREATE INDEX idx_profile_logs_processed ON profile_collection_logs(processed);

-- ÌôúÎèô Î°úÍ∑∏ (ÌòëÏóÖ Ï∂îÏ†Å)
CREATE TABLE activity_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    empno VARCHAR(20) NOT NULL REFERENCES users(empno),
    action VARCHAR(100) NOT NULL,
    resource_type VARCHAR(50),
    resource_id UUID,
    metadata JSONB DEFAULT '{}'::jsonb,
    ip_address INET,
    user_agent TEXT,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ÌôúÎèô Î°úÍ∑∏ ÌååÌã∞ÏÖîÎãù (ÏÑ±Îä• ÏµúÏ†ÅÌôî)
CREATE INDEX idx_activity_logs_user_time ON activity_logs(empno, timestamp);
CREATE INDEX idx_activity_logs_action ON activity_logs(action);
CREATE INDEX idx_activity_logs_resource ON activity_logs(resource_type, resource_id);

-- ÌååÏùº Î∞è ÏóÖÎ°úÎìú Í¥ÄÎ¶¨
CREATE TABLE uploaded_files (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_empno VARCHAR(20) NOT NULL REFERENCES users(empno),
    workspace_id UUID REFERENCES workspaces(id),
    original_name VARCHAR(500) NOT NULL,
    stored_name VARCHAR(500) NOT NULL,
    file_type VARCHAR(100),
    file_size BIGINT,
    mime_type VARCHAR(200),
    storage_path TEXT NOT NULL,
    checksum VARCHAR(64),  -- SHA-256 Ìï¥Ïãú
    processing_status VARCHAR(20) DEFAULT 'pending' 
        CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed')),
    extracted_text TEXT,
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_files_user ON uploaded_files(user_empno);
CREATE INDEX idx_files_workspace ON uploaded_files(workspace_id);
CREATE INDEX idx_files_status ON uploaded_files(processing_status);
CREATE INDEX idx_files_type ON uploaded_files(file_type);
CREATE INDEX idx_files_checksum ON uploaded_files(checksum);

-- Ìä∏Î¶¨Í±∞ Ìï®ÏàòÎì§ (ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏)
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- updated_at ÏûêÎèô ÏóÖÎç∞Ïù¥Ìä∏ Ìä∏Î¶¨Í±∞
CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_workspaces_updated_at
    BEFORE UPDATE ON workspaces
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_artifacts_updated_at
    BEFORE UPDATE ON artifacts
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_conversations_updated_at
    BEFORE UPDATE ON conversations
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- ÏÑ±Îä• ÏµúÏ†ÅÌôîÎ•º ÏúÑÌïú Î∑∞
CREATE VIEW active_workspaces AS
SELECT w.*, u.name_ko as owner_name
FROM workspaces w
JOIN users u ON w.owner_empno = u.empno
WHERE w.status = 'active';

CREATE VIEW recent_activity AS
SELECT 
    al.*, 
    u.name_ko as user_name,
    CASE 
        WHEN al.resource_type = 'workspace' THEN w.title
        WHEN al.resource_type = 'artifact' THEN a.title
        ELSE NULL
    END as resource_name
FROM activity_logs al
JOIN users u ON al.empno = u.empno
LEFT JOIN workspaces w ON al.resource_type = 'workspace' AND al.resource_id = w.id
LEFT JOIN artifacts a ON al.resource_type = 'artifact' AND al.resource_id = a.id
WHERE al.timestamp >= NOW() - INTERVAL '7 days'
ORDER BY al.timestamp DESC;
```

### Redis ÎåÄÏ≤¥ Ï∫êÏã± ÏãúÏä§ÌÖú
```python
import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Any, Optional, Dict, List, Set
from collections import defaultdict
from cachetools import TTLCache
import asyncpg
from contextlib import asynccontextmanager

class OptimizedCacheManager:
    """Redis ÏóÜÏù¥ PostgreSQL + Î©îÎ™®Î¶¨Î•º ÌôúÏö©Ìïú Í≥†ÏÑ±Îä• Ï∫êÏã±"""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
        
        # L1 Ï∫êÏãú: Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î©îÎ™®Î¶¨ (Í∞ÄÏû• Îπ†Î¶Ñ)
        self.memory_cache = TTLCache(maxsize=10000, ttl=300)  # 5Î∂Ñ TTL
        
        # Hot Îç∞Ïù¥ÌÑ∞ ÏãùÎ≥ÑÏùÑ ÏúÑÌïú Ïï°ÏÑ∏Ïä§ Ï∂îÏ†Å
        self.access_tracker = defaultdict(int)
        self.last_cleanup = time.time()
        
        # Ï∫êÏãú ÌÜµÍ≥Ñ
        self.stats = {
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0,
            'writes': 0
        }
    
    async def get(self, key: str) -> Optional[Any]:
        """Ï∫êÏãúÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå (L1 -> L2 ÏàúÏÑú)"""
        
        # L1: Î©îÎ™®Î¶¨ Ï∫êÏãú ÌôïÏù∏ (Í∞ÄÏû• Îπ†Î¶Ñ)
        if key in self.memory_cache:
            self.stats['l1_hits'] += 1
            self.access_tracker[key] += 1
            return self.memory_cache[key]
        
        # L2: PostgreSQL Ï∫êÏãú ÌÖåÏù¥Î∏î ÌôïÏù∏
        try:
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(
                    """
                    UPDATE cache_entries 
                    SET accessed_at = NOW(), access_count = access_count + 1
                    WHERE key = $1 AND expires_at > NOW()
                    RETURNING data, access_count
                    """,
                    key
                )
                
                if row:
                    data = row['data']
                    self.stats['l2_hits'] += 1
                    
                    # L1 Ï∫êÏãúÏóê ÏäπÍ≤© (ÏûêÏ£º Ïï°ÏÑ∏Ïä§ÎêòÎäî Îç∞Ïù¥ÌÑ∞)
                    if row['access_count'] > 3:
                        self.memory_cache[key] = data
                    
                    return data
        
        except Exception as e:
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïò§Î•ò ÏãúÏóêÎèÑ ÏÑúÎπÑÏä§ Í≥ÑÏÜç Ï†úÍ≥µ
            print(f"Cache L2 error: {e}")
        
        self.stats['misses'] += 1
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 300, tags: List[str] = None) -> bool:
        """Ï∫êÏãúÏóê Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
        try:
            # L1: Î©îÎ™®Î¶¨ Ï∫êÏãúÏóê Ï†ÄÏû• (TTL Ï†ÅÏö©)
            self.memory_cache[key] = value
            
            # L2: PostgreSQL ÏòÅÏÜç Ï†ÄÏû•
            expires_at = datetime.utcnow() + timedelta(seconds=ttl)
            tags = tags or []
            
            async with self.db_pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO cache_entries (key, data, expires_at, tags)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (key) DO UPDATE SET
                        data = EXCLUDED.data,
                        expires_at = EXCLUDED.expires_at,
                        tags = EXCLUDED.tags,
                        accessed_at = NOW()
                    """,
                    key, json.dumps(value), expires_at, tags
                )
            
            self.stats['writes'] += 1
            return True
            
        except Exception as e:
            print(f"Cache write error: {e}")
            return False
    
    async def invalidate(self, key: str) -> bool:
        """ÌäπÏ†ï ÌÇ§ Ï∫êÏãú Î¨¥Ìö®Ìôî"""
        # L1 Ï∫êÏãúÏóêÏÑú Ï†úÍ±∞
        self.memory_cache.pop(key, None)
        
        # L2 Ï∫êÏãúÏóêÏÑú Ï†úÍ±∞
        try:
            async with self.db_pool.acquire() as conn:
                result = await conn.execute(
                    "DELETE FROM cache_entries WHERE key = $1", key
                )
                return result == "DELETE 1"
        except Exception as e:
            print(f"Cache invalidation error: {e}")
            return False
    
    async def invalidate_by_tags(self, tags: List[str]) -> int:
        """ÌÉúÍ∑∏ Í∏∞Î∞ò Ï∫êÏãú Î¨¥Ìö®Ìôî"""
        if not tags:
            return 0
            
        try:
            async with self.db_pool.acquire() as conn:
                # Ìï¥Îãπ ÌÉúÍ∑∏Î•º Í∞ÄÏßÑ ÌÇ§Îì§ Ï°∞Ìöå
                rows = await conn.fetch(
                    "SELECT key FROM cache_entries WHERE tags && $1", tags
                )
                
                # L1 Ï∫êÏãúÏóêÏÑú Ìï¥Îãπ ÌÇ§Îì§ Ï†úÍ±∞
                for row in rows:
                    self.memory_cache.pop(row['key'], None)
                
                # L2 Ï∫êÏãúÏóêÏÑú ÏÇ≠Ï†ú
                result = await conn.execute(
                    "DELETE FROM cache_entries WHERE tags && $1", tags
                )
                
                return int(result.split()[-1])  # "DELETE N"ÏóêÏÑú N Ï∂îÏ∂ú
                
        except Exception as e:
            print(f"Cache tag invalidation error: {e}")
            return 0
    
    async def cleanup_expired(self) -> int:
        """ÎßåÎ£åÎêú Ï∫êÏãú Ï†ïÎ¶¨"""
        current_time = time.time()
        
        # 1Î∂ÑÏóê Ìïú Î≤àÎßå Ï†ïÎ¶¨ Ïã§Ìñâ
        if current_time - self.last_cleanup < 60:
            return 0
        
        try:
            async with self.db_pool.acquire() as conn:
                deleted_count = await conn.fetchval(
                    "SELECT cleanup_expired_cache()"
                )
                
                self.last_cleanup = current_time
                return deleted_count or 0
                
        except Exception as e:
            print(f"Cache cleanup error: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Ï∫êÏãú ÌÜµÍ≥Ñ Î∞òÌôò"""
        total_requests = sum([
            self.stats['l1_hits'],
            self.stats['l2_hits'], 
            self.stats['misses']
        ])
        
        if total_requests == 0:
            return self.stats
        
        return {
            **self.stats,
            'hit_rate': (self.stats['l1_hits'] + self.stats['l2_hits']) / total_requests,
            'l1_hit_rate': self.stats['l1_hits'] / total_requests,
            'memory_cache_size': len(self.memory_cache),
            'memory_cache_maxsize': self.memory_cache.maxsize
        }
    
    async def warm_up(self, keys: List[str]) -> int:
        """Ï∫êÏãú ÏõúÏóÖ - ÏûêÏ£º ÏÇ¨Ïö©ÎêòÎäî ÌÇ§Îì§ÏùÑ ÎØ∏Î¶¨ L1 Ï∫êÏãúÎ°ú Î°úÎìú"""
        warmed_count = 0
        
        try:
            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch(
                    """
                    SELECT key, data FROM cache_entries 
                    WHERE key = ANY($1) AND expires_at > NOW()
                    ORDER BY access_count DESC
                    """,
                    keys
                )
                
                for row in rows:
                    self.memory_cache[row['key']] = row['data']
                    warmed_count += 1
                    
        except Exception as e:
            print(f"Cache warmup error: {e}")
        
        return warmed_count

# WebSocket ÏÉÅÌÉú Í¥ÄÎ¶¨
class WebSocketStateManager:
    """Ïã§ÏãúÍ∞Ñ ÌòëÏóÖÏùÑ ÏúÑÌïú WebSocket ÏÉÅÌÉú Í¥ÄÎ¶¨"""
    
    def __init__(self, cache_manager: OptimizedCacheManager):
        self.cache = cache_manager
        self.active_connections: Dict[str, Set] = defaultdict(set)
        self.user_workspaces: Dict[str, Set[str]] = defaultdict(set)
    
    async def join_workspace(self, workspace_id: str, websocket, user_id: str):
        """ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Ï∞∏Ïó¨"""
        self.active_connections[workspace_id].add(websocket)
        self.user_workspaces[user_id].add(workspace_id)
        
        # ÏÇ¨Ïö©Ïûê ÏÉÅÌÉúÎ•º Ï∫êÏãúÏóê Ï†ÄÏû•
        await self.cache.set(
            f"ws_user:{workspace_id}:{user_id}",
            {
                "status": "active",
                "joined_at": time.time(),
                "workspace_id": workspace_id
            },
            ttl=3600,  # 1ÏãúÍ∞Ñ
            tags=[f"workspace:{workspace_id}", f"user:{user_id}"]
        )
        
        # Îã§Î•∏ ÏÇ¨Ïö©ÏûêÎì§ÏóêÍ≤å Ï∞∏Ïó¨ ÏïåÎ¶º
        await self.broadcast_to_workspace(workspace_id, {
            "type": "user_joined",
            "user_id": user_id,
            "timestamp": time.time()
        }, exclude_user=user_id)
    
    async def leave_workspace(self, workspace_id: str, websocket, user_id: str):
        """ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ Îñ†ÎÇòÍ∏∞"""
        self.active_connections[workspace_id].discard(websocket)
        self.user_workspaces[user_id].discard(workspace_id)
        
        # ÏÇ¨Ïö©Ïûê ÏÉÅÌÉú Ï†úÍ±∞
        await self.cache.invalidate(f"ws_user:{workspace_id}:{user_id}")
        
        # Îã§Î•∏ ÏÇ¨Ïö©ÏûêÎì§ÏóêÍ≤å Îñ†ÎÇ® ÏïåÎ¶º
        await self.broadcast_to_workspace(workspace_id, {
            "type": "user_left", 
            "user_id": user_id,
            "timestamp": time.time()
        })
    
    async def broadcast_to_workspace(self, workspace_id: str, message: Dict, 
                                   exclude_user: str = None):
        """ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ ÎÇ¥ Î™®Îì† ÏÇ¨Ïö©ÏûêÏóêÍ≤å Î©îÏãúÏßÄ Î∏åÎ°úÎìúÏ∫êÏä§Ìä∏"""
        connections = self.active_connections.get(workspace_id, set()).copy()
        
        failed_connections = set()
        
        for websocket in connections:
            try:
                # exclude_user Ï≤¥ÌÅ¨ (websocketÏóê user_idÍ∞Ä ÏûàÎã§Î©¥)
                if hasattr(websocket, 'user_id') and websocket.user_id == exclude_user:
                    continue
                    
                await websocket.send_json(message)
                
            except Exception as e:
                print(f"WebSocket send error: {e}")
                failed_connections.add(websocket)
        
        # Ïã§Ìå®Ìïú Ïó∞Í≤∞Îì§ Ï†ïÎ¶¨
        for failed_ws in failed_connections:
            self.active_connections[workspace_id].discard(failed_ws)
    
    async def get_active_users(self, workspace_id: str) -> List[Dict]:
        """ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ ÌôúÏÑ± ÏÇ¨Ïö©Ïûê Î™©Î°ù"""
        try:
            # Ï∫êÏãúÏóêÏÑú ÌôúÏÑ± ÏÇ¨Ïö©Ïûê Ï°∞Ìöå
            pattern_key = f"ws_user:{workspace_id}:*"
            # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Redis SCANÍ≥º Ïú†ÏÇ¨Ìïú Í∏∞Îä• ÌïÑÏöî
            # Ïó¨Í∏∞ÏÑúÎäî Îã®ÏàúÌôîÎêú Î≤ÑÏ†Ñ
            
            active_users = []
            for user_id in self.user_workspaces:
                if workspace_id in self.user_workspaces[user_id]:
                    user_data = await self.cache.get(f"ws_user:{workspace_id}:{user_id}")
                    if user_data:
                        active_users.append({
                            "user_id": user_id,
                            **user_data
                        })
            
            return active_users
            
        except Exception as e:
            print(f"Get active users error: {e}")
            return []
```

---

## üîê Î≥¥Ïïà ÏïÑÌÇ§ÌÖçÏ≤ò

### Ïù∏Ï¶ù Î∞è Í∂åÌïú Í¥ÄÎ¶¨
```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from typing import List, Optional
import jwt
from datetime import datetime, timedelta

class SecurityManager:
    """ÌÜµÌï© Î≥¥Ïïà Í¥ÄÎ¶¨"""
    
    def __init__(self, secret_key: str, db_pool):
        self.secret_key = secret_key
        self.db_pool = db_pool
        self.security = HTTPBearer()
    
    async def create_access_token(self, user_data: dict, expires_delta: Optional[timedelta] = None):
        """JWT ÌÜ†ÌÅ∞ ÏÉùÏÑ±"""
        to_encode = user_data.copy()
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(hours=24)
        
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm="HS256")
        return encoded_jwt
    
    async def verify_token(self, credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer())):
        """JWT ÌÜ†ÌÅ∞ Í≤ÄÏ¶ù"""
        try:
            payload = jwt.decode(credentials.credentials, self.secret_key, algorithms=["HS256"])
            empno = payload.get("empno")
            if empno is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid authentication credentials"
                )
            return payload
        except jwt.PyJWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid authentication credentials"
            )
    
    async def get_current_user(self, token_data: dict = Depends(verify_token)):
        """ÌòÑÏû¨ ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥ Ï°∞Ìöå"""
        async with self.db_pool.acquire() as conn:
            user = await conn.fetchrow(
                "SELECT * FROM users WHERE empno = $1", token_data["empno"]
            )
            if not user:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="User not found"
                )
            return dict(user)
    
    async def check_permissions(self, user: dict, resource: str, action: str) -> bool:
        """Í∂åÌïú ÌôïÏù∏"""
        user_permissions = user.get('permissions', {})
        
        # Í¥ÄÎ¶¨ÏûêÎäî Î™®Îì† Í∂åÌïú
        if 'admin' in user.get('roles', []):
            return True
        
        # Î¶¨ÏÜåÏä§Î≥Ñ Í∂åÌïú ÌôïÏù∏
        resource_permissions = user_permissions.get(resource, {})
        return resource_permissions.get(action, False)
    
    async def check_agent_access(self, user_empno: str, agent_type: str) -> bool:
        """ÏóêÏù¥Ï†ÑÌä∏ Ï†ëÍ∑º Í∂åÌïú ÌôïÏù∏"""
        async with self.db_pool.acquire() as conn:
            result = await conn.fetchrow(
                """
                SELECT permissions FROM user_agent_permissions 
                WHERE empno = $1 AND agent_type = $2 
                AND (expires_at IS NULL OR expires_at > NOW())
                """,
                user_empno, agent_type
            )
            
            return result is not None

# ÏûÖÎ†• Í≤ÄÏ¶ù
from pydantic import BaseModel, validator
from typing import List, Optional
import re

class ChatRequest(BaseModel):
    message: str
    agent_type: Optional[str] = "general"
    model_preference: Optional[str] = None
    workspace_id: Optional[str] = None
    files: Optional[List[str]] = None
    
    @validator('message')
    def validate_message(cls, v):
        if len(v.strip()) == 0:
            raise ValueError('Message cannot be empty')
        if len(v) > 10000:  # Î©îÏãúÏßÄ Í∏∏Ïù¥ Ï†úÌïú
            raise ValueError('Message too long')
        
        # ÏïÖÏÑ± Ìå®ÌÑ¥ Í≤ÄÏÇ¨
        malicious_patterns = [
            r'<script.*?>.*?</script>',  # XSS Î∞©ÏßÄ
            r'javascript:',
            r'on\w+\s*=',  # Ïù¥Î≤§Ìä∏ Ìï∏Îì§Îü¨
        ]
        
        for pattern in malicious_patterns:
            if re.search(pattern, v, re.IGNORECASE):
                raise ValueError('Malicious content detected')
        
        return v.strip()
    
    @validator('agent_type')
    def validate_agent_type(cls, v):
        allowed_agents = [
            'general', 'web_search', 'multimodal_rag', 
            'canvas', 'regulations', 'legal', 'accounting', 'tax'
        ]
        if v not in allowed_agents:
            raise ValueError(f'Invalid agent type. Must be one of: {allowed_agents}')
        return v

# Rate Limiting
from fastapi import Request
from collections import defaultdict
import time

class RateLimiter:
    """API Rate Limiting"""
    
    def __init__(self):
        self.requests = defaultdict(list)
        self.limits = {
            'default': {'requests': 100, 'window': 3600},  # 1ÏãúÍ∞ÑÏóê 100Ìöå
            'chat': {'requests': 50, 'window': 3600},      # 1ÏãúÍ∞ÑÏóê 50Ìöå Ï±ÑÌåÖ
            'upload': {'requests': 20, 'window': 3600},    # 1ÏãúÍ∞ÑÏóê 20Ìöå ÏóÖÎ°úÎìú
        }
    
    async def check_rate_limit(self, request: Request, endpoint: str = 'default') -> bool:
        """Rate limit ÌôïÏù∏"""
        client_ip = request.client.host
        current_time = time.time()
        
        limit_config = self.limits.get(endpoint, self.limits['default'])
        window_size = limit_config['window']
        max_requests = limit_config['requests']
        
        # ÌòÑÏû¨ ÏãúÍ∞Ñ Í∏∞Ï§ÄÏúºÎ°ú ÏúàÎèÑÏö∞ ÎÇ¥ ÏöîÏ≤≠Îì§Îßå Ïú†ÏßÄ
        user_requests = self.requests[f"{client_ip}:{endpoint}"]
        self.requests[f"{client_ip}:{endpoint}"] = [
            req_time for req_time in user_requests 
            if current_time - req_time < window_size
        ]
        
        # ÌòÑÏû¨ ÏöîÏ≤≠ Ï∂îÍ∞Ä
        self.requests[f"{client_ip}:{endpoint}"].append(current_time)
        
        # Rate limit ÌôïÏù∏
        return len(self.requests[f"{client_ip}:{endpoint}"]) <= max_requests

# ÎØ∏Îì§Ïõ®Ïñ¥ ÏÑ§Ï†ï
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware

def setup_security_middleware(app: FastAPI):
    """Î≥¥Ïïà ÎØ∏Îì§Ïõ®Ïñ¥ ÏÑ§Ï†ï"""
    
    # CORS ÏÑ§Ï†ï
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000"],  # ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî Ïã§Ï†ú ÎèÑÎ©îÏù∏
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE"],
        allow_headers=["*"],
    )
    
    # Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî Ìò∏Ïä§Ìä∏Îßå ÌóàÏö©
    app.add_middleware(
        TrustedHostMiddleware, 
        allowed_hosts=["localhost", "127.0.0.1", "*.yourdomain.com"]
    )
```

---

## üìà ÏÑ±Îä• ÏµúÏ†ÅÌôî

### Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏµúÏ†ÅÌôî
```python
import asyncpg
from typing import Optional
import json

class DatabaseOptimizer:
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÑ±Îä• ÏµúÏ†ÅÌôî"""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
    
    async def optimize_queries(self):
        """ÏøºÎ¶¨ ÏµúÏ†ÅÌôî ÏÑ§Ï†ï"""
        async with self.db_pool.acquire() as conn:
            # PostgreSQL ÏÑ±Îä• ÏÑ§Ï†ï
            await conn.execute("SET work_mem = '256MB'")
            await conn.execute("SET shared_buffers = '256MB'") 
            await conn.execute("SET effective_cache_size = '1GB'")
            await conn.execute("SET random_page_cost = 1.1")  # SSD ÏµúÏ†ÅÌôî
            
            # JSON ÏøºÎ¶¨ ÏµúÏ†ÅÌôî
            await conn.execute("SET enable_seqscan = off")  # Ïù∏Îç±Ïä§ ÏÇ¨Ïö© Í∞ïÏ†ú
    
    async def create_optimal_indexes(self):
        """ÏÑ±Îä• ÏµúÏ†ÅÌôî Ïù∏Îç±Ïä§ ÏÉùÏÑ±"""
        indexes = [
            # Î≥µÌï© Ïù∏Îç±Ïä§
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_conv_created ON messages(conversation_id, created_at DESC)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_artifacts_workspace_updated ON artifacts(workspace_id, updated_at DESC)",
            
            # Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§ (Ï°∞Í±¥Î∂Ä)
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_active_workspaces ON workspaces(owner_empno) WHERE status = 'active'",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_recent_activity ON activity_logs(empno, timestamp) WHERE timestamp >= NOW() - INTERVAL '30 days'",
            
            # JSON Ïù∏Îç±Ïä§ (JSONB ÌïÑÎìúÏö©)
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_profile_skills ON users USING GIN((profile_data->'skills'))",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_artifact_metadata ON artifacts USING GIN(metadata)",
            
            # Ï†ÑÎ¨∏ Í≤ÄÏÉâ Ïù∏Îç±Ïä§
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_workspace_title_search ON workspaces USING GIN(to_tsvector('english', title))",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_artifact_content_search ON artifacts USING GIN(to_tsvector('english', coalesce(title, '') || ' ' || coalesce(description, '')))",
        ]
        
        async with self.db_pool.acquire() as conn:
            for index_sql in indexes:
                try:
                    await conn.execute(index_sql)
                except Exception as e:
                    print(f"Index creation failed: {e}")
    
    async def analyze_query_performance(self, query: str, params: tuple = None) -> dict:
        """ÏøºÎ¶¨ ÏÑ±Îä• Î∂ÑÏÑù"""
        async with self.db_pool.acquire() as conn:
            # EXPLAIN ANALYZE Ïã§Ìñâ
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}"
            
            if params:
                result = await conn.fetchval(explain_query, *params)
            else:
                result = await conn.fetchval(explain_query)
            
            return result[0]  # JSON Í≤∞Í≥º Î∞òÌôò

# Ïó∞Í≤∞ ÌíÄ ÏµúÏ†ÅÌôî
async def create_optimized_pool():
    """ÏµúÏ†ÅÌôîÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌíÄ"""
    return await asyncpg.create_pool(
        host="localhost",
        port=5432,
        user="postgres", 
        password="password",
        database="ai_portal",
        min_size=10,           # ÏµúÏÜå Ïó∞Í≤∞ Ïàò
        max_size=50,           # ÏµúÎåÄ Ïó∞Í≤∞ Ïàò
        max_queries=5000,      # Ïó∞Í≤∞Îãπ ÏµúÎåÄ ÏøºÎ¶¨ Ïàò
        max_inactive_connection_lifetime=300.0,  # ÎπÑÌôúÏÑ± Ïó∞Í≤∞ ÏàòÎ™Ö (5Î∂Ñ)
        command_timeout=60,    # Î™ÖÎ†π ÌÉÄÏûÑÏïÑÏõÉ
        server_settings={
            'application_name': 'ai_portal',
            'jit': 'off',      # JIT Ïª¥ÌååÏùº ÎπÑÌôúÏÑ±Ìôî (ÏûëÏùÄ ÏøºÎ¶¨Ïóê Ïò§Î≤ÑÌó§Îìú)
        }
    )

# API ÏùëÎãµ ÏµúÏ†ÅÌôî
from fastapi import BackgroundTasks
import asyncio

class ResponseOptimizer:
    """API ÏùëÎãµ ÏµúÏ†ÅÌôî"""
    
    @staticmethod
    async def batch_database_operations(operations: list):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏûëÏóÖ Î∞∞Ïπò Ï≤òÎ¶¨"""
        # Ïó¨Îü¨ ÏûëÏóÖÏùÑ ÌïòÎÇòÏùò Ìä∏ÎûúÏû≠ÏÖòÏúºÎ°ú Î¨∂Ïñ¥ Ï≤òÎ¶¨
        async with db_pool.acquire() as conn:
            async with conn.transaction():
                results = []
                for operation in operations:
                    result = await operation(conn)
                    results.append(result)
                return results
    
    @staticmethod
    async def parallel_data_fetch(fetchers: list):
        """Î≥ëÎ†¨ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå"""
        # ÎèÖÎ¶ΩÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå ÏûëÏóÖÎì§ÏùÑ Î≥ëÎ†¨Î°ú Ïã§Ìñâ
        tasks = [asyncio.create_task(fetcher()) for fetcher in fetchers]
        return await asyncio.gather(*tasks)
    
    @staticmethod
    def background_task_handler(background_tasks: BackgroundTasks, task_func, *args, **kwargs):
        """Î∞±Í∑∏ÎùºÏö¥Îìú ÏûëÏóÖ Ï≤òÎ¶¨"""
        background_tasks.add_task(task_func, *args, **kwargs)

# ÌîÑÎ°†Ìä∏ÏóîÎìú ÏÑ±Îä• ÏµúÏ†ÅÌôî
class FrontendOptimizer:
    """ÌîÑÎ°†Ìä∏ÏóîÎìú ÏÑ±Îä• ÏµúÏ†ÅÌôî Í∞ÄÏù¥Îìú"""
    
    @staticmethod
    def get_optimization_config():
        return {
            # React ÏµúÏ†ÅÌôî
            "react": {
                "lazy_loading": "React.lazy() ÏÇ¨Ïö©ÏúºÎ°ú Ïª¥Ìè¨ÎÑåÌä∏ ÏßÄÏó∞ Î°úÎî©",
                "memoization": "React.memo, useMemo, useCallback Ï†ÅÏ†àÌûà ÌôúÏö©",
                "virtualization": "react-windowÎ°ú Í∏¥ Î¶¨Ïä§Ìä∏ Í∞ÄÏÉÅÌôî",
                "code_splitting": "Dynamic importÎ°ú Î≤àÎì§ Î∂ÑÌï†"
            },
            
            # ÏÉÅÌÉú Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî
            "state_management": {
                "zustand_slices": "Zustand Ïä§ÌÜ†Ïñ¥Î•º Í∏∞Îä•Î≥ÑÎ°ú Î∂ÑÌï†",
                "selective_subscription": "ÌïÑÏöîÌïú ÏÉÅÌÉúÎßå Íµ¨ÎèÖ",
                "computed_values": "ÌååÏÉù ÏÉÅÌÉúÎäî selectorÎ°ú Í≥ÑÏÇ∞",
                "persistence": "Ï§ëÏöîÌïú ÏÉÅÌÉúÎßå localStorageÏóê Ï†ÄÏû•"
            },
            
            # ÎÑ§Ìä∏ÏõåÌÅ¨ ÏµúÏ†ÅÌôî
            "network": {
                "swr_caching": "SWRÎ°ú Îç∞Ïù¥ÌÑ∞ Ï∫êÏã± Î∞è Ïû¨Í≤ÄÏ¶ù",
                "request_deduplication": "ÎèôÏùº ÏöîÏ≤≠ Ï§ëÎ≥µ Ï†úÍ±∞",
                "optimistic_updates": "ÎÇôÍ¥ÄÏ†Å ÏóÖÎç∞Ïù¥Ìä∏Î°ú UX Í∞úÏÑ†",
                "websocket_management": "WebSocket Ïó∞Í≤∞ Ìö®Ïú®Ï†Å Í¥ÄÎ¶¨"
            },
            
            # ÎπåÎìú ÏµúÏ†ÅÌôî
            "build": {
                "tree_shaking": "ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÎäî ÏΩîÎìú Ï†úÍ±∞",
                "asset_optimization": "Ïù¥ÎØ∏ÏßÄ, Ìè∞Ìä∏ ÏµúÏ†ÅÌôî",
                "service_worker": "Ï∫êÏã± Ï†ÑÎûµÏúºÎ°ú Î°úÎî© ÏÑ±Îä• Ìñ•ÏÉÅ",
                "cdn": "Ï†ïÏ†Å ÏûêÏÇ∞ CDN Î∞∞Ìè¨"
            }
        }
```

---

## üîÑ Î∞∞Ìè¨ Î∞è Ïö¥ÏòÅ

### Docker ÏµúÏ†ÅÌôî
```dockerfile
# ÌîÑÎ°†Ìä∏ÏóîÎìú ÏµúÏ†ÅÌôî Dockerfile
FROM node:18-alpine AS frontend-build

# ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
WORKDIR /app

# Ìå®ÌÇ§ÏßÄ ÌååÏùº Î≥µÏÇ¨ (Ï∫êÏãú ÏµúÏ†ÅÌôî)
COPY package*.json ./
RUN npm ci --only=production

# ÏÜåÏä§ ÏΩîÎìú Î≥µÏÇ¨
COPY . .

# ÌîÑÎ°úÎçïÏÖò ÎπåÎìú
RUN npm run build

# ÌîÑÎ°úÎçïÏÖò Ïù¥ÎØ∏ÏßÄ
FROM nginx:alpine AS production
COPY --from=frontend-build /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]

---

# Î∞±ÏóîÎìú ÏµúÏ†ÅÌôî Dockerfile
FROM python:3.11-slim AS backend-build

# ÏãúÏä§ÌÖú ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Python ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò (Ï∫êÏãú ÏµúÏ†ÅÌôî)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏΩîÎìú Î≥µÏÇ¨
COPY . .

# ÌîÑÎ°úÎçïÏÖò ÏÑúÎ≤Ñ Ïã§Ìñâ
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Kubernetes Î∞∞Ìè¨ ÏÑ§Ï†ï
```yaml
# ai-portal-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-portal-backend
  labels:
    app: ai-portal-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-portal-backend
  template:
    metadata:
      labels:
        app: ai-portal-backend
    spec:
      containers:
      - name: backend
        image: ai-portal/backend:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ai-portal-secrets
              key: database-url
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-portal-secrets
              key: anthropic-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: ai-portal-backend-service
spec:
  selector:
    app: ai-portal-backend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-portal-frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ai-portal-frontend
  template:
    metadata:
      labels:
        app: ai-portal-frontend
    spec:
      containers:
      - name: frontend
        image: ai-portal/frontend:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ai-portal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - ai-portal.yourdomain.com
    secretName: ai-portal-tls
  rules:
  - host: ai-portal.yourdomain.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: ai-portal-backend-service
            port:
              number: 80
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ai-portal-frontend-service
            port:
              number: 80
```

### CI/CD ÌååÏù¥ÌîÑÎùºÏù∏
```yaml
# .github/workflows/deploy.yml
name: AI Portal CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install backend dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Run backend tests
      run: |
        cd backend
        pytest tests/ -v --cov=app --cov-report=xml
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test:ci
    
    - name: Build frontend
      run: |
        cd frontend
        npm run build

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: security-scan-results.sarif

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push backend image
      uses: docker/build-push-action@v4
      with:
        context: ./backend
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:latest
    
    - name: Build and push frontend image
      uses: docker/build-push-action@v4
      with:
        context: ./frontend
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:latest

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Kubernetes
      uses: azure/k8s-deploy@v1
      with:
        manifests: |
          k8s/ai-portal-deployment.yaml
          k8s/ai-portal-service.yaml
          k8s/ai-portal-ingress.yaml
```

---

## üìä Î™®ÎãàÌÑ∞ÎßÅ Î∞è Í¥ÄÏ∞∞Í∞ÄÎä•ÏÑ±

### Langsmith ÌÜµÌï©
```python
from langsmith import traceable, Client
from typing import Dict, Any
import os

class LangsmithTracker:
    """Langsmith ÌÜµÌï© Ï∂îÏ†Å"""
    
    def __init__(self):
        self.client = Client(
            api_url="https://api.smith.langchain.com",
            api_key=os.getenv("LANGSMITH_API_KEY")
        )
    
    @traceable(name="agent_execution")
    async def track_agent_execution(self, agent_type: str, input_data: Dict, 
                                  user_id: str, workspace_id: str = None) -> Dict:
        """ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ Ï∂îÏ†Å"""
        
        metadata = {
            "agent_type": agent_type,
            "user_id": user_id,
            "workspace_id": workspace_id,
            "timestamp": time.time()
        }
        
        # ÏóêÏù¥Ï†ÑÌä∏ Ïã§Ìñâ Î°úÏßÅ
        result = await self._execute_agent(agent_type, input_data)
        
        # Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
        metadata.update({
            "execution_time": result.get("execution_time"),
            "model_used": result.get("model_used"),
            "tokens_used": result.get("tokens_used"),
            "success": result.get("success", False)
        })
        
        return {
            **result,
            "trace_metadata": metadata
        }
    
    @traceable(name="llm_call")
    async def track_llm_call(self, model: str, prompt: str, response: str, 
                           tokens_used: int, execution_time: float):
        """LLM Ìò∏Ï∂ú Ï∂îÏ†Å"""
        return {
            "model": model,
            "prompt_length": len(prompt),
            "response_length": len(response), 
            "tokens_used": tokens_used,
            "execution_time": execution_time,
            "cost_estimate": self._calculate_cost(model, tokens_used)
        }
    
    def _calculate_cost(self, model: str, tokens: int) -> float:
        """ÎπÑÏö© Í≥ÑÏÇ∞"""
        cost_per_token = {
            "claude-4.0-sonnet": 0.000015,
            "claude-3.7-sonnet": 0.000010,
            "claude-3.5-haiku": 0.000005,
            "gemini-2.5-pro": 0.000007,
            "gemini-2.5-flash": 0.000003,
            "gemini-2.0-flash": 0.000002
        }
        return tokens * cost_per_token.get(model, 0.00001)
```

### ÏÑ±Îä• Î©îÌä∏Î¶≠
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Request, Response
import time

# Prometheus Î©îÌä∏Î¶≠ Ï†ïÏùò
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'websocket_active_connections',
    'Active WebSocket connections',
    ['workspace_id']
)

AGENT_EXECUTIONS = Counter(
    'agent_executions_total',
    'Total agent executions',
    ['agent_type', 'status']
)

LLM_TOKEN_USAGE = Counter(
    'llm_tokens_used_total',
    'Total LLM tokens used',
    ['model', 'agent_type']
)

CACHE_OPERATIONS = Counter(
    'cache_operations_total',
    'Cache operations',
    ['operation', 'level', 'status']
)

class MetricsMiddleware:
    """ÏÑ±Îä• Î©îÌä∏Î¶≠ ÎØ∏Îì§Ïõ®Ïñ¥"""
    
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            request = Request(scope, receive)
            start_time = time.time()
            
            # ÏùëÎãµ Ï≤òÎ¶¨
            async def send_wrapper(message):
                if message["type"] == "http.response.start":
                    status_code = message["status"]
                    duration = time.time() - start_time
                    
                    # Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
                    REQUEST_COUNT.labels(
                        method=request.method,
                        endpoint=request.url.path,
                        status=status_code
                    ).inc()
                    
                    REQUEST_DURATION.labels(
                        method=request.method,
                        endpoint=request.url.path
                    ).observe(duration)
                
                await send(message)
            
            await self.app(scope, receive, send_wrapper)
        else:
            await self.app(scope, receive, send)

# ÎåÄÏãúÎ≥¥Îìú ÏÑ§Ï†ï
GRAFANA_DASHBOARD_CONFIG = {
    "dashboard": {
        "title": "AI Portal Monitoring",
        "panels": [
            {
                "title": "Request Rate",
                "type": "graph",
                "targets": [
                    {
                        "expr": "rate(http_requests_total[5m])",
                        "legendFormat": "{{method}} {{endpoint}}"
                    }
                ]
            },
            {
                "title": "Response Time",
                "type": "graph", 
                "targets": [
                    {
                        "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                        "legendFormat": "95th percentile"
                    }
                ]
            },
            {
                "title": "Agent Performance",
                "type": "table",
                "targets": [
                    {
                        "expr": "rate(agent_executions_total[1h])",
                        "legendFormat": "{{agent_type}}"
                    }
                ]
            },
            {
                "title": "LLM Token Usage",
                "type": "pie",
                "targets": [
                    {
                        "expr": "sum by (model) (llm_tokens_used_total)",
                        "legendFormat": "{{model}}"
                    }
                ]
            }
        ]
    }
}
```

---

## üöÄ **ÏµúÏã† ÏïÑÌÇ§ÌÖçÏ≤ò Í∞úÏÑ† (2025-08-19)**

### ‚úÖ Ïù∏ÎùºÏù∏ UI ÏïÑÌÇ§ÌÖçÏ≤ò ÌòÅÏã†
```typescript
// ÏÉàÎ°úÏö¥ ÌÜµÌï© ChatInput ÏïÑÌÇ§ÌÖçÏ≤ò
interface ChatInputArchitecture {
  // ÌåùÏóÖ Í∏∞Î∞ò ‚Üí Ïù∏ÎùºÏù∏ ÌÜµÌï© ÏÑ§Í≥Ñ
  popup_elimination: {
    before: "Î≥ÑÎèÑ PopupAISettings Ïª¥Ìè¨ÎÑåÌä∏",
    after: "ChatInput ÎÇ¥Î∂Ä ÌÜµÌï© ÎìúÎ°≠Îã§Ïö¥",
    benefits: ["UX Îã®ÏàúÌôî", "ÏÉÅÌÉú Í¥ÄÎ¶¨ ÏµúÏ†ÅÌôî", "Ï†ëÍ∑ºÏÑ± Ìñ•ÏÉÅ"]
  },
  
  // Î™®Îç∏ ÏÑ†ÌÉù ÏïÑÌÇ§ÌÖçÏ≤ò  
  model_selection: {
    component: "InlineModelDropdown",
    positioning: "bottom-full mb-2 (ÏÉÅÌñ• Ïó¥Î¶º)",
    providers: ["Claude (Star ÏïÑÏù¥ÏΩò)", "Gemini (Zap ÏïÑÏù¥ÏΩò)"],
    models: "8Í∞ú ÌÜµÌï© (4 + 4)",
    responsive: "isMobile Í∏∞Î∞ò Ï†ÅÏùëÌòï ÌÅ¨Í∏∞"
  },
  
  // Í∏∞Îä• ÌÜ†Í∏Ä ÏãúÏä§ÌÖú
  feature_toggles: {
    layout: "inline horizontal",
    position: "ÌååÏùº Ï≤®Î∂Ä ÏïÑÏù¥ÏΩò ÏòÜ",
    buttons: ["Ïõπ Í≤ÄÏÉâ üîç", "Ïã¨Ï∏µ Î¶¨ÏÑúÏπò üìä", "Canvas üé®"],
    interaction: "Îã®Ïùº ÏÑ†ÌÉù + Ïû¨ÌÅ¥Î¶≠ Ìï¥Ï†ú",
    styling: "Î∞òÏùëÌòï ÌÖçÏä§Ìä∏ ÎùºÎ≤®"
  }
}
```

### ‚úÖ Î©îÌÉÄ Í≤ÄÏÉâ ÏãúÏä§ÌÖú ÏïÑÌÇ§ÌÖçÏ≤ò
```python
# 2Îã®Í≥Ñ Î©îÌÉÄ Í≤ÄÏÉâ Ï≤¥Ïù∏ ÏÑ§Í≥Ñ
class MetaSearchArchitecture:
    def __init__(self):
        self.conversation_context_service = ConversationContextService()
        self.information_gap_analyzer = InformationGapAnalyzer() 
        self.agent_suggestion_modal = AgentSuggestionModal()
    
    async def meta_search_pipeline(self, user_query: str, context: Dict):
        # 1Îã®Í≥Ñ: ÏÇ¨Ïö©Ïûê ÏùòÎèÑ + ÎåÄÌôî Îß•ÎùΩ Î∂ÑÏÑù
        intent_analysis = await self.analyze_user_intent(user_query, context)
        
        # 2Îã®Í≥Ñ: ÏµúÏ†ÅÌôîÎêú Í≤ÄÏÉâ ÏøºÎ¶¨ ÏÉùÏÑ±
        optimized_queries = await self.generate_search_queries(intent_analysis)
        
        # 3Îã®Í≥Ñ: Ï†ïÎ≥¥ Î∂ÄÏ°± ÏòÅÏó≠ ÏãùÎ≥Ñ
        info_gaps = await self.information_gap_analyzer.analyze(user_query)
        
        # 4Îã®Í≥Ñ: ÏµúÏ†Å ÏóêÏù¥Ï†ÑÌä∏ Ï∂îÏ≤ú
        recommended_agent = await self.suggest_optimal_agent(intent_analysis)
        
        return SearchPlan(
            queries=optimized_queries,
            info_gaps=info_gaps,
            recommended_agent=recommended_agent
        )
```

### ‚úÖ Gemini 2.x Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÏóÖÍ∑∏Î†àÏù¥Îìú
```typescript
// ÏóÖÍ∑∏Î†àÏù¥ÎìúÎêú Î™®Îç∏ ÌÉÄÏûÖ ÏãúÏä§ÌÖú
type GeminiModelArchitecture = {
  "gemini-2.5-pro": {
    capabilities: ["reasoning", "multimodal", "analysis", "creative"],
    speed: "medium",
    use_cases: ["ÎåÄÏö©Îüâ Ïª®ÌÖçÏä§Ìä∏", "Î©ÄÌã∞Î™®Îã¨ ÏûëÏóÖ"],
    isRecommended: true
  },
  "gemini-2.5-flash": {
    capabilities: ["reasoning", "quick_tasks", "multimodal"], 
    speed: "fast",
    use_cases: ["Îπ†Î•∏ Ï∂îÎ°†", "Ïã§ÏãúÍ∞Ñ Ï≤òÎ¶¨"]
  },
  "gemini-2.0-pro": {
    capabilities: ["reasoning", "analysis", "multimodal"],
    speed: "medium", 
    use_cases: ["ÏïàÏ†ïÏ†ÅÏù∏ Í≥†ÏÑ±Îä• ÏûëÏóÖ"]
  },
  "gemini-2.0-flash": {
    capabilities: ["reasoning", "quick_tasks", "multimodal"],
    speed: "fast",
    use_cases: ["Îπ†Î•¥Í≥† Ìö®Ïú®Ï†ÅÏù∏ Ï≤òÎ¶¨"]
  }
}
```

### üîß Ïª¥Ìè¨ÎÑåÌä∏ ÏïÑÌÇ§ÌÖçÏ≤ò ÌòÅÏã†
- **Îã®Ïùº Ï±ÖÏûÑ ÏõêÏπô**: ChatInputÏù¥ Î™®Îì† AI ÏÑ§Ï†ï Îã¥Îãπ
- **ÏÉÅÌÉú ÎèôÍ∏∞Ìôî**: Provider-Model ÏûêÎèô Ïó∞Îèô Î°úÏßÅ
- **Î∞òÏùëÌòï ÏÑ§Í≥Ñ**: Î™®Î∞îÏùº/Îç∞Ïä§ÌÅ¨ÌÜ± ÌÜµÌï© ÎåÄÏùë
- **ÌÉÄÏûÖ ÏïàÏ†ÑÏÑ±**: ÏôÑÏ†ÑÌïú TypeScript ÌÉÄÏûÖ ÏãúÏä§ÌÖú

---

**Î¨∏ÏÑú Î≤ÑÏ†Ñ**: v1.1  
**ÏµúÏ¢Ö ÏóÖÎç∞Ïù¥Ìä∏**: 2025-08-19  
**ÏûëÏÑ±Ïûê**: AI Ìè¨ÌÉà ÏïÑÌÇ§ÌÖçÏ≤ò ÌåÄ  
**Í≤ÄÌÜ†Ïûê**: CTO

---

> üìö **Ï∞∏Í≥†ÏÇ¨Ìï≠**: Ïù¥ ÏïÑÌÇ§ÌÖçÏ≤ò Î¨∏ÏÑúÎäî develop.mdÏùò ÏÑ§Í≥Ñ Î™ÖÏÑ∏Î•º Íµ¨Ï≤¥Ï†ÅÏù∏ Íµ¨ÌòÑ Í∞ÄÏù¥ÎìúÎ°ú Î≥ÄÌôòÌïú Í≤ÉÏûÖÎãàÎã§. ÏµúÏã† Ïù∏ÎùºÏù∏ UI ÏãúÏä§ÌÖúÍ≥º Î©îÌÉÄ Í≤ÄÏÉâ ÏïÑÌÇ§ÌÖçÏ≤òÍ∞Ä Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§. Ïã§Ï†ú Íµ¨ÌòÑ Ïãú ÌîÑÎ°úÏ†ùÌä∏ ÏöîÍµ¨ÏÇ¨Ìï≠Í≥º ÌôòÍ≤ΩÏóê ÎßûÍ≤å Ï°∞Ï†ïÌïòÏó¨ ÏÇ¨Ïö©ÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.