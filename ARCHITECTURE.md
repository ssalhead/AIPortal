# AI í¬íƒˆ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ
## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë° êµ¬í˜„ ê°€ì´ë“œë¼ì¸

---

## ğŸ“Š ì‹œìŠ¤í…œ ê°œìš”

### ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™
- **ê²€ì¦ëœ ì•ˆì •ì„±**: SYGenai ìš´ì˜ í™˜ê²½ì—ì„œ ê²€ì¦ëœ íŒ¨í„´ í™œìš©
- **í™•ì¥ ê°€ëŠ¥ì„±**: ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ìš©ì´
- **ì„±ëŠ¥ ìµœì í™”**: Redis ì—†ì´ë„ ê³ ì„±ëŠ¥ì„ ë³´ì¥í•˜ëŠ” ìºì‹± ì „ëµ
- **ì‚¬ìš©ì ì¤‘ì‹¬**: ì§ê´€ì ì´ê³  ë°˜ì‘ì„± ìˆëŠ” ì‚¬ìš©ì ê²½í—˜

### ê¸°ìˆ  ìŠ¤íƒ ê°œìš”
```
â”Œâ”€ Frontend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ React 18+ â€¢ TypeScript â€¢ TailwindCSS â€¢ Zustand â€¢ SWR      â”‚
â”œâ”€ API Gateway â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FastAPI â€¢ Pydantic â€¢ Uvicorn â€¢ WebSocket                  â”‚
â”œâ”€ AI Agents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LangGraph â€¢ Claude (Bedrock) â€¢ Gemini (GenerativeAI)      â”‚
â”œâ”€ Data Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PostgreSQL â€¢ DynamoDB â€¢ OpenSearch â€¢ S3                   â”‚
â””â”€ Infrastructure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Docker â€¢ Kubernetes â€¢ GitHub Actions â€¢ AWS/GCP           â”‚
```

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°
```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[React UI]
        WS[WebSocket Client]
    end
    
    subgraph "API Gateway Layer"
        API[FastAPI Server]
        WSS[WebSocket Server]
        AUTH[Auth Middleware]
    end
    
    subgraph "AI Agent Layer"
        SUP[Supervisor Agent]
        ROUTE[LLM Router]
        WSA[Web Search Agent]
        RAG[Multimodal RAG Agent]
        CAN[Canvas Agent]
        T1A[Tier1 Domain Agents]
    end
    
    subgraph "Data Layer"
        PG[(PostgreSQL)]
        DDB[(DynamoDB)]
        OS[(OpenSearch)]
        S3[(S3 Storage)]
        CACHE[Cache Manager]
    end
    
    subgraph "External Services"
        BEDROCK[AWS Bedrock]
        GENAI[GCP GenerativeAI]
        MCP[MCP Servers]
    end
    
    UI --> API
    WS --> WSS
    API --> AUTH
    AUTH --> SUP
    SUP --> ROUTE
    ROUTE --> WSA
    ROUTE --> RAG
    ROUTE --> CAN
    ROUTE --> T1A
    
    WSA --> BEDROCK
    RAG --> GENAI
    T1A --> MCP
    
    SUP --> CACHE
    CACHE --> PG
    API --> DDB
    RAG --> OS
    API --> S3
```

### ë°ì´í„° í”Œë¡œìš°
```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant A as API Gateway
    participant S as Supervisor
    participant W as Worker Agent
    participant L as LLM
    participant D as Database
    
    U->>F: ì‚¬ìš©ì ì…ë ¥
    F->>A: API Request
    A->>S: ì—ì´ì „íŠ¸ ìš”ì²­
    S->>S: ì˜ë„ ë¶„ì„
    S->>W: Worker ì„ íƒ
    W->>L: LLM í˜¸ì¶œ
    L->>W: ì‘ë‹µ
    W->>D: ê²°ê³¼ ì €ì¥
    W->>S: ì‘ì—… ì™„ë£Œ
    S->>A: ì‘ë‹µ ì „ë‹¬
    A->>F: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    F->>U: ê²°ê³¼ í‘œì‹œ
```

---

## ğŸ¤– AI ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜

### LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ êµ¬ì¡°
```python
from langgraph.graph import StateGraph
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    current_agent: str
    context: dict
    user_profile: dict
    workspace_id: str
    artifacts: list

class SupervisorAgent:
    """ì¤‘ì•™ ì¡°ì • ì—ì´ì „íŠ¸ - ì˜ë„ ë¶„ì„ ë° Worker í• ë‹¹"""
    
    def __init__(self):
        self.llm_router = OptimizedLLMRouter()
        self.workers = {
            'web_search': WebSearchWorker(),
            'multimodal_rag': MultimodalRAGWorker(),
            'canvas': CanvasWorker(),
            'regulations': RegulationsWorker(),
            'legal': LegalWorker(),
            'accounting': AccountingWorker(),
            'tax': TaxWorker()
        }
        
        # LangGraph ìƒíƒœ ê·¸ë˜í”„ êµ¬ì„±
        self.workflow = StateGraph(AgentState)
        self._build_workflow()
    
    def _build_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        # ë…¸ë“œ ì¶”ê°€
        self.workflow.add_node("supervisor", self.supervisor_node)
        self.workflow.add_node("web_search", self.workers['web_search'].execute)
        self.workflow.add_node("multimodal_rag", self.workers['multimodal_rag'].execute)
        self.workflow.add_node("canvas", self.workers['canvas'].execute)
        
        # Tier1 ë„ë©”ì¸ ì—ì´ì „íŠ¸
        for agent_type in ['regulations', 'legal', 'accounting', 'tax']:
            self.workflow.add_node(agent_type, self.workers[agent_type].execute)
        
        # ì—£ì§€ ë° ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        self.workflow.set_entry_point("supervisor")
        self.workflow.add_conditional_edges(
            "supervisor",
            self.route_to_worker,
            {
                "web_search": "web_search",
                "multimodal_rag": "multimodal_rag", 
                "canvas": "canvas",
                "regulations": "regulations",
                "legal": "legal",
                "accounting": "accounting",
                "tax": "tax"
            }
        )
        
        # ì¢…ë£Œ ì¡°ê±´
        for worker in self.workers.keys():
            self.workflow.add_edge(worker, "__end__")
    
    async def supervisor_node(self, state: AgentState) -> AgentState:
        """Supervisor ì˜ì‚¬ê²°ì • ë…¸ë“œ"""
        last_message = state["messages"][-1]
        user_profile = state.get("user_profile", {})
        
        # ì˜ë„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        analysis_prompt = f"""
        ì‚¬ìš©ì ë©”ì‹œì§€: {last_message['content']}
        ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}
        
        ë‹¤ìŒ ì¤‘ ê°€ì¥ ì í•©í•œ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:
        - web_search: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
        - multimodal_rag: ì—…ë¡œë“œëœ íŒŒì¼ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
        - canvas: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì—ì„œ í˜‘ì—… ì‘ì—…
        - regulations: ê·œì • ë° ì»´í”Œë¼ì´ì–¸ìŠ¤ ë¬¸ì˜
        - legal: ë²•ë¬´ ê´€ë ¨ ë¬¸ì˜
        - accounting: íšŒê³„ ë° ì¬ë¬´ ë¶„ì„
        - tax: ì„¸ë¬´ ì²˜ë¦¬ ë° ì‹ ê³ 
        """
        
        # LLMìœ¼ë¡œ ì˜ë„ ë¶„ì„
        model = await self.llm_router.get_optimal_model("intent_analysis", len(analysis_prompt))
        response = await model.ainvoke(analysis_prompt)
        
        selected_agent = self._parse_agent_selection(response.content)
        state["current_agent"] = selected_agent
        
        return state
    
    def route_to_worker(self, state: AgentState) -> str:
        """ì›Œì»¤ ë¼ìš°íŒ… í•¨ìˆ˜"""
        return state["current_agent"]
```

### LLM ë¼ìš°íŒ… ì‹œìŠ¤í…œ
```python
from enum import Enum
from typing import Optional, Dict, Any
import asyncio
from anthropic import AsyncAnthropic
from google.generativeai import GenerativeModel
import google.generativeai as genai

class ModelType(Enum):
    CLAUDE_4_SONNET = "claude-4.0-sonnet"
    CLAUDE_37_SONNET = "claude-3.7-sonnet"
    CLAUDE_35_HAIKU = "claude-3.5-haiku"
    GEMINI_25_PRO = "gemini-2.5-pro"
    GEMINI_25_FLASH = "gemini-2.5-flash"
    GEMINI_20_FLASH = "gemini-2.0-flash"

class TaskType(Enum):
    COMPLEX_ANALYSIS = "complex_analysis"
    BALANCED_REASONING = "balanced_reasoning"
    QUICK_RESPONSE = "quick_response"
    MULTIMODAL = "multimodal"
    LARGE_CONTEXT = "large_context"

class OptimizedLLMRouter:
    """ìµœì í™”ëœ LLM ë¼ìš°íŒ… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.anthropic = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        
        # ëª¨ë¸ ì„±ëŠ¥ íŠ¹ì„± ì •ì˜
        self.model_specs = {
            ModelType.CLAUDE_4_SONNET: {
                "max_tokens": 200000,
                "cost_per_1k": 0.015,
                "speed_score": 7,
                "reasoning_score": 10,
                "multimodal": False
            },
            ModelType.CLAUDE_37_SONNET: {
                "max_tokens": 200000,
                "cost_per_1k": 0.010,
                "speed_score": 8,
                "reasoning_score": 9,
                "multimodal": False
            },
            ModelType.CLAUDE_35_HAIKU: {
                "max_tokens": 200000,
                "cost_per_1k": 0.005,
                "speed_score": 10,
                "reasoning_score": 7,
                "multimodal": False
            },
            ModelType.GEMINI_25_PRO: {
                "max_tokens": 2000000,
                "cost_per_1k": 0.007,
                "speed_score": 6,
                "reasoning_score": 9,
                "multimodal": True
            },
            ModelType.GEMINI_25_FLASH: {
                "max_tokens": 1000000,
                "cost_per_1k": 0.003,
                "speed_score": 9,
                "reasoning_score": 8,
                "multimodal": True
            },
            ModelType.GEMINI_20_FLASH: {
                "max_tokens": 1000000,
                "cost_per_1k": 0.002,
                "speed_score": 10,
                "reasoning_score": 7,
                "multimodal": True
            }
        }
        
        # ì‘ì—…ë³„ ìµœì  ëª¨ë¸ ë§¤í•‘
        self.task_model_mapping = {
            TaskType.COMPLEX_ANALYSIS: [
                ModelType.CLAUDE_4_SONNET,
                ModelType.GEMINI_25_PRO,
                ModelType.CLAUDE_37_SONNET
            ],
            TaskType.BALANCED_REASONING: [
                ModelType.CLAUDE_37_SONNET,
                ModelType.GEMINI_25_FLASH,
                ModelType.CLAUDE_35_HAIKU
            ],
            TaskType.QUICK_RESPONSE: [
                ModelType.CLAUDE_35_HAIKU,
                ModelType.GEMINI_20_FLASH,
                ModelType.GEMINI_25_FLASH
            ],
            TaskType.MULTIMODAL: [
                ModelType.GEMINI_25_PRO,
                ModelType.GEMINI_25_FLASH,
                ModelType.GEMINI_20_FLASH
            ],
            TaskType.LARGE_CONTEXT: [
                ModelType.GEMINI_25_PRO,
                ModelType.CLAUDE_4_SONNET,
                ModelType.CLAUDE_37_SONNET
            ]
        }
    
    async def get_optimal_model(self, task_type: str, context_length: int = 0, 
                              has_images: bool = False, priority: str = "balanced") -> Any:
        """ìµœì  ëª¨ë¸ ì„ íƒ ë° ë°˜í™˜"""
        
        # ì‘ì—… ìœ í˜• ê²°ì •
        if has_images:
            task_enum = TaskType.MULTIMODAL
        elif context_length > 100000:
            task_enum = TaskType.LARGE_CONTEXT
        elif task_type in ["complex_analysis", "deep_reasoning"]:
            task_enum = TaskType.COMPLEX_ANALYSIS
        elif task_type in ["quick_response", "simple_qa"]:
            task_enum = TaskType.QUICK_RESPONSE
        else:
            task_enum = TaskType.BALANCED_REASONING
        
        # í›„ë³´ ëª¨ë¸ ëª©ë¡
        candidate_models = self.task_model_mapping[task_enum]
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
        selected_model = self._select_by_priority(candidate_models, priority, context_length)
        
        # ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜
        return await self._get_model_client(selected_model)
    
    def _select_by_priority(self, candidates: list, priority: str, context_length: int) -> ModelType:
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ"""
        if priority == "cost":
            # ë¹„ìš© ìµœì í™”
            return min(candidates, key=lambda m: self.model_specs[m]["cost_per_1k"])
        elif priority == "speed":
            # ì†ë„ ìµœì í™”
            return max(candidates, key=lambda m: self.model_specs[m]["speed_score"])
        elif priority == "quality":
            # í’ˆì§ˆ ìµœì í™”
            return max(candidates, key=lambda m: self.model_specs[m]["reasoning_score"])
        else:
            # ê· í˜•ì¡íŒ ì„ íƒ (ê¸°ë³¸ê°’)
            return self._balanced_selection(candidates, context_length)
    
    def _balanced_selection(self, candidates: list, context_length: int) -> ModelType:
        """ê· í˜•ì¡íŒ ëª¨ë¸ ì„ íƒ ì•Œê³ ë¦¬ì¦˜"""
        scores = {}
        
        for model in candidates:
            spec = self.model_specs[model]
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì í•©ì„±
            context_score = 1.0 if context_length <= spec["max_tokens"] else 0.5
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì†ë„, í’ˆì§ˆ, ë¹„ìš©ì˜ ê°€ì¤‘í‰ê· )
            total_score = (
                spec["speed_score"] * 0.3 +
                spec["reasoning_score"] * 0.4 +
                (10 - spec["cost_per_1k"] * 100) * 0.2 +  # ë¹„ìš© ì—­ìˆ˜
                context_score * 0.1
            )
            
            scores[model] = total_score
        
        return max(scores, key=scores.get)
    
    async def _get_model_client(self, model_type: ModelType) -> Any:
        """ëª¨ë¸ë³„ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if model_type.value.startswith("claude"):
            return await self._get_claude_client(model_type)
        elif model_type.value.startswith("gemini"):
            return await self._get_gemini_client(model_type)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
    
    async def _get_claude_client(self, model_type: ModelType):
        """Claude ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸"""
        # AWS Bedrockì„ í†µí•œ Claude ì ‘ê·¼
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” boto3ì˜ bedrock-runtime í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        class ClaudeWrapper:
            def __init__(self, client, model_id):
                self.client = client
                self.model_id = model_id
            
            async def ainvoke(self, prompt: str, **kwargs):
                # Bedrock API í˜¸ì¶œ êµ¬í˜„
                pass
        
        return ClaudeWrapper(self.anthropic, model_type.value)
    
    async def _get_gemini_client(self, model_type: ModelType):
        """Gemini ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸"""
        model_id_mapping = {
            ModelType.GEMINI_25_PRO: "gemini-2.5-pro",
            ModelType.GEMINI_25_FLASH: "gemini-2.5-flash",
            ModelType.GEMINI_20_FLASH: "gemini-2.0-flash"
        }
        
        return GenerativeModel(model_id_mapping[model_type])
```

---

## ğŸ’¾ ë°ì´í„° ì•„í‚¤í…ì²˜

### PostgreSQL ìŠ¤í‚¤ë§ˆ ì„¤ê³„
```sql
-- í™•ì¥ ë° ê¸°ë³¸ ì„¤ì •
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";  -- í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ

-- ì‚¬ìš©ì ë° ì¸ì¦ (SYGenai í˜¸í™˜ í™•ì¥)
CREATE TABLE users (
    empno VARCHAR(20) PRIMARY KEY,
    name_ko VARCHAR(100) NOT NULL,
    company VARCHAR(100),
    company_code VARCHAR(10),
    roles JSONB DEFAULT '[]'::jsonb,
    permissions JSONB DEFAULT '{}'::jsonb,
    profile_data JSONB DEFAULT '{}'::jsonb,  -- ìë™ ìˆ˜ì§‘ëœ í”„ë¡œíŒŒì¼
    preferences JSONB DEFAULT '{}'::jsonb,   -- ì‚¬ìš©ì ì„¤ì •
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì‚¬ìš©ì ì¸ë±ìŠ¤
CREATE INDEX idx_users_company ON users(company_code);
CREATE INDEX idx_users_profile ON users USING GIN(profile_data);
CREATE INDEX idx_users_updated ON users(updated_at);

-- ìºì‹œ í…Œì´ë¸” (Redis ëŒ€ì²´)
CREATE TABLE cache_entries (
    key VARCHAR(255) PRIMARY KEY,
    data JSONB NOT NULL,
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
    tags TEXT[] DEFAULT '{}',  -- ìºì‹œ íƒœê¹…
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    accessed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    access_count INTEGER DEFAULT 0
);

-- ìºì‹œ ì¸ë±ìŠ¤ (ì„±ëŠ¥ ìµœì í™”)
CREATE INDEX idx_cache_expires ON cache_entries(expires_at);
CREATE INDEX idx_cache_tags ON cache_entries USING GIN(tags);
CREATE INDEX idx_cache_accessed ON cache_entries(accessed_at);

-- ìë™ ë§Œë£Œëœ ìºì‹œ ì •ë¦¬ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION cleanup_expired_cache()
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    DELETE FROM cache_entries WHERE expires_at <= NOW();
    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

-- ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë° í˜‘ì—…
CREATE TABLE workspaces (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    owner_empno VARCHAR(20) NOT NULL REFERENCES users(empno) ON DELETE CASCADE,
    title VARCHAR(200) NOT NULL,
    description TEXT,
    settings JSONB DEFAULT '{}'::jsonb,
    collaborators JSONB DEFAULT '[]'::jsonb,
    visibility VARCHAR(20) DEFAULT 'private' CHECK (visibility IN ('private', 'team', 'public')),
    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'archived', 'deleted')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì¸ë±ìŠ¤
CREATE INDEX idx_workspaces_owner ON workspaces(owner_empno);
CREATE INDEX idx_workspaces_status ON workspaces(status);
CREATE INDEX idx_workspaces_updated ON workspaces(updated_at);
CREATE INDEX idx_workspaces_collaborators ON workspaces USING GIN(collaborators);

-- ì•„í‹°íŒ©íŠ¸ (ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê²°ê³¼ë¬¼)
CREATE TABLE artifacts (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    workspace_id UUID NOT NULL REFERENCES workspaces(id) ON DELETE CASCADE,
    parent_id UUID REFERENCES artifacts(id),  -- ë²„ì „ ê´€ë¦¬ìš©
    type VARCHAR(50) NOT NULL,  -- 'document', 'code', 'chart', 'image', 'data'
    title VARCHAR(200),
    description TEXT,
    content JSONB NOT NULL,
    metadata JSONB DEFAULT '{}'::jsonb,
    version INTEGER DEFAULT 1,
    status VARCHAR(20) DEFAULT 'draft' CHECK (status IN ('draft', 'published', 'archived')),
    created_by VARCHAR(20) NOT NULL REFERENCES users(empno),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì•„í‹°íŒ©íŠ¸ ì¸ë±ìŠ¤
CREATE INDEX idx_artifacts_workspace ON artifacts(workspace_id);
CREATE INDEX idx_artifacts_type ON artifacts(type);
CREATE INDEX idx_artifacts_created_by ON artifacts(created_by);
CREATE INDEX idx_artifacts_parent ON artifacts(parent_id);
CREATE INDEX idx_artifacts_updated ON artifacts(updated_at);

-- ëŒ€í™” ë° ë©”ì‹œì§€ (ê¸°ë³¸ êµ¬ì¡°)
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_empno VARCHAR(20) NOT NULL REFERENCES users(empno),
    workspace_id UUID REFERENCES workspaces(id),
    title VARCHAR(200),
    context JSONB DEFAULT '{}'::jsonb,
    agent_config JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_conversations_user ON conversations(user_empno);
CREATE INDEX idx_conversations_workspace ON conversations(workspace_id);
CREATE INDEX idx_conversations_updated ON conversations(updated_at);

CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}'::jsonb,
    agent_type VARCHAR(50),
    model_used VARCHAR(50),
    tokens_used INTEGER,
    processing_time FLOAT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_messages_conversation ON messages(conversation_id);
CREATE INDEX idx_messages_created ON messages(created_at);
CREATE INDEX idx_messages_agent ON messages(agent_type);

-- Tier1 ì—ì´ì „íŠ¸ MCP ì„¤ì •
CREATE TABLE agent_mcp_configs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL UNIQUE,  -- 'regulations', 'legal', 'accounting', 'tax'
    name VARCHAR(100) NOT NULL,
    description TEXT,
    config_data JSONB NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_by VARCHAR(20) NOT NULL REFERENCES users(empno),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_agent_configs_type ON agent_mcp_configs(agent_type);
CREATE INDEX idx_agent_configs_active ON agent_mcp_configs(is_active);

-- ì‚¬ìš©ìë³„ ì—ì´ì „íŠ¸ ê¶Œí•œ
CREATE TABLE user_agent_permissions (
    empno VARCHAR(20) REFERENCES users(empno) ON DELETE CASCADE,
    agent_type VARCHAR(50),
    permissions JSONB DEFAULT '{}'::jsonb,
    granted_by VARCHAR(20) REFERENCES users(empno),
    granted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    expires_at TIMESTAMP WITH TIME ZONE,
    PRIMARY KEY (empno, agent_type)
);

CREATE INDEX idx_user_agent_perms_granted ON user_agent_permissions(granted_at);
CREATE INDEX idx_user_agent_perms_expires ON user_agent_permissions(expires_at);

-- ìƒˆë¯¸ GPT ê¸°ëŠ¥ ì„¤ì •
CREATE TABLE user_feature_preferences (
    empno VARCHAR(20) PRIMARY KEY REFERENCES users(empno) ON DELETE CASCADE,
    enable_deep_research BOOLEAN DEFAULT true,
    enable_web_search BOOLEAN DEFAULT true,
    enable_canvas_mode BOOLEAN DEFAULT true,
    enable_file_processing BOOLEAN DEFAULT true,
    model_preference JSONB DEFAULT '{}'::jsonb,  -- ì‚¬ìš©ìë³„ ëª¨ë¸ ì„ í˜¸ë„
    ui_preferences JSONB DEFAULT '{}'::jsonb,    -- UI ì„¤ì •
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìë™ ìˆ˜ì§‘ ë¡œê·¸
CREATE TABLE profile_collection_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    empno VARCHAR(20) NOT NULL REFERENCES users(empno) ON DELETE CASCADE,
    data_source VARCHAR(100) NOT NULL,  -- 'chat_interaction', 'file_upload', 'workspace_activity'
    collected_data JSONB NOT NULL,
    confidence_score FLOAT CHECK (confidence_score >= 0 AND confidence_score <= 1),
    processed BOOLEAN DEFAULT false,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_profile_logs_user ON profile_collection_logs(empno, created_at);
CREATE INDEX idx_profile_logs_source ON profile_collection_logs(data_source);
CREATE INDEX idx_profile_logs_processed ON profile_collection_logs(processed);

-- í™œë™ ë¡œê·¸ (í˜‘ì—… ì¶”ì )
CREATE TABLE activity_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    empno VARCHAR(20) NOT NULL REFERENCES users(empno),
    action VARCHAR(100) NOT NULL,
    resource_type VARCHAR(50),
    resource_id UUID,
    metadata JSONB DEFAULT '{}'::jsonb,
    ip_address INET,
    user_agent TEXT,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- í™œë™ ë¡œê·¸ íŒŒí‹°ì…”ë‹ (ì„±ëŠ¥ ìµœì í™”)
CREATE INDEX idx_activity_logs_user_time ON activity_logs(empno, timestamp);
CREATE INDEX idx_activity_logs_action ON activity_logs(action);
CREATE INDEX idx_activity_logs_resource ON activity_logs(resource_type, resource_id);

-- íŒŒì¼ ë° ì—…ë¡œë“œ ê´€ë¦¬
CREATE TABLE uploaded_files (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_empno VARCHAR(20) NOT NULL REFERENCES users(empno),
    workspace_id UUID REFERENCES workspaces(id),
    original_name VARCHAR(500) NOT NULL,
    stored_name VARCHAR(500) NOT NULL,
    file_type VARCHAR(100),
    file_size BIGINT,
    mime_type VARCHAR(200),
    storage_path TEXT NOT NULL,
    checksum VARCHAR(64),  -- SHA-256 í•´ì‹œ
    processing_status VARCHAR(20) DEFAULT 'pending' 
        CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed')),
    extracted_text TEXT,
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    processed_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_files_user ON uploaded_files(user_empno);
CREATE INDEX idx_files_workspace ON uploaded_files(workspace_id);
CREATE INDEX idx_files_status ON uploaded_files(processing_status);
CREATE INDEX idx_files_type ON uploaded_files(file_type);
CREATE INDEX idx_files_checksum ON uploaded_files(checksum);

-- íŠ¸ë¦¬ê±° í•¨ìˆ˜ë“¤ (ìë™ ì—…ë°ì´íŠ¸)
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- updated_at ìë™ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°
CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_workspaces_updated_at
    BEFORE UPDATE ON workspaces
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_artifacts_updated_at
    BEFORE UPDATE ON artifacts
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_conversations_updated_at
    BEFORE UPDATE ON conversations
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë·°
CREATE VIEW active_workspaces AS
SELECT w.*, u.name_ko as owner_name
FROM workspaces w
JOIN users u ON w.owner_empno = u.empno
WHERE w.status = 'active';

CREATE VIEW recent_activity AS
SELECT 
    al.*, 
    u.name_ko as user_name,
    CASE 
        WHEN al.resource_type = 'workspace' THEN w.title
        WHEN al.resource_type = 'artifact' THEN a.title
        ELSE NULL
    END as resource_name
FROM activity_logs al
JOIN users u ON al.empno = u.empno
LEFT JOIN workspaces w ON al.resource_type = 'workspace' AND al.resource_id = w.id
LEFT JOIN artifacts a ON al.resource_type = 'artifact' AND al.resource_id = a.id
WHERE al.timestamp >= NOW() - INTERVAL '7 days'
ORDER BY al.timestamp DESC;
```

### Redis ëŒ€ì²´ ìºì‹± ì‹œìŠ¤í…œ
```python
import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Any, Optional, Dict, List, Set
from collections import defaultdict
from cachetools import TTLCache
import asyncpg
from contextlib import asynccontextmanager

class OptimizedCacheManager:
    """Redis ì—†ì´ PostgreSQL + ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ìºì‹±"""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
        
        # L1 ìºì‹œ: ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”ëª¨ë¦¬ (ê°€ì¥ ë¹ ë¦„)
        self.memory_cache = TTLCache(maxsize=10000, ttl=300)  # 5ë¶„ TTL
        
        # Hot ë°ì´í„° ì‹ë³„ì„ ìœ„í•œ ì•¡ì„¸ìŠ¤ ì¶”ì 
        self.access_tracker = defaultdict(int)
        self.last_cleanup = time.time()
        
        # ìºì‹œ í†µê³„
        self.stats = {
            'l1_hits': 0,
            'l2_hits': 0,
            'misses': 0,
            'writes': 0
        }
    
    async def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ (L1 -> L2 ìˆœì„œ)"""
        
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
        if key in self.memory_cache:
            self.stats['l1_hits'] += 1
            self.access_tracker[key] += 1
            return self.memory_cache[key]
        
        # L2: PostgreSQL ìºì‹œ í…Œì´ë¸” í™•ì¸
        try:
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(
                    """
                    UPDATE cache_entries 
                    SET accessed_at = NOW(), access_count = access_count + 1
                    WHERE key = $1 AND expires_at > NOW()
                    RETURNING data, access_count
                    """,
                    key
                )
                
                if row:
                    data = row['data']
                    self.stats['l2_hits'] += 1
                    
                    # L1 ìºì‹œì— ìŠ¹ê²© (ìì£¼ ì•¡ì„¸ìŠ¤ë˜ëŠ” ë°ì´í„°)
                    if row['access_count'] > 3:
                        self.memory_cache[key] = data
                    
                    return data
        
        except Exception as e:
            # ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ì‹œì—ë„ ì„œë¹„ìŠ¤ ê³„ì† ì œê³µ
            print(f"Cache L2 error: {e}")
        
        self.stats['misses'] += 1
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 300, tags: List[str] = None) -> bool:
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        try:
            # L1: ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥ (TTL ì ìš©)
            self.memory_cache[key] = value
            
            # L2: PostgreSQL ì˜ì† ì €ì¥
            expires_at = datetime.utcnow() + timedelta(seconds=ttl)
            tags = tags or []
            
            async with self.db_pool.acquire() as conn:
                await conn.execute(
                    """
                    INSERT INTO cache_entries (key, data, expires_at, tags)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (key) DO UPDATE SET
                        data = EXCLUDED.data,
                        expires_at = EXCLUDED.expires_at,
                        tags = EXCLUDED.tags,
                        accessed_at = NOW()
                    """,
                    key, json.dumps(value), expires_at, tags
                )
            
            self.stats['writes'] += 1
            return True
            
        except Exception as e:
            print(f"Cache write error: {e}")
            return False
    
    async def invalidate(self, key: str) -> bool:
        """íŠ¹ì • í‚¤ ìºì‹œ ë¬´íš¨í™”"""
        # L1 ìºì‹œì—ì„œ ì œê±°
        self.memory_cache.pop(key, None)
        
        # L2 ìºì‹œì—ì„œ ì œê±°
        try:
            async with self.db_pool.acquire() as conn:
                result = await conn.execute(
                    "DELETE FROM cache_entries WHERE key = $1", key
                )
                return result == "DELETE 1"
        except Exception as e:
            print(f"Cache invalidation error: {e}")
            return False
    
    async def invalidate_by_tags(self, tags: List[str]) -> int:
        """íƒœê·¸ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        if not tags:
            return 0
            
        try:
            async with self.db_pool.acquire() as conn:
                # í•´ë‹¹ íƒœê·¸ë¥¼ ê°€ì§„ í‚¤ë“¤ ì¡°íšŒ
                rows = await conn.fetch(
                    "SELECT key FROM cache_entries WHERE tags && $1", tags
                )
                
                # L1 ìºì‹œì—ì„œ í•´ë‹¹ í‚¤ë“¤ ì œê±°
                for row in rows:
                    self.memory_cache.pop(row['key'], None)
                
                # L2 ìºì‹œì—ì„œ ì‚­ì œ
                result = await conn.execute(
                    "DELETE FROM cache_entries WHERE tags && $1", tags
                )
                
                return int(result.split()[-1])  # "DELETE N"ì—ì„œ N ì¶”ì¶œ
                
        except Exception as e:
            print(f"Cache tag invalidation error: {e}")
            return 0
    
    async def cleanup_expired(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        current_time = time.time()
        
        # 1ë¶„ì— í•œ ë²ˆë§Œ ì •ë¦¬ ì‹¤í–‰
        if current_time - self.last_cleanup < 60:
            return 0
        
        try:
            async with self.db_pool.acquire() as conn:
                deleted_count = await conn.fetchval(
                    "SELECT cleanup_expired_cache()"
                )
                
                self.last_cleanup = current_time
                return deleted_count or 0
                
        except Exception as e:
            print(f"Cache cleanup error: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = sum([
            self.stats['l1_hits'],
            self.stats['l2_hits'], 
            self.stats['misses']
        ])
        
        if total_requests == 0:
            return self.stats
        
        return {
            **self.stats,
            'hit_rate': (self.stats['l1_hits'] + self.stats['l2_hits']) / total_requests,
            'l1_hit_rate': self.stats['l1_hits'] / total_requests,
            'memory_cache_size': len(self.memory_cache),
            'memory_cache_maxsize': self.memory_cache.maxsize
        }
    
    async def warm_up(self, keys: List[str]) -> int:
        """ìºì‹œ ì›œì—… - ìì£¼ ì‚¬ìš©ë˜ëŠ” í‚¤ë“¤ì„ ë¯¸ë¦¬ L1 ìºì‹œë¡œ ë¡œë“œ"""
        warmed_count = 0
        
        try:
            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch(
                    """
                    SELECT key, data FROM cache_entries 
                    WHERE key = ANY($1) AND expires_at > NOW()
                    ORDER BY access_count DESC
                    """,
                    keys
                )
                
                for row in rows:
                    self.memory_cache[row['key']] = row['data']
                    warmed_count += 1
                    
        except Exception as e:
            print(f"Cache warmup error: {e}")
        
        return warmed_count

# WebSocket ìƒíƒœ ê´€ë¦¬
class WebSocketStateManager:
    """ì‹¤ì‹œê°„ í˜‘ì—…ì„ ìœ„í•œ WebSocket ìƒíƒœ ê´€ë¦¬"""
    
    def __init__(self, cache_manager: OptimizedCacheManager):
        self.cache = cache_manager
        self.active_connections: Dict[str, Set] = defaultdict(set)
        self.user_workspaces: Dict[str, Set[str]] = defaultdict(set)
    
    async def join_workspace(self, workspace_id: str, websocket, user_id: str):
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì°¸ì—¬"""
        self.active_connections[workspace_id].add(websocket)
        self.user_workspaces[user_id].add(workspace_id)
        
        # ì‚¬ìš©ì ìƒíƒœë¥¼ ìºì‹œì— ì €ì¥
        await self.cache.set(
            f"ws_user:{workspace_id}:{user_id}",
            {
                "status": "active",
                "joined_at": time.time(),
                "workspace_id": workspace_id
            },
            ttl=3600,  # 1ì‹œê°„
            tags=[f"workspace:{workspace_id}", f"user:{user_id}"]
        )
        
        # ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ì—ê²Œ ì°¸ì—¬ ì•Œë¦¼
        await self.broadcast_to_workspace(workspace_id, {
            "type": "user_joined",
            "user_id": user_id,
            "timestamp": time.time()
        }, exclude_user=user_id)
    
    async def leave_workspace(self, workspace_id: str, websocket, user_id: str):
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë– ë‚˜ê¸°"""
        self.active_connections[workspace_id].discard(websocket)
        self.user_workspaces[user_id].discard(workspace_id)
        
        # ì‚¬ìš©ì ìƒíƒœ ì œê±°
        await self.cache.invalidate(f"ws_user:{workspace_id}:{user_id}")
        
        # ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ì—ê²Œ ë– ë‚¨ ì•Œë¦¼
        await self.broadcast_to_workspace(workspace_id, {
            "type": "user_left", 
            "user_id": user_id,
            "timestamp": time.time()
        })
    
    async def broadcast_to_workspace(self, workspace_id: str, message: Dict, 
                                   exclude_user: str = None):
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë‚´ ëª¨ë“  ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        connections = self.active_connections.get(workspace_id, set()).copy()
        
        failed_connections = set()
        
        for websocket in connections:
            try:
                # exclude_user ì²´í¬ (websocketì— user_idê°€ ìˆë‹¤ë©´)
                if hasattr(websocket, 'user_id') and websocket.user_id == exclude_user:
                    continue
                    
                await websocket.send_json(message)
                
            except Exception as e:
                print(f"WebSocket send error: {e}")
                failed_connections.add(websocket)
        
        # ì‹¤íŒ¨í•œ ì—°ê²°ë“¤ ì •ë¦¬
        for failed_ws in failed_connections:
            self.active_connections[workspace_id].discard(failed_ws)
    
    async def get_active_users(self, workspace_id: str) -> List[Dict]:
        """ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í™œì„± ì‚¬ìš©ì ëª©ë¡"""
        try:
            # ìºì‹œì—ì„œ í™œì„± ì‚¬ìš©ì ì¡°íšŒ
            pattern_key = f"ws_user:{workspace_id}:*"
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redis SCANê³¼ ìœ ì‚¬í•œ ê¸°ëŠ¥ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „
            
            active_users = []
            for user_id in self.user_workspaces:
                if workspace_id in self.user_workspaces[user_id]:
                    user_data = await self.cache.get(f"ws_user:{workspace_id}:{user_id}")
                    if user_data:
                        active_users.append({
                            "user_id": user_id,
                            **user_data
                        })
            
            return active_users
            
        except Exception as e:
            print(f"Get active users error: {e}")
            return []
```

---

## ğŸ” ë³´ì•ˆ ì•„í‚¤í…ì²˜

### ì¸ì¦ ë° ê¶Œí•œ ê´€ë¦¬
```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from typing import List, Optional
import jwt
from datetime import datetime, timedelta

class SecurityManager:
    """í†µí•© ë³´ì•ˆ ê´€ë¦¬"""
    
    def __init__(self, secret_key: str, db_pool):
        self.secret_key = secret_key
        self.db_pool = db_pool
        self.security = HTTPBearer()
    
    async def create_access_token(self, user_data: dict, expires_delta: Optional[timedelta] = None):
        """JWT í† í° ìƒì„±"""
        to_encode = user_data.copy()
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(hours=24)
        
        to_encode.update({"exp": expire})
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm="HS256")
        return encoded_jwt
    
    async def verify_token(self, credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer())):
        """JWT í† í° ê²€ì¦"""
        try:
            payload = jwt.decode(credentials.credentials, self.secret_key, algorithms=["HS256"])
            empno = payload.get("empno")
            if empno is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid authentication credentials"
                )
            return payload
        except jwt.PyJWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid authentication credentials"
            )
    
    async def get_current_user(self, token_data: dict = Depends(verify_token)):
        """í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
        async with self.db_pool.acquire() as conn:
            user = await conn.fetchrow(
                "SELECT * FROM users WHERE empno = $1", token_data["empno"]
            )
            if not user:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="User not found"
                )
            return dict(user)
    
    async def check_permissions(self, user: dict, resource: str, action: str) -> bool:
        """ê¶Œí•œ í™•ì¸"""
        user_permissions = user.get('permissions', {})
        
        # ê´€ë¦¬ìëŠ” ëª¨ë“  ê¶Œí•œ
        if 'admin' in user.get('roles', []):
            return True
        
        # ë¦¬ì†ŒìŠ¤ë³„ ê¶Œí•œ í™•ì¸
        resource_permissions = user_permissions.get(resource, {})
        return resource_permissions.get(action, False)
    
    async def check_agent_access(self, user_empno: str, agent_type: str) -> bool:
        """ì—ì´ì „íŠ¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸"""
        async with self.db_pool.acquire() as conn:
            result = await conn.fetchrow(
                """
                SELECT permissions FROM user_agent_permissions 
                WHERE empno = $1 AND agent_type = $2 
                AND (expires_at IS NULL OR expires_at > NOW())
                """,
                user_empno, agent_type
            )
            
            return result is not None

# ì…ë ¥ ê²€ì¦
from pydantic import BaseModel, validator
from typing import List, Optional
import re

class ChatRequest(BaseModel):
    message: str
    agent_type: Optional[str] = "general"
    model_preference: Optional[str] = None
    workspace_id: Optional[str] = None
    files: Optional[List[str]] = None
    
    @validator('message')
    def validate_message(cls, v):
        if len(v.strip()) == 0:
            raise ValueError('Message cannot be empty')
        if len(v) > 10000:  # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
            raise ValueError('Message too long')
        
        # ì•…ì„± íŒ¨í„´ ê²€ì‚¬
        malicious_patterns = [
            r'<script.*?>.*?</script>',  # XSS ë°©ì§€
            r'javascript:',
            r'on\w+\s*=',  # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        ]
        
        for pattern in malicious_patterns:
            if re.search(pattern, v, re.IGNORECASE):
                raise ValueError('Malicious content detected')
        
        return v.strip()
    
    @validator('agent_type')
    def validate_agent_type(cls, v):
        allowed_agents = [
            'general', 'web_search', 'multimodal_rag', 
            'canvas', 'regulations', 'legal', 'accounting', 'tax'
        ]
        if v not in allowed_agents:
            raise ValueError(f'Invalid agent type. Must be one of: {allowed_agents}')
        return v

# Rate Limiting
from fastapi import Request
from collections import defaultdict
import time

class RateLimiter:
    """API Rate Limiting"""
    
    def __init__(self):
        self.requests = defaultdict(list)
        self.limits = {
            'default': {'requests': 100, 'window': 3600},  # 1ì‹œê°„ì— 100íšŒ
            'chat': {'requests': 50, 'window': 3600},      # 1ì‹œê°„ì— 50íšŒ ì±„íŒ…
            'upload': {'requests': 20, 'window': 3600},    # 1ì‹œê°„ì— 20íšŒ ì—…ë¡œë“œ
        }
    
    async def check_rate_limit(self, request: Request, endpoint: str = 'default') -> bool:
        """Rate limit í™•ì¸"""
        client_ip = request.client.host
        current_time = time.time()
        
        limit_config = self.limits.get(endpoint, self.limits['default'])
        window_size = limit_config['window']
        max_requests = limit_config['requests']
        
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìœˆë„ìš° ë‚´ ìš”ì²­ë“¤ë§Œ ìœ ì§€
        user_requests = self.requests[f"{client_ip}:{endpoint}"]
        self.requests[f"{client_ip}:{endpoint}"] = [
            req_time for req_time in user_requests 
            if current_time - req_time < window_size
        ]
        
        # í˜„ì¬ ìš”ì²­ ì¶”ê°€
        self.requests[f"{client_ip}:{endpoint}"].append(current_time)
        
        # Rate limit í™•ì¸
        return len(self.requests[f"{client_ip}:{endpoint}"]) <= max_requests

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware

def setup_security_middleware(app: FastAPI):
    """ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´ ì„¤ì •"""
    
    # CORS ì„¤ì •
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‹¤ì œ ë„ë©”ì¸
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE"],
        allow_headers=["*"],
    )
    
    # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í˜¸ìŠ¤íŠ¸ë§Œ í—ˆìš©
    app.add_middleware(
        TrustedHostMiddleware, 
        allowed_hosts=["localhost", "127.0.0.1", "*.yourdomain.com"]
    )
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
```python
import asyncpg
from typing import Optional
import json

class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”"""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
    
    async def optimize_queries(self):
        """ì¿¼ë¦¬ ìµœì í™” ì„¤ì •"""
        async with self.db_pool.acquire() as conn:
            # PostgreSQL ì„±ëŠ¥ ì„¤ì •
            await conn.execute("SET work_mem = '256MB'")
            await conn.execute("SET shared_buffers = '256MB'") 
            await conn.execute("SET effective_cache_size = '1GB'")
            await conn.execute("SET random_page_cost = 1.1")  # SSD ìµœì í™”
            
            # JSON ì¿¼ë¦¬ ìµœì í™”
            await conn.execute("SET enable_seqscan = off")  # ì¸ë±ìŠ¤ ì‚¬ìš© ê°•ì œ
    
    async def create_optimal_indexes(self):
        """ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„±"""
        indexes = [
            # ë³µí•© ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_messages_conv_created ON messages(conversation_id, created_at DESC)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_artifacts_workspace_updated ON artifacts(workspace_id, updated_at DESC)",
            
            # ë¶€ë¶„ ì¸ë±ìŠ¤ (ì¡°ê±´ë¶€)
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_active_workspaces ON workspaces(owner_empno) WHERE status = 'active'",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_recent_activity ON activity_logs(empno, timestamp) WHERE timestamp >= NOW() - INTERVAL '30 days'",
            
            # JSON ì¸ë±ìŠ¤ (JSONB í•„ë“œìš©)
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_profile_skills ON users USING GIN((profile_data->'skills'))",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_artifact_metadata ON artifacts USING GIN(metadata)",
            
            # ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_workspace_title_search ON workspaces USING GIN(to_tsvector('english', title))",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_artifact_content_search ON artifacts USING GIN(to_tsvector('english', coalesce(title, '') || ' ' || coalesce(description, '')))",
        ]
        
        async with self.db_pool.acquire() as conn:
            for index_sql in indexes:
                try:
                    await conn.execute(index_sql)
                except Exception as e:
                    print(f"Index creation failed: {e}")
    
    async def analyze_query_performance(self, query: str, params: tuple = None) -> dict:
        """ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„"""
        async with self.db_pool.acquire() as conn:
            # EXPLAIN ANALYZE ì‹¤í–‰
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}"
            
            if params:
                result = await conn.fetchval(explain_query, *params)
            else:
                result = await conn.fetchval(explain_query)
            
            return result[0]  # JSON ê²°ê³¼ ë°˜í™˜

# ì—°ê²° í’€ ìµœì í™”
async def create_optimized_pool():
    """ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€"""
    return await asyncpg.create_pool(
        host="localhost",
        port=5432,
        user="postgres", 
        password="password",
        database="ai_portal",
        min_size=10,           # ìµœì†Œ ì—°ê²° ìˆ˜
        max_size=50,           # ìµœëŒ€ ì—°ê²° ìˆ˜
        max_queries=5000,      # ì—°ê²°ë‹¹ ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜
        max_inactive_connection_lifetime=300.0,  # ë¹„í™œì„± ì—°ê²° ìˆ˜ëª… (5ë¶„)
        command_timeout=60,    # ëª…ë ¹ íƒ€ì„ì•„ì›ƒ
        server_settings={
            'application_name': 'ai_portal',
            'jit': 'off',      # JIT ì»´íŒŒì¼ ë¹„í™œì„±í™” (ì‘ì€ ì¿¼ë¦¬ì— ì˜¤ë²„í—¤ë“œ)
        }
    )

# API ì‘ë‹µ ìµœì í™”
from fastapi import BackgroundTasks
import asyncio

class ResponseOptimizer:
    """API ì‘ë‹µ ìµœì í™”"""
    
    @staticmethod
    async def batch_database_operations(operations: list):
        """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ë°°ì¹˜ ì²˜ë¦¬"""
        # ì—¬ëŸ¬ ì‘ì—…ì„ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë¬¶ì–´ ì²˜ë¦¬
        async with db_pool.acquire() as conn:
            async with conn.transaction():
                results = []
                for operation in operations:
                    result = await operation(conn)
                    results.append(result)
                return results
    
    @staticmethod
    async def parallel_data_fetch(fetchers: list):
        """ë³‘ë ¬ ë°ì´í„° ì¡°íšŒ"""
        # ë…ë¦½ì ì¸ ë°ì´í„° ì¡°íšŒ ì‘ì—…ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        tasks = [asyncio.create_task(fetcher()) for fetcher in fetchers]
        return await asyncio.gather(*tasks)
    
    @staticmethod
    def background_task_handler(background_tasks: BackgroundTasks, task_func, *args, **kwargs):
        """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì²˜ë¦¬"""
        background_tasks.add_task(task_func, *args, **kwargs)

# í”„ë¡ íŠ¸ì—”ë“œ ì„±ëŠ¥ ìµœì í™”
class FrontendOptimizer:
    """í”„ë¡ íŠ¸ì—”ë“œ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ"""
    
    @staticmethod
    def get_optimization_config():
        return {
            # React ìµœì í™”
            "react": {
                "lazy_loading": "React.lazy() ì‚¬ìš©ìœ¼ë¡œ ì»´í¬ë„ŒíŠ¸ ì§€ì—° ë¡œë”©",
                "memoization": "React.memo, useMemo, useCallback ì ì ˆíˆ í™œìš©",
                "virtualization": "react-windowë¡œ ê¸´ ë¦¬ìŠ¤íŠ¸ ê°€ìƒí™”",
                "code_splitting": "Dynamic importë¡œ ë²ˆë“¤ ë¶„í• "
            },
            
            # ìƒíƒœ ê´€ë¦¬ ìµœì í™”
            "state_management": {
                "zustand_slices": "Zustand ìŠ¤í† ì–´ë¥¼ ê¸°ëŠ¥ë³„ë¡œ ë¶„í• ",
                "selective_subscription": "í•„ìš”í•œ ìƒíƒœë§Œ êµ¬ë…",
                "computed_values": "íŒŒìƒ ìƒíƒœëŠ” selectorë¡œ ê³„ì‚°",
                "persistence": "ì¤‘ìš”í•œ ìƒíƒœë§Œ localStorageì— ì €ì¥"
            },
            
            # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
            "network": {
                "swr_caching": "SWRë¡œ ë°ì´í„° ìºì‹± ë° ì¬ê²€ì¦",
                "request_deduplication": "ë™ì¼ ìš”ì²­ ì¤‘ë³µ ì œê±°",
                "optimistic_updates": "ë‚™ê´€ì  ì—…ë°ì´íŠ¸ë¡œ UX ê°œì„ ",
                "websocket_management": "WebSocket ì—°ê²° íš¨ìœ¨ì  ê´€ë¦¬"
            },
            
            # ë¹Œë“œ ìµœì í™”
            "build": {
                "tree_shaking": "ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì½”ë“œ ì œê±°",
                "asset_optimization": "ì´ë¯¸ì§€, í°íŠ¸ ìµœì í™”",
                "service_worker": "ìºì‹± ì „ëµìœ¼ë¡œ ë¡œë”© ì„±ëŠ¥ í–¥ìƒ",
                "cdn": "ì •ì  ìì‚° CDN ë°°í¬"
            }
        }
```

---

## ğŸ”„ ë°°í¬ ë° ìš´ì˜

### Docker ìµœì í™”
```dockerfile
# í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™” Dockerfile
FROM node:18-alpine AS frontend-build

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# íŒ¨í‚¤ì§€ íŒŒì¼ ë³µì‚¬ (ìºì‹œ ìµœì í™”)
COPY package*.json ./
RUN npm ci --only=production

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬
COPY . .

# í”„ë¡œë•ì…˜ ë¹Œë“œ
RUN npm run build

# í”„ë¡œë•ì…˜ ì´ë¯¸ì§€
FROM nginx:alpine AS production
COPY --from=frontend-build /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]

---

# ë°±ì—”ë“œ ìµœì í™” Dockerfile
FROM python:3.11-slim AS backend-build

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Python ì˜ì¡´ì„± ì„¤ì¹˜ (ìºì‹œ ìµœì í™”)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í”„ë¡œë•ì…˜ ì„œë²„ ì‹¤í–‰
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Kubernetes ë°°í¬ ì„¤ì •
```yaml
# ai-portal-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-portal-backend
  labels:
    app: ai-portal-backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-portal-backend
  template:
    metadata:
      labels:
        app: ai-portal-backend
    spec:
      containers:
      - name: backend
        image: ai-portal/backend:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ai-portal-secrets
              key: database-url
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-portal-secrets
              key: anthropic-key
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: ai-portal-backend-service
spec:
  selector:
    app: ai-portal-backend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-portal-frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ai-portal-frontend
  template:
    metadata:
      labels:
        app: ai-portal-frontend
    spec:
      containers:
      - name: frontend
        image: ai-portal/frontend:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ai-portal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - ai-portal.yourdomain.com
    secretName: ai-portal-tls
  rules:
  - host: ai-portal.yourdomain.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: ai-portal-backend-service
            port:
              number: 80
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ai-portal-frontend-service
            port:
              number: 80
```

### CI/CD íŒŒì´í”„ë¼ì¸
```yaml
# .github/workflows/deploy.yml
name: AI Portal CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install backend dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Run backend tests
      run: |
        cd backend
        pytest tests/ -v --cov=app --cov-report=xml
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test:ci
    
    - name: Build frontend
      run: |
        cd frontend
        npm run build

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: security-scan-results.sarif

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push backend image
      uses: docker/build-push-action@v4
      with:
        context: ./backend
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/backend:latest
    
    - name: Build and push frontend image
      uses: docker/build-push-action@v4
      with:
        context: ./frontend
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:latest

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Kubernetes
      uses: azure/k8s-deploy@v1
      with:
        manifests: |
          k8s/ai-portal-deployment.yaml
          k8s/ai-portal-service.yaml
          k8s/ai-portal-ingress.yaml
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ê°€ëŠ¥ì„±

### Langsmith í†µí•©
```python
from langsmith import traceable, Client
from typing import Dict, Any
import os

class LangsmithTracker:
    """Langsmith í†µí•© ì¶”ì """
    
    def __init__(self):
        self.client = Client(
            api_url="https://api.smith.langchain.com",
            api_key=os.getenv("LANGSMITH_API_KEY")
        )
    
    @traceable(name="agent_execution")
    async def track_agent_execution(self, agent_type: str, input_data: Dict, 
                                  user_id: str, workspace_id: str = None) -> Dict:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¶”ì """
        
        metadata = {
            "agent_type": agent_type,
            "user_id": user_id,
            "workspace_id": workspace_id,
            "timestamp": time.time()
        }
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰ ë¡œì§
        result = await self._execute_agent(agent_type, input_data)
        
        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        metadata.update({
            "execution_time": result.get("execution_time"),
            "model_used": result.get("model_used"),
            "tokens_used": result.get("tokens_used"),
            "success": result.get("success", False)
        })
        
        return {
            **result,
            "trace_metadata": metadata
        }
    
    @traceable(name="llm_call")
    async def track_llm_call(self, model: str, prompt: str, response: str, 
                           tokens_used: int, execution_time: float):
        """LLM í˜¸ì¶œ ì¶”ì """
        return {
            "model": model,
            "prompt_length": len(prompt),
            "response_length": len(response), 
            "tokens_used": tokens_used,
            "execution_time": execution_time,
            "cost_estimate": self._calculate_cost(model, tokens_used)
        }
    
    def _calculate_cost(self, model: str, tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        cost_per_token = {
            "claude-4.0-sonnet": 0.000015,
            "claude-3.7-sonnet": 0.000010,
            "claude-3.5-haiku": 0.000005,
            "gemini-2.5-pro": 0.000007,
            "gemini-2.5-flash": 0.000003,
            "gemini-2.0-flash": 0.000002
        }
        return tokens * cost_per_token.get(model, 0.00001)
```

### ì„±ëŠ¥ ë©”íŠ¸ë¦­
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Request, Response
import time

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'websocket_active_connections',
    'Active WebSocket connections',
    ['workspace_id']
)

AGENT_EXECUTIONS = Counter(
    'agent_executions_total',
    'Total agent executions',
    ['agent_type', 'status']
)

LLM_TOKEN_USAGE = Counter(
    'llm_tokens_used_total',
    'Total LLM tokens used',
    ['model', 'agent_type']
)

CACHE_OPERATIONS = Counter(
    'cache_operations_total',
    'Cache operations',
    ['operation', 'level', 'status']
)

class MetricsMiddleware:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¯¸ë“¤ì›¨ì–´"""
    
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            request = Request(scope, receive)
            start_time = time.time()
            
            # ì‘ë‹µ ì²˜ë¦¬
            async def send_wrapper(message):
                if message["type"] == "http.response.start":
                    status_code = message["status"]
                    duration = time.time() - start_time
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    REQUEST_COUNT.labels(
                        method=request.method,
                        endpoint=request.url.path,
                        status=status_code
                    ).inc()
                    
                    REQUEST_DURATION.labels(
                        method=request.method,
                        endpoint=request.url.path
                    ).observe(duration)
                
                await send(message)
            
            await self.app(scope, receive, send_wrapper)
        else:
            await self.app(scope, receive, send)

# ëŒ€ì‹œë³´ë“œ ì„¤ì •
GRAFANA_DASHBOARD_CONFIG = {
    "dashboard": {
        "title": "AI Portal Monitoring",
        "panels": [
            {
                "title": "Request Rate",
                "type": "graph",
                "targets": [
                    {
                        "expr": "rate(http_requests_total[5m])",
                        "legendFormat": "{{method}} {{endpoint}}"
                    }
                ]
            },
            {
                "title": "Response Time",
                "type": "graph", 
                "targets": [
                    {
                        "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                        "legendFormat": "95th percentile"
                    }
                ]
            },
            {
                "title": "Agent Performance",
                "type": "table",
                "targets": [
                    {
                        "expr": "rate(agent_executions_total[1h])",
                        "legendFormat": "{{agent_type}}"
                    }
                ]
            },
            {
                "title": "LLM Token Usage",
                "type": "pie",
                "targets": [
                    {
                        "expr": "sum by (model) (llm_tokens_used_total)",
                        "legendFormat": "{{model}}"
                    }
                ]
            }
        ]
    }
}
```

---

## ğŸš€ **ìµœì‹  ì•„í‚¤í…ì²˜ ê°œì„  (2025-08-19)**

### âœ… ì¸ë¼ì¸ UI ì•„í‚¤í…ì²˜ í˜ì‹ 
```typescript
// ìƒˆë¡œìš´ í†µí•© ChatInput ì•„í‚¤í…ì²˜
interface ChatInputArchitecture {
  // íŒì—… ê¸°ë°˜ â†’ ì¸ë¼ì¸ í†µí•© ì„¤ê³„
  popup_elimination: {
    before: "ë³„ë„ PopupAISettings ì»´í¬ë„ŒíŠ¸",
    after: "ChatInput ë‚´ë¶€ í†µí•© ë“œë¡­ë‹¤ìš´",
    benefits: ["UX ë‹¨ìˆœí™”", "ìƒíƒœ ê´€ë¦¬ ìµœì í™”", "ì ‘ê·¼ì„± í–¥ìƒ"]
  },
  
  // ëª¨ë¸ ì„ íƒ ì•„í‚¤í…ì²˜  
  model_selection: {
    component: "InlineModelDropdown",
    positioning: "bottom-full mb-2 (ìƒí–¥ ì—´ë¦¼)",
    providers: ["Claude (Star ì•„ì´ì½˜)", "Gemini (Zap ì•„ì´ì½˜)"],
    models: "8ê°œ í†µí•© (4 + 4)",
    responsive: "isMobile ê¸°ë°˜ ì ì‘í˜• í¬ê¸°"
  },
  
  // ê¸°ëŠ¥ í† ê¸€ ì‹œìŠ¤í…œ
  feature_toggles: {
    layout: "inline horizontal",
    position: "íŒŒì¼ ì²¨ë¶€ ì•„ì´ì½˜ ì˜†",
    buttons: ["ì›¹ ê²€ìƒ‰ ğŸ”", "ì‹¬ì¸µ ë¦¬ì„œì¹˜ ğŸ“Š", "Canvas ğŸ¨"],
    interaction: "ë‹¨ì¼ ì„ íƒ + ì¬í´ë¦­ í•´ì œ",
    styling: "ë°˜ì‘í˜• í…ìŠ¤íŠ¸ ë¼ë²¨"
  }
}
```

### âœ… ë©”íƒ€ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
```python
# 2ë‹¨ê³„ ë©”íƒ€ ê²€ìƒ‰ ì²´ì¸ ì„¤ê³„
class MetaSearchArchitecture:
    def __init__(self):
        self.conversation_context_service = ConversationContextService()
        self.information_gap_analyzer = InformationGapAnalyzer() 
        self.agent_suggestion_modal = AgentSuggestionModal()
    
    async def meta_search_pipeline(self, user_query: str, context: Dict):
        # 1ë‹¨ê³„: ì‚¬ìš©ì ì˜ë„ + ëŒ€í™” ë§¥ë½ ë¶„ì„
        intent_analysis = await self.analyze_user_intent(user_query, context)
        
        # 2ë‹¨ê³„: ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        optimized_queries = await self.generate_search_queries(intent_analysis)
        
        # 3ë‹¨ê³„: ì •ë³´ ë¶€ì¡± ì˜ì—­ ì‹ë³„
        info_gaps = await self.information_gap_analyzer.analyze(user_query)
        
        # 4ë‹¨ê³„: ìµœì  ì—ì´ì „íŠ¸ ì¶”ì²œ
        recommended_agent = await self.suggest_optimal_agent(intent_analysis)
        
        return SearchPlan(
            queries=optimized_queries,
            info_gaps=info_gaps,
            recommended_agent=recommended_agent
        )
```

### âœ… Gemini 2.x ëª¨ë¸ ì•„í‚¤í…ì²˜ ì—…ê·¸ë ˆì´ë“œ
```typescript
// ì—…ê·¸ë ˆì´ë“œëœ ëª¨ë¸ íƒ€ì… ì‹œìŠ¤í…œ
type GeminiModelArchitecture = {
  "gemini-2.5-pro": {
    capabilities: ["reasoning", "multimodal", "analysis", "creative"],
    speed: "medium",
    use_cases: ["ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸", "ë©€í‹°ëª¨ë‹¬ ì‘ì—…"],
    isRecommended: true
  },
  "gemini-2.5-flash": {
    capabilities: ["reasoning", "quick_tasks", "multimodal"], 
    speed: "fast",
    use_cases: ["ë¹ ë¥¸ ì¶”ë¡ ", "ì‹¤ì‹œê°„ ì²˜ë¦¬"]
  },
  "gemini-2.0-pro": {
    capabilities: ["reasoning", "analysis", "multimodal"],
    speed: "medium", 
    use_cases: ["ì•ˆì •ì ì¸ ê³ ì„±ëŠ¥ ì‘ì—…"]
  },
  "gemini-2.0-flash": {
    capabilities: ["reasoning", "quick_tasks", "multimodal"],
    speed: "fast",
    use_cases: ["ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì²˜ë¦¬"]
  }
}
```

### ğŸ”§ ì»´í¬ë„ŒíŠ¸ ì•„í‚¤í…ì²˜ í˜ì‹ 
- **ë‹¨ì¼ ì±…ì„ ì›ì¹™**: ChatInputì´ ëª¨ë“  AI ì„¤ì • ë‹´ë‹¹
- **ìƒíƒœ ë™ê¸°í™”**: Provider-Model ìë™ ì—°ë™ ë¡œì§
- **ë°˜ì‘í˜• ì„¤ê³„**: ëª¨ë°”ì¼/ë°ìŠ¤í¬í†± í†µí•© ëŒ€ì‘
- **íƒ€ì… ì•ˆì „ì„±**: ì™„ì „í•œ TypeScript íƒ€ì… ì‹œìŠ¤í…œ

---

**ë¬¸ì„œ ë²„ì „**: v1.1  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-08-19  
**ì‘ì„±ì**: AI í¬íƒˆ ì•„í‚¤í…ì²˜ íŒ€  
**ê²€í† ì**: CTO

---

> ğŸ“š **ì°¸ê³ ì‚¬í•­**: ì´ ì•„í‚¤í…ì²˜ ë¬¸ì„œëŠ” develop.mdì˜ ì„¤ê³„ ëª…ì„¸ë¥¼ êµ¬ì²´ì ì¸ êµ¬í˜„ ê°€ì´ë“œë¡œ ë³€í™˜í•œ ê²ƒì…ë‹ˆë‹¤. ìµœì‹  ì¸ë¼ì¸ UI ì‹œìŠ¤í…œê³¼ ë©”íƒ€ ê²€ìƒ‰ ì•„í‚¤í…ì²˜ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤ì œ êµ¬í˜„ ì‹œ í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ê³¼ í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•˜ì—¬ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.