"""
ê²€ìƒ‰ ì„œë¹„ìŠ¤ - ì›¹ ê²€ìƒ‰ ë° ê²°ê³¼ ìºì‹±
"""

import asyncio
import hashlib
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from urllib.parse import quote_plus
from dataclasses import dataclass, field
from enum import Enum
import httpx
from bs4 import BeautifulSoup
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.repositories.cache import CacheRepository
from app.services.cache_manager import cache_manager
from app.agents.llm_router import llm_router


class SearchType(Enum):
    """ê²€ìƒ‰ íƒ€ì…"""
    WEB = "web"
    NEWS = "news" 
    ACADEMIC = "academic"
    TECHNICAL = "technical"
    GOVERNMENT = "government"
    SHOPPING = "shopping"


class SearchIntent(Enum):
    """ê²€ìƒ‰ ì˜ë„"""
    INFORMATION = "information"  # ì •ë³´ ê²€ìƒ‰
    RECENT = "recent"           # ìµœì‹  ì •ë³´
    COMPARISON = "comparison"    # ë¹„êµ
    TUTORIAL = "tutorial"       # ì‚¬ìš©ë²•/ë°©ë²•
    DEFINITION = "definition"    # ì •ì˜
    NEWS = "news"              # ë‰´ìŠ¤
    ACADEMIC = "academic"       # í•™ìˆ 


@dataclass
class GoogleSearchParameters:
    """Google Custom Search API ê³ ê¸‰ íŒŒë¼ë¯¸í„°"""
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    query: str
    num: int = 10
    start: int = 1
    hl: str = "ko"  # ì¸í„°í˜ì´ìŠ¤ ì–¸ì–´
    
    # ì§€ì—­/ì–¸ì–´ ì„¤ì •
    gl: Optional[str] = "KR"  # ì§€ì—­ ì„¤ì • (KR=í•œêµ­)
    cr: Optional[str] = "countryKR"  # êµ­ê°€ ì œí•œ
    lr: Optional[str] = "lang_ko"  # ì–¸ì–´ ì œí•œ
    
    # ì‹œê°„ ê¸°ë°˜ í•„í„°ë§
    dateRestrict: Optional[str] = None  # d[1-365], w[1-52], m[1-12], y[1-10]
    sort: Optional[str] = None  # date, date-sdate:d:w, date-sdate:d:s
    
    # ì •ë°€ ê²€ìƒ‰
    exactTerms: Optional[str] = None  # ì •í™•í•œ êµ¬ë¬¸
    excludeTerms: Optional[str] = None  # ì œì™¸í•  ë‹¨ì–´
    orTerms: Optional[str] = None  # OR ê²€ìƒ‰
    
    # ì‚¬ì´íŠ¸/URL í•„í„°ë§
    siteSearch: Optional[str] = None  # íŠ¹ì • ì‚¬ì´íŠ¸ ê²€ìƒ‰
    siteSearchFilter: Optional[str] = "i"  # i=í¬í•¨, e=ì œì™¸
    
    # ì½˜í…ì¸  íƒ€ì…
    fileType: Optional[str] = None  # pdf, doc, ppt ë“±
    searchType: Optional[str] = None  # image, news
    
    # ê¶Œí•œ/ë¼ì´ì„ ìŠ¤
    rights: Optional[str] = None  # cc_publicdomain, cc_attribute ë“±
    
    # ê³ ê¸‰ ê²€ìƒ‰
    linkSite: Optional[str] = None  # ë§í¬í•˜ëŠ” ì‚¬ì´íŠ¸
    relatedSite: Optional[str] = None  # ê´€ë ¨ ì‚¬ì´íŠ¸
    
    # ì•ˆì „ ê²€ìƒ‰
    safe: str = "medium"  # off, medium, high
    
    def to_params(self) -> Dict[str, str]:
        """API íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜"""
        params = {
            "q": self.query,
            "num": str(self.num),
            "start": str(self.start),
            "hl": self.hl,
            "safe": self.safe
        }
        
        # ì˜µì…”ë„ íŒŒë¼ë¯¸í„° ì¶”ê°€
        optional_fields = [
            "gl", "cr", "lr", "dateRestrict", "sort", 
            "exactTerms", "excludeTerms", "orTerms",
            "siteSearch", "siteSearchFilter", "fileType", 
            "searchType", "rights", "linkSite", "relatedSite"
        ]
        
        for field in optional_fields:
            value = getattr(self, field)
            if value is not None:
                params[field] = str(value)
        
        return params


class QueryAnalyzer:
    """ê²€ìƒ‰ ì¿¼ë¦¬ ë¶„ì„ê¸° - í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ìë™ ì„¤ì •"""
    
    def __init__(self):
        # ì‹œê°„ í‚¤ì›Œë“œ ë§¤í•‘
        self.time_keywords = {
            "ì˜¤ëŠ˜": "d1",
            "ì–´ì œ": "d2", 
            "ì´ë²ˆì£¼": "w1",
            "ì§€ë‚œì£¼": "w2",
            "ì´ë²ˆë‹¬": "m1",
            "ì§€ë‚œë‹¬": "m2",
            "ì˜¬í•´": "y1",
            "ì‘ë…„": "y2",
            "ìµœì‹ ": "m1",
            "recent": "m1",
            "latest": "d7",
            "today": "d1",
            "yesterday": "d2",
            "week": "w1",
            "month": "m1"
        }
        
        # íŒŒì¼ íƒ€ì… í‚¤ì›Œë“œ
        self.filetype_keywords = {
            "pdf": "pdf",
            "ë¬¸ì„œ": "pdf",
            "ë…¼ë¬¸": "pdf", 
            "ë³´ê³ ì„œ": "pdf",
            "ppt": "ppt",
            "í”„ë ˆì  í…Œì´ì…˜": "ppt",
            "ìŠ¬ë¼ì´ë“œ": "ppt",
            "ì—‘ì…€": "xls",
            "spreadsheet": "xls"
        }
        
        # ë„ë©”ì¸ë³„ ì‚¬ì´íŠ¸
        self.domain_sites = {
            "ë‰´ìŠ¤": "news.naver.com OR news.daum.net OR news.joins.com",
            "ìœ„í‚¤í”¼ë””ì•„": "wikipedia.org",
            "ê¹ƒí—ˆë¸Œ": "github.com", 
            "ìŠ¤íƒì˜¤ë²„í”Œë¡œ": "stackoverflow.com",
            "ì •ë¶€": ".go.kr",
            "ëŒ€í•™": ".ac.kr",
            "ë…¼ë¬¸": "scholar.google.com OR arxiv.org"
        }
        
        # ì–¸ì–´ ê°ì§€ íŒ¨í„´
        self.korean_pattern = re.compile(r'[ê°€-í£]')
        self.english_pattern = re.compile(r'[a-zA-Z]')
    
    def analyze_query(self, query: str, search_type: SearchType = SearchType.WEB) -> GoogleSearchParameters:
        """ì¿¼ë¦¬ ë¶„ì„í•˜ì—¬ ìµœì  íŒŒë¼ë¯¸í„° ìƒì„±"""
        
        params = GoogleSearchParameters(query=query)
        query_lower = query.lower()
        
        # 1. ì‹œê°„ ê¸°ë°˜ ë¶„ì„
        date_restrict = self._analyze_time_keywords(query_lower)
        if date_restrict:
            params.dateRestrict = date_restrict
            if any(word in query_lower for word in ["ë‰´ìŠ¤", "news", "ìµœì‹ ", "latest"]):
                params.sort = "date"
        
        # 2. íŒŒì¼ íƒ€ì… ë¶„ì„
        file_type = self._analyze_filetype_keywords(query_lower)
        if file_type:
            params.fileType = file_type
        
        # 3. ë„ë©”ì¸ ë¶„ì„
        site_search = self._analyze_domain_keywords(query_lower)
        if site_search:
            params.siteSearch = site_search
        
        # 4. ê²€ìƒ‰ íƒ€ì…ë³„ ìµœì í™”
        self._optimize_by_search_type(params, search_type, query_lower)
        
        # 5. ì–¸ì–´/ì§€ì—­ ìµœì í™”
        self._optimize_language_region(params, query)
        
        # 6. í˜„ì¬ ì—°ë„ ìë™ ì¶”ê°€ (í•˜ë“œì½”ë”© ë¬¸ì œ í•´ê²°)
        current_year = datetime.now().year
        if any(word in query_lower for word in ["ìµœì‹ ", "í˜„ì¬", "ì˜¤ëŠ˜", "ì´ë²ˆë…„", str(current_year-1)]):
            # ì‘ë…„ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í˜„ì¬ ì—°ë„ë¥¼ exactTermsì— ì¶”ê°€
            if params.exactTerms:
                params.exactTerms += f" {current_year}"
            else:
                params.exactTerms = str(current_year)
        
        return params
    
    def _analyze_time_keywords(self, query: str) -> Optional[str]:
        """ì‹œê°„ í‚¤ì›Œë“œ ë¶„ì„"""
        for keyword, date_restrict in self.time_keywords.items():
            if keyword in query:
                return date_restrict
        return None
    
    def _analyze_filetype_keywords(self, query: str) -> Optional[str]:
        """íŒŒì¼ íƒ€ì… í‚¤ì›Œë“œ ë¶„ì„"""
        for keyword, filetype in self.filetype_keywords.items():
            if keyword in query:
                return filetype
        return None
    
    def _analyze_domain_keywords(self, query: str) -> Optional[str]:
        """ë„ë©”ì¸ í‚¤ì›Œë“œ ë¶„ì„"""
        for keyword, sites in self.domain_sites.items():
            if keyword in query:
                return sites
        return None
    
    def _optimize_by_search_type(self, params: GoogleSearchParameters, search_type: SearchType, query: str):
        """ê²€ìƒ‰ íƒ€ì…ë³„ ìµœì í™”"""
        
        if search_type == SearchType.NEWS:
            params.searchType = "news"
            params.sort = "date"
            params.dateRestrict = params.dateRestrict or "m1"
            
        elif search_type == SearchType.ACADEMIC:
            params.siteSearch = "scholar.google.com OR arxiv.org OR researchgate.net"
            params.fileType = "pdf"
            params.rights = "cc_publicdomain"
            
        elif search_type == SearchType.TECHNICAL:
            params.siteSearch = "github.com OR stackoverflow.com OR docs.python.org"
            
        elif search_type == SearchType.GOVERNMENT:
            params.siteSearch = ".go.kr OR .gov"
            params.cr = "countryKR"
            
        elif search_type == SearchType.SHOPPING:
            params.siteSearch = "shopping.naver.com OR gmarket.co.kr OR coupang.com"
            params.dateRestrict = "m3"  # 3ê°œì›” ì´ë‚´
    
    def _optimize_language_region(self, params: GoogleSearchParameters, query: str):
        """ì–¸ì–´/ì§€ì—­ ìµœì í™”"""
        
        korean_chars = len(self.korean_pattern.findall(query))
        english_chars = len(self.english_pattern.findall(query))
        
        if korean_chars > english_chars:
            # í•œêµ­ì–´ ì¤‘ì‹¬ ê²€ìƒ‰
            params.gl = "KR"
            params.cr = "countryKR" 
            params.lr = "lang_ko"
        elif english_chars > korean_chars * 2:
            # ì˜ì–´ ì¤‘ì‹¬ ê²€ìƒ‰
            params.gl = "US"
            params.cr = None
            params.lr = "lang_en"
        else:
            # í˜¼í•© ê²€ìƒ‰ - í•œêµ­ì–´ ìš°ì„ 
            params.gl = "KR"
            params.lr = None  # ì–¸ì–´ ì œí•œ í•´ì œ


@dataclass
class CandidateSite:
    """ë©”íƒ€ ê²€ìƒ‰ í›„ë³´ ì‚¬ì´íŠ¸"""
    domain: str
    reason: str
    confidence: float
    search_method: str  # "api", "site_operator", "crawling"
    specific_pages: List[str] = field(default_factory=list)
    has_search_api: bool = False
    has_search_url: bool = True


class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        title: str,
        url: str,
        snippet: str,
        source: str = "web",
        score: float = 0.0,
        timestamp: str = None
    ):
        self.title = title
        self.url = url
        self.snippet = snippet
        self.source = source
        self.score = score
        self.timestamp = timestamp or datetime.utcnow().isoformat()
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "title": self.title,
            "url": self.url,
            "snippet": self.snippet,
            "source": self.source,
            "score": self.score,
            "timestamp": self.timestamp
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "SearchResult":
        return cls(
            title=data.get("title", ""),
            url=data.get("url", ""),
            snippet=data.get("snippet", ""),
            source=data.get("source", "web"),
            score=data.get("score", 0.0),
            timestamp=data.get("timestamp")
        )


class BalancedSearchStrategy:
    """ê· í˜•ì¡íŒ ê²€ìƒ‰ ì „ëµ í´ë˜ìŠ¤"""
    
    def __init__(self, search_service):
        self.search_service = search_service
        self.category_weight = 0.4      # ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ë¹„ì¤‘
        self.general_weight = 0.6       # ì¼ë°˜ ê²€ìƒ‰ ë¹„ì¤‘
        self.max_category_results = 4   # ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ ê²°ê³¼
        self.max_general_results = 6    # ì¼ë°˜ ê²€ìƒ‰ ìµœëŒ€ ê²°ê³¼
        self.confidence_threshold = 0.3 # ì¹´í…Œê³ ë¦¬ ê°ì§€ ì„ê³„ê°’
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ ì‚¬ì´íŠ¸ ë§¤í•‘
        self.category_sites = {
            "business": {
                "ì±„ìš©": "saramin.co.kr OR jobkorea.co.kr OR wanted.co.kr OR rocketpunch.com",
                "ê¸°ì—…ì •ë³´": "jobplanet.co.kr OR dart.fss.or.kr OR crunchbase.com OR kind.or.kr", 
                "ìŠ¤íƒ€íŠ¸ì—…": "rocketpunch.com OR platum.kr OR venturesquare.net OR thevc.kr",
                "B2Bì†”ë£¨ì…˜": "gobizkorea.com OR kotra.or.kr OR kita.net",
                "í”„ë¦¬ëœì‹±": "kmong.com OR soomgo.com OR wishket.com OR taling.me"
            },
            "research": {
                "ë…¼ë¬¸": "scholar.google.com OR arxiv.org OR riss.kr OR dbpia.co.kr OR kiss.kstudy.com",
                "íŠ¹í—ˆ": "kipris.or.kr OR patents.google.com OR wipo.int",
                "ì—°êµ¬ê³¼ì œ": "ntis.go.kr OR kistep.re.kr OR nrf.re.kr",
                "ê¸°ìˆ ì •ë³´": "kosen21.org OR keit.re.kr OR kist.re.kr OR etri.re.kr",
                "í‘œì¤€": "kats.go.kr OR ks.go.kr OR iso.org OR iec.ch",
                "ì—°êµ¬ê¸°ê´€": "kist.re.kr OR etri.re.kr OR kaist.ac.kr OR kriss.re.kr"
            },
            "trade": {
                "ìˆ˜ì¶œì…": "kotra.or.kr OR kita.net OR unipass.go.kr OR ktdb.go.kr",
                "ê´€ì„¸": "customs.go.kr OR unipass.go.kr OR ktdb.go.kr",
                "ë¬´ì—­í†µê³„": "kita.net OR kotis.net OR trademap.org",
                "FTA": "fta.go.kr OR kotra.or.kr",
                "í•´ì™¸ì§„ì¶œ": "kotra.or.kr OR koreaexim.go.kr OR k-sure.or.kr",
                "ì›ì‚°ì§€": "fta.go.kr OR customs.go.kr"
            }
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.category_keywords = {
            "research": ["ë…¼ë¬¸", "ì—°êµ¬", "íŠ¹í—ˆ", "ê¸°ìˆ ê°œë°œ", "R&D", "í‘œì¤€", "ê°œë°œ", "ì‹¤í—˜", "ë¶„ì„", "í•™ìˆ "],
            "trade": ["ìˆ˜ì¶œ", "ìˆ˜ì…", "ë¬´ì—­", "ê´€ì„¸", "FTA", "ì›ì‚°ì§€", "í•´ì™¸ì§„ì¶œ", "êµ­ì œ", "í†µê´€", "ìˆ˜ì¶œì…"],
            "business": ["íšŒì‚¬", "ê¸°ì—…", "ì±„ìš©", "ì·¨ì—…", "ë¹„ì¦ˆë‹ˆìŠ¤", "ìŠ¤íƒ€íŠ¸ì—…", "ë©´ì ‘", "ì—°ë´‰", "ì´ë ¥ì„œ", "êµ¬ì¸"]
        }
    
    def detect_category_with_confidence(self, query: str) -> Tuple[Optional[str], float]:
        """ì¹´í…Œê³ ë¦¬ ê°ì§€ (ë³´ìˆ˜ì  ì ‘ê·¼)"""
        category_scores = {}
        query_lower = query.lower()
        
        for category, keywords in self.category_keywords.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            matches = sum(1 for kw in keywords if kw in query_lower)
            if matches > 0:
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ / ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ì˜ ë¹„ìœ¨ë¡œ ì ìˆ˜ ê³„ì‚°
                category_scores[category] = min(matches / 3, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
        
        # ìµœê³  ì ìˆ˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜ (ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ)
        if category_scores and max(category_scores.values()) >= self.confidence_threshold:
            best_category = max(category_scores.items(), key=lambda x: x[1])
            return best_category
        else:
            return None, 0.0
    
    def _get_category_sites(self, category: str, query: str) -> str:
        """ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ì‚¬ì´íŠ¸ ì„ íƒ"""
        if category not in self.category_sites:
            return ""
        
        # ì¿¼ë¦¬ì— ê°€ì¥ ì í•©í•œ ì„œë¸Œì¹´í…Œê³ ë¦¬ ì°¾ê¸°
        query_lower = query.lower()
        category_mapping = self.category_sites[category]
        
        for subcategory, sites in category_mapping.items():
            subcategory_keywords = {
                "ì±„ìš©": ["ì±„ìš©", "êµ¬ì¸", "ì·¨ì—…", "ë©´ì ‘", "ì´ë ¥ì„œ", "job"],
                "ê¸°ì—…ì •ë³´": ["íšŒì‚¬", "ê¸°ì—…", "ì •ë³´", "ì†Œê°œ", "ì—°í˜"],
                "ë…¼ë¬¸": ["ë…¼ë¬¸", "ì—°êµ¬", "í•™ìˆ ", "ì €ë„", "paper"],
                "íŠ¹í—ˆ": ["íŠ¹í—ˆ", "patent", "ë°œëª…", "ì§€ì‹ì¬ì‚°"],
                "ìˆ˜ì¶œì…": ["ìˆ˜ì¶œ", "ìˆ˜ì…", "export", "import"],
                "ê´€ì„¸": ["ê´€ì„¸", "ì„¸ê¸ˆ", "tax", "duty"]
            }
            
            if subcategory in subcategory_keywords:
                keywords = subcategory_keywords[subcategory]
                if any(keyword in query_lower for keyword in keywords):
                    return sites
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        return list(category_mapping.values())[0]
    
    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return url
    
    def _ensure_diversity(self, results: List[SearchResult]) -> List[SearchResult]:
        """ë„ë©”ì¸ ë‹¤ì–‘ì„± ë³´ì¥ (ê°™ì€ ë„ë©”ì¸ 3ê°œ ì´ìƒ ë°©ì§€)"""
        domain_counts = {}
        diverse_results = []
        
        for result in results:
            domain = self._extract_domain(result.url)
            if domain_counts.get(domain, 0) < 3:  # ë„ë©”ì¸ë‹¹ ìµœëŒ€ 3ê°œ
                diverse_results.append(result)
                domain_counts[domain] = domain_counts.get(domain, 0) + 1
        
        return diverse_results
    
    async def execute_balanced_search(
        self, 
        query: str, 
        category: str, 
        max_results: int = 10,
        session = None
    ) -> List[SearchResult]:
        """ê· í˜•ì¡íŒ ê²€ìƒ‰ ì‹¤í–‰"""
        
        # 1. ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        general_task = asyncio.create_task(
            self.search_service._execute_general_search(
                query, self.max_general_results, session
            )
        )
        
        category_task = asyncio.create_task(
            self._execute_category_search(
                query, category, self.max_category_results, session
            )
        )
        
        try:
            general_results, category_results = await asyncio.gather(
                general_task, category_task, return_exceptions=True
            )
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            if isinstance(general_results, Exception):
                print(f"ì¼ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: {general_results}")
                general_results = []
            if isinstance(category_results, Exception):
                print(f"ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨: {category_results}")
                category_results = []
                
        except Exception as e:
            print(f"ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # Fallback: ì¼ë°˜ ê²€ìƒ‰ë§Œ ì‹¤í–‰
            general_results = await self.search_service._execute_general_search(
                query, max_results, session
            )
            category_results = []
        
        # 2. ê²°ê³¼ í†µí•© ë° ê· í˜• ì¡°ì •
        balanced_results = self._merge_and_balance(
            general_results, category_results, query
        )
        
        return balanced_results[:max_results]
    
    async def _execute_category_search(
        self, 
        query: str, 
        category: str, 
        max_results: int,
        session = None
    ) -> List[SearchResult]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ ê²€ìƒ‰ ì‹¤í–‰"""
        
        # ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ì‚¬ì´íŠ¸ ê²€ìƒ‰
        category_sites = self._get_category_sites(category, query)
        if not category_sites:
            return []
        
        # ì‚¬ì´íŠ¸ ì œí•œ ê²€ìƒ‰ ì‹¤í–‰
        search_params = GoogleSearchParameters(
            query=query,
            num=max_results,
            siteSearch=category_sites
        )
        
        return await self.search_service.search_google(
            query=query,
            max_results=max_results,
            search_type=SearchType.WEB,
            session=session,
            **{"custom_params": search_params}
        )
    
    def _merge_and_balance(
        self, 
        general_results: List[SearchResult], 
        category_results: List[SearchResult], 
        query: str
    ) -> List[SearchResult]:
        """ì¼ë°˜ ê²€ìƒ‰ê³¼ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê· í˜•ìˆê²Œ í†µí•©"""
        
        # 1. ì¤‘ë³µ URL ì œê±° (ë„ë©”ì¸ ê¸°ì¤€)
        all_results = {}
        
        # 2. ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ìš°ì„  ì¶”ê°€ (ê¸°ë³¸ ë‹¤ì–‘ì„± í™•ë³´)
        for result in general_results[:self.max_general_results]:
            domain = self._extract_domain(result.url)
            if domain not in all_results:
                # SearchResult ê°ì²´ì— ì¶”ê°€ ì†ì„± ì„¤ì •
                result.source_type = "general"
                result.boost_score = 1.0  # ê¸°ë³¸ ì ìˆ˜
                all_results[domain] = result
        
        # 3. ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì „ë¬¸ì„± ê°•í™”)
        category_boost = 1.2  # ì¹´í…Œê³ ë¦¬ ê²°ê³¼ì— ì•½ê°„ì˜ ë¶€ìŠ¤íŠ¸
        added_category_count = 0
        
        for result in category_results:
            domain = self._extract_domain(result.url)
            if domain not in all_results and added_category_count < self.max_category_results:
                result.source_type = "category"
                result.boost_score = category_boost
                all_results[domain] = result
                added_category_count += 1
            elif domain in all_results:
                # ì´ë¯¸ ìˆëŠ” ê²°ê³¼ë©´ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ë¡œ ì—…ê·¸ë ˆì´ë“œ
                existing = all_results[domain]
                existing.boost_score = max(getattr(existing, 'boost_score', 1.0), category_boost)
                existing.source_type = "mixed"
        
        # 4. ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        final_results = list(all_results.values())
        for result in final_results:
            boost_score = getattr(result, 'boost_score', 1.0)
            result.final_score = result.score * boost_score
        
        final_results.sort(key=lambda x: getattr(x, 'final_score', x.score), reverse=True)
        
        # 5. ë‹¤ì–‘ì„± ê²€ì¦
        return self._ensure_diversity(final_results)


class MetaSearchStrategy:
    """2ë‹¨ê³„ ë©”íƒ€ ê²€ìƒ‰ ì „ëµ í´ë˜ìŠ¤"""
    
    def __init__(self, search_service):
        self.search_service = search_service
        self.meta_weight = 0.5      # ë©”íƒ€ ê²€ìƒ‰ ë¹„ì¤‘ 50%
        self.general_weight = 0.5   # ì¼ë°˜ ê²€ìƒ‰ ë¹„ì¤‘ 50%
        self.max_meta_results = 5   # ë©”íƒ€ ê²€ìƒ‰ ìµœëŒ€ ê²°ê³¼
        self.max_general_results = 5 # ì¼ë°˜ ê²€ìƒ‰ ìµœëŒ€ ê²°ê³¼
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ì¶”ì²œ ì‚¬ì´íŠ¸
        self.category_default_sites = {
            "programming": [
                CandidateSite("stackoverflow.com", "í”„ë¡œê·¸ë˜ë° Q&A ì „ë¬¸", 0.9, "site_operator"),
                CandidateSite("github.com", "ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œ ì €ì¥ì†Œ", 0.9, "site_operator"),
                CandidateSite("dev.to", "ê°œë°œì ì»¤ë®¤ë‹ˆí‹°", 0.8, "site_operator"),
                CandidateSite("medium.com", "ê¸°ìˆ  ë¸”ë¡œê·¸ í”Œë«í¼", 0.7, "site_operator")
            ],
            "business": [
                CandidateSite("linkedin.com", "ë¹„ì¦ˆë‹ˆìŠ¤ ë„¤íŠ¸ì›Œí¬", 0.8, "site_operator"),
                CandidateSite("harvard.edu", "í•˜ë²„ë“œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ë·°", 0.9, "site_operator"),
                CandidateSite("mckinsey.com", "ê²½ì˜ ì»¨ì„¤íŒ…", 0.8, "site_operator")
            ],
            "research": [
                CandidateSite("scholar.google.com", "í•™ìˆ  ê²€ìƒ‰", 0.9, "site_operator"),
                CandidateSite("arxiv.org", "ë…¼ë¬¸ ì €ì¥ì†Œ", 0.9, "site_operator"),
                CandidateSite("researchgate.net", "ì—°êµ¬ì ë„¤íŠ¸ì›Œí¬", 0.8, "site_operator")
            ],
            "news": [
                CandidateSite("bbc.com", "êµ­ì œ ë‰´ìŠ¤", 0.8, "site_operator"),
                CandidateSite("reuters.com", "í†µì‹ ì‚¬", 0.9, "site_operator"),
                CandidateSite("news.naver.com", "í•œêµ­ ë‰´ìŠ¤", 0.8, "site_operator")
            ]
        }
    
    async def execute_meta_search(
        self, 
        query: str, 
        max_results: int = 10,
        session = None
    ) -> List[SearchResult]:
        """2ë‹¨ê³„ ë©”íƒ€ ê²€ìƒ‰ ì‹¤í–‰"""
        
        print(f"ğŸ¯ ë©”íƒ€ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # 1ë‹¨ê³„: ì í•©í•œ ì‚¬ì´íŠ¸ ë°œê²¬
        candidate_sites = await self._discover_relevant_sites(query)
        print(f"ğŸ” í›„ë³´ ì‚¬ì´íŠ¸ ë°œê²¬: {len(candidate_sites)}ê°œ")
        
        # 2ë‹¨ê³„: ê° ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰ ì‹¤í–‰
        meta_results = await self._search_within_sites(query, candidate_sites, session)
        print(f"ğŸ¯ ë©”íƒ€ ê²€ìƒ‰ ê²°ê³¼: {len(meta_results)}ê°œ")
        
        # 3ë‹¨ê³„: ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ë„ ê°€ì ¸ì˜¤ê¸°
        general_results = await self.search_service._execute_general_search(
            query, self.max_general_results, session
        )
        print(f"ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼: {len(general_results)}ê°œ")
        
        # 4ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ìˆœìœ„í™”
        final_results = self._merge_meta_results(meta_results, general_results, query)
        
        print(f"ğŸ”€ ìµœì¢… í†µí•© ê²°ê³¼: {len(final_results)}ê°œ")
        return final_results[:max_results]
    
    async def _discover_relevant_sites(self, query: str) -> List[CandidateSite]:
        """ê²€ìƒ‰ì–´ì— ì í•©í•œ ì‚¬ì´íŠ¸ ë°œê²¬"""
        all_candidates = []
        
        # ë°©ë²• 1: LLM ê¸°ë°˜ ì‚¬ì´íŠ¸ ì¶”ì²œ
        try:
            llm_recommendations = await self._get_llm_site_recommendations(query)
            all_candidates.extend(llm_recommendations)
            print(f"ğŸ’¡ LLM ì¶”ì²œ ì‚¬ì´íŠ¸: {len(llm_recommendations)}ê°œ")
        except Exception as e:
            print(f"âŒ LLM ì¶”ì²œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê¸°ë³¸ ì‚¬ì´íŠ¸
        category_sites = self._get_category_default_sites(query)
        all_candidates.extend(category_sites)
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ê¸°ë³¸ ì‚¬ì´íŠ¸: {len(category_sites)}ê°œ")
        
        # ë°©ë²• 3: ì‚¬ì´íŠ¸ ë°œê²¬ ê²€ìƒ‰ (í–¥í›„ êµ¬í˜„)
        # discovery_sites = await self._search_for_sites(query)
        # all_candidates.extend(discovery_sites)
        
        # ì¤‘ë³µ ì œê±° ë° ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬
        unique_sites = self._deduplicate_sites(all_candidates)
        return sorted(unique_sites, key=lambda x: x.confidence, reverse=True)[:8]
    
    async def _get_llm_site_recommendations(self, query: str) -> List[CandidateSite]:
        """LLMì„ í™œìš©í•œ ì í•©í•œ ì‚¬ì´íŠ¸ ì¶”ì²œ"""
        
        prompt = f"""
ë‹¤ìŒ ê²€ìƒ‰ì–´ì— ê°€ì¥ ì í•©í•œ ì›¹ì‚¬ì´íŠ¸ 5ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”: "{query}"

ê³ ë ¤ì‚¬í•­:
1. ê²€ìƒ‰ì–´ì˜ ì£¼ì œì™€ ì„±ê²© ë¶„ì„
2. í•´ë‹¹ ì£¼ì œì— ì „ë¬¸í™”ëœ ì‚¬ì´íŠ¸
3. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì›
4. í•œêµ­ì–´/ì˜ì–´ ì‚¬ì´íŠ¸ ëª¨ë‘ ê³ ë ¤

JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "recommended_sites": [
    {{
      "domain": "example.com",
      "reason": "ì¶”ì²œ ì´ìœ  (í•œ ì¤„ë¡œ)",
      "confidence": 0.9,
      "search_method": "site_operator"
    }}
  ]
}}
"""
        
        try:
            response, _ = await llm_router.generate_response("gemini", prompt)
            return self._parse_llm_recommendations(response)
        except Exception as e:
            print(f"LLM ì‚¬ì´íŠ¸ ì¶”ì²œ ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_llm_recommendations(self, response: str) -> List[CandidateSite]:
        """LLM ì‘ë‹µì—ì„œ ì‚¬ì´íŠ¸ ì¶”ì²œ íŒŒì‹±"""
        try:
            # JSON í˜•íƒœë¡œ íŒŒì‹±
            clean_response = response.strip()
            if clean_response.startswith('```json'):
                clean_response = clean_response[7:]
            if clean_response.endswith('```'):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            data = json.loads(clean_response)
            candidates = []
            
            for site_data in data.get("recommended_sites", []):
                candidate = CandidateSite(
                    domain=site_data.get("domain", ""),
                    reason=site_data.get("reason", ""),
                    confidence=site_data.get("confidence", 0.5),
                    search_method=site_data.get("search_method", "site_operator"),
                    specific_pages=site_data.get("specific_pages", [])
                )
                candidates.append(candidate)
            
            return candidates
            
        except json.JSONDecodeError as e:
            print(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            print(f"LLM ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_category_default_sites(self, query: str) -> List[CandidateSite]:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê¸°ë³¸ ì‚¬ì´íŠ¸ ì„ íƒ"""
        query_lower = query.lower()
        selected_sites = []
        
        # í”„ë¡œê·¸ë˜ë° ê´€ë ¨
        programming_keywords = ["python", "javascript", "react", "ì½”ë”©", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ", "api"]
        if any(keyword in query_lower for keyword in programming_keywords):
            selected_sites.extend(self.category_default_sites.get("programming", []))
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨
        business_keywords = ["ë¹„ì¦ˆë‹ˆìŠ¤", "ê²½ì˜", "ë§ˆì¼€íŒ…", "ì „ëµ", "business", "startup", "íšŒì‚¬"]
        if any(keyword in query_lower for keyword in business_keywords):
            selected_sites.extend(self.category_default_sites.get("business", []))
        
        # ì—°êµ¬ ê´€ë ¨
        research_keywords = ["ë…¼ë¬¸", "ì—°êµ¬", "í•™ìˆ ", "research", "study", "academic"]
        if any(keyword in query_lower for keyword in research_keywords):
            selected_sites.extend(self.category_default_sites.get("research", []))
        
        # ë‰´ìŠ¤ ê´€ë ¨
        news_keywords = ["ë‰´ìŠ¤", "ì†Œì‹", "í˜„ì¬", "ìµœê·¼", "news", "latest"]
        if any(keyword in query_lower for keyword in news_keywords):
            selected_sites.extend(self.category_default_sites.get("news", []))
        
        return selected_sites
    
    def _deduplicate_sites(self, sites: List[CandidateSite]) -> List[CandidateSite]:
        """ì‚¬ì´íŠ¸ ì¤‘ë³µ ì œê±°"""
        seen_domains = set()
        unique_sites = []
        
        for site in sites:
            if site.domain not in seen_domains:
                seen_domains.add(site.domain)
                unique_sites.append(site)
        
        return unique_sites
    
    async def _search_within_sites(
        self, 
        query: str, 
        sites: List[CandidateSite],
        session = None
    ) -> List[SearchResult]:
        """ê° ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰ ì‹¤í–‰"""
        
        search_tasks = []
        for site in sites[:5]:  # ìµœëŒ€ 5ê°œ ì‚¬ì´íŠ¸
            if site.search_method == "site_operator":
                task = self._search_via_site_operator(query, site, session)
            elif site.search_method == "crawling":
                task = self._search_via_crawling(query, site, session)
            else:
                task = self._search_via_site_operator(query, site, session)  # ê¸°ë³¸ê°’
            
            search_tasks.append(task)
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
        all_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨ [{sites[i].domain}]: {result}")
            elif isinstance(result, list):
                all_results.extend(result)
                print(f"âœ… ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì„±ê³µ [{sites[i].domain}]: {len(result)}ê°œ ê²°ê³¼")
        
        return all_results
    
    async def _search_via_site_operator(
        self, 
        query: str, 
        site: CandidateSite,
        session = None
    ) -> List[SearchResult]:
        """Google site: ì—°ì‚°ì í™œìš© ê²€ìƒ‰"""
        
        site_query = f"site:{site.domain} {query}"
        
        try:
            results = await self.search_service.search_google(
                site_query, 
                max_results=3, 
                search_type=SearchType.WEB,
                session=session
            )
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for result in results:
                result.source = f"meta_{site.domain}"
                result.meta_site = site.domain
                result.meta_confidence = site.confidence
                result.meta_reason = site.reason
            
            return results
            
        except Exception as e:
            print(f"ì‚¬ì´íŠ¸ ì—°ì‚°ì ê²€ìƒ‰ ì‹¤íŒ¨ [{site.domain}]: {e}")
            return []
    
    async def _search_via_crawling(
        self, 
        query: str, 
        site: CandidateSite,
        session = None
    ) -> List[SearchResult]:
        """í¬ë¡¤ë§ ê¸°ë°˜ ê²€ìƒ‰ (í–¥í›„ êµ¬í˜„)"""
        # í˜„ì¬ëŠ” ì‚¬ì´íŠ¸ ì—°ì‚°ìë¡œ ëŒ€ì²´
        return await self._search_via_site_operator(query, site, session)
    
    def _merge_meta_results(
        self, 
        meta_results: List[SearchResult], 
        general_results: List[SearchResult],
        query: str
    ) -> List[SearchResult]:
        """ë©”íƒ€ ê²€ìƒ‰ê³¼ ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        
        all_results = {}
        
        # 1. ë©”íƒ€ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ë†’ì€ ë¶€ìŠ¤íŠ¸)
        for result in meta_results:
            domain = self._extract_domain(result.url)
            if domain not in all_results:
                result.source_type = "meta_search"
                result.boost_score = 1.4  # ë©”íƒ€ ê²€ìƒ‰ ë†’ì€ ë³´ë„ˆìŠ¤
                all_results[domain] = result
        
        # 2. ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë³´ì™„
        added_general = 0
        max_general = max(len(meta_results), 3)  # ìµœì†Œ 3ê°œëŠ” ë³´ì¥
        
        for result in general_results:
            domain = self._extract_domain(result.url)
            if domain not in all_results and added_general < max_general:
                result.source_type = "general"
                result.boost_score = 1.0
                all_results[domain] = result
                added_general += 1
        
        # 3. ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        final_results = list(all_results.values())
        for result in final_results:
            boost_score = getattr(result, 'boost_score', 1.0)
            meta_confidence = getattr(result, 'meta_confidence', 0.5)
            
            # ë©”íƒ€ ê²€ìƒ‰ì˜ ê²½ìš° ì‚¬ì´íŠ¸ ì‹ ë¢°ë„ë„ ë°˜ì˜
            if hasattr(result, 'meta_confidence'):
                result.final_score = result.score * boost_score * (0.5 + meta_confidence * 0.5)
            else:
                result.final_score = result.score * boost_score
        
        final_results.sort(key=lambda x: getattr(x, 'final_score', x.score), reverse=True)
        
        return final_results
    
    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return url


class SearchService:
    """ê²€ìƒ‰ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(30.0),
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
        )
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ
        self.query_analyzer = QueryAnalyzer()  # ì¿¼ë¦¬ ë¶„ì„ê¸° ì¶”ê°€
        self.balanced_strategy = BalancedSearchStrategy(self)  # ê· í˜• ê²€ìƒ‰ ì „ëµ ì¶”ê°€
        self.meta_strategy = MetaSearchStrategy(self)  # ë©”íƒ€ ê²€ìƒ‰ ì „ëµ ì¶”ê°€
    
    def _generate_cache_key(self, query: str, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì¿¼ë¦¬ì™€ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
        key_data = {
            "query": query.lower().strip(),
            "source": kwargs.get("source", "web"),
            "max_results": kwargs.get("max_results", 5),
            "language": kwargs.get("language", "ko"),
            "search_type": kwargs.get("search_type", "web"),
            "category": kwargs.get("category", "none"),
            "meta_search": kwargs.get("meta_search", False)
        }
        
        key_string = json.dumps(key_data, sort_keys=True)
        hash_key = hashlib.sha256(key_string.encode()).hexdigest()[:16]
        return f"search:{hash_key}"
    
    def _get_dynamic_cache_ttl(self, search_type: SearchType, query: str) -> int:
        """ê²€ìƒ‰ íƒ€ì…ê³¼ ì¿¼ë¦¬ì— ë”°ë¥¸ ë™ì  TTL ê³„ì‚°"""
        base_ttl = self.cache_ttl  # ê¸°ë³¸ 1ì‹œê°„
        
        # ì‹œê°„ ë¯¼ê°ë„ì— ë”°ë¥¸ TTL ì¡°ì •
        time_sensitive_keywords = ["ì˜¤ëŠ˜", "ìµœì‹ ", "latest", "recent", "ì§€ê¸ˆ", "í˜„ì¬", "today"]
        if any(keyword in query.lower() for keyword in time_sensitive_keywords):
            return int(base_ttl * 0.1)  # 6ë¶„ - ì‹œê°„ ë¯¼ê° ì •ë³´
        
        # ê²€ìƒ‰ íƒ€ì…ë³„ TTL ì „ëµ
        if search_type == SearchType.NEWS:
            return int(base_ttl * 0.25)  # 15ë¶„ - ë‰´ìŠ¤ëŠ” ë¹ ë¥´ê²Œ ë³€í•¨
        elif search_type == SearchType.ACADEMIC:
            return int(base_ttl * 4)     # 4ì‹œê°„ - í•™ìˆ  ì •ë³´ëŠ” ì•ˆì •ì 
        elif search_type == SearchType.GOVERNMENT:
            return int(base_ttl * 2)     # 2ì‹œê°„ - ê³µì‹ ì •ë³´ëŠ” ë¹„êµì  ì•ˆì •ì 
        elif search_type == SearchType.TECHNICAL:
            return int(base_ttl * 3)     # 3ì‹œê°„ - ê¸°ìˆ  ë¬¸ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì 
        elif search_type == SearchType.SHOPPING:
            return int(base_ttl * 0.5)   # 30ë¶„ - ì‡¼í•‘ ì •ë³´ëŠ” ìì£¼ ë³€í•¨
        else:
            return base_ttl              # 1ì‹œê°„ - ì¼ë°˜ ì›¹ ê²€ìƒ‰
    
    
    async def search_duckduckgo(
        self,
        query: str,
        max_results: int = 5,
        **kwargs
    ) -> List[SearchResult]:
        """DuckDuckGo ê²€ìƒ‰ (API í‚¤ ë¶ˆí•„ìš”)"""
        try:
            # DuckDuckGo Instant Answer API ì‚¬ìš©
            url = "https://api.duckduckgo.com/"
            params = {
                "q": query,
                "format": "json",
                "no_html": "1",
                "skip_disambig": "1"
            }
            
            response = await self.client.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # Abstract ì •ë³´
            if data.get("Abstract"):
                results.append(SearchResult(
                    title=data.get("Heading", query),
                    url=data.get("AbstractURL", ""),
                    snippet=data["Abstract"],
                    source="duckduckgo_abstract",
                    score=0.95
                ))
            
            # Definition ì •ë³´ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            if data.get("Definition"):
                results.append(SearchResult(
                    title=data.get("Definition", {}).get("Source", query),
                    url=data.get("Definition", {}).get("FirstURL", ""),
                    snippet=data.get("Definition", {}).get("Text", ""),
                    source="duckduckgo_definition",
                    score=0.93
                ))
            
            # Related topics
            for topic in data.get("RelatedTopics", [])[:max_results-len(results)]:
                if isinstance(topic, dict) and "Text" in topic:
                    results.append(SearchResult(
                        title=topic.get("Result", "").split(" - ")[0] if " - " in topic.get("Result", "") else query,
                        url=topic.get("FirstURL", ""),
                        snippet=topic["Text"],
                        source="duckduckgo_topic",
                        score=0.80
                    ))
            
            return results[:max_results]
            
        except Exception as e:
            print(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _process_search_operators(self, query: str) -> Tuple[str, Dict[str, str]]:
        """Google ê²€ìƒ‰ ì—°ì‚°ì ì²˜ë¦¬"""
        processed_query = query
        operators = {}
        
        # site: ì—°ì‚°ì ì²˜ë¦¬
        site_pattern = r'site:([^\s]+)'
        site_match = re.search(site_pattern, query, re.IGNORECASE)
        if site_match:
            site_domain = site_match.group(1)
            operators['site'] = site_domain
            processed_query = re.sub(site_pattern, '', query, flags=re.IGNORECASE).strip()
        
        # inurl: ì—°ì‚°ì ì²˜ë¦¬
        inurl_pattern = r'inurl:([^\s]+)'
        inurl_match = re.search(inurl_pattern, query, re.IGNORECASE)
        if inurl_match:
            url_part = inurl_match.group(1)
            operators['inurl'] = url_part
            processed_query = re.sub(inurl_pattern, '', query, flags=re.IGNORECASE).strip()
        
        # intitle: ì—°ì‚°ì ì²˜ë¦¬  
        intitle_pattern = r'intitle:([^\s]+)'
        intitle_match = re.search(intitle_pattern, query, re.IGNORECASE)
        if intitle_match:
            title_part = intitle_match.group(1)
            operators['intitle'] = title_part
            processed_query = re.sub(intitle_pattern, '', query, flags=re.IGNORECASE).strip()
        
        return processed_query, operators

    async def search_google(
        self,
        query: str,
        max_results: int = 5,
        search_type: SearchType = SearchType.WEB,
        **kwargs
    ) -> List[SearchResult]:
        """Google Custom Search APIë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì›¹ ê²€ìƒ‰"""
        
        print(f"ğŸ” Google ê²€ìƒ‰ ì‹œë„: '{query}' (íƒ€ì…: {search_type.value})")
        print(f"ğŸ”‘ API í‚¤ ìƒíƒœ: GOOGLE_API_KEY={'ìˆìŒ' if settings.GOOGLE_API_KEY else 'ì—†ìŒ'}")
        print(f"ğŸ”‘ CSE ID ìƒíƒœ: GOOGLE_CSE_ID={'ìˆìŒ' if settings.GOOGLE_CSE_ID else 'ì—†ìŒ'}")
        
        if not settings.GOOGLE_API_KEY or not settings.GOOGLE_CSE_ID:
            print("âŒ Google API í‚¤ ë˜ëŠ” CSE IDê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return []
        
        try:
            # 1. ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ê· í˜• ê²€ìƒ‰ìš©)
            if "custom_params" in kwargs and kwargs["custom_params"]:
                search_params = kwargs["custom_params"]
                search_params.num = min(max_results, 10)
                print(f"ğŸ¯ ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„° ì‚¬ìš©: ì‚¬ì´íŠ¸ì œí•œ={search_params.siteSearch}")
            else:
                # 2. ì§€ëŠ¥í˜• ì¿¼ë¦¬ ë¶„ì„ìœ¼ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ìƒì„±
                search_params = self.query_analyzer.analyze_query(query, search_type)
                search_params.num = min(max_results, 10)  # Google APIëŠ” ìµœëŒ€ 10ê°œê¹Œì§€
            
            # 3. ê¸°ì¡´ ê²€ìƒ‰ ì—°ì‚°ì ì²˜ë¦¬ë„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
            processed_query, operators = self._process_search_operators(query)
            if operators:
                # ê¸°ì¡´ ì—°ì‚°ìê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ì™€ ë³‘í•©
                if 'site' in operators and not search_params.siteSearch:
                    search_params.siteSearch = operators['site']
                # ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš© (Googleì´ ì§ì ‘ ì²˜ë¦¬)
                search_params.query = query
            
            # 4. API íŒŒë¼ë¯¸í„° ìƒì„±
            url = "https://www.googleapis.com/customsearch/v1"
            api_params = {
                "key": settings.GOOGLE_API_KEY,
                "cx": settings.GOOGLE_CSE_ID,
                **search_params.to_params()
            }
            
            # 4. ê²€ìƒ‰ ìµœì í™” ë¡œê¹…
            optimizations = []
            if search_params.dateRestrict:
                optimizations.append(f"ì‹œê°„í•„í„°: {search_params.dateRestrict}")
            if search_params.siteSearch:
                optimizations.append(f"ì‚¬ì´íŠ¸ì œí•œ: {search_params.siteSearch}")
            if search_params.fileType:
                optimizations.append(f"íŒŒì¼íƒ€ì…: {search_params.fileType}")
            if search_params.exactTerms:
                optimizations.append(f"ì •í™•í•œêµ¬ë¬¸: {search_params.exactTerms}")
            if search_params.sort:
                optimizations.append(f"ì •ë ¬: {search_params.sort}")
            
            if optimizations:
                print(f"ğŸ¯ Google ê²€ìƒ‰ ìµœì í™” ì ìš©: {', '.join(optimizations)}")
            
            print(f"ğŸ“‹ API íŒŒë¼ë¯¸í„°: {len(api_params)}ê°œ ì„¤ì •ë¨")
            for key, value in api_params.items():
                if key not in ["key", "cx"]:  # ë¯¼ê° ì •ë³´ ì œì™¸
                    print(f"   {key}: {value}")
            
            # 5. API í˜¸ì¶œ
            response = await self.client.get(url, params=api_params, timeout=15.0)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # 6. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì¡°ê¸° ë°˜í™˜
            if "items" not in data:
                print(f"âŒ Google ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}")
                # ë„ˆë¬´ ì œí•œì ì¸ ê²½ìš° fallback ê²€ìƒ‰ ì‹œë„
                if search_params.dateRestrict or search_params.siteSearch:
                    print("ğŸ”„ ì œí•œ ì¡°ê±´ ì™„í™”í•˜ì—¬ ì¬ê²€ìƒ‰ ì‹œë„")
                    fallback_params = GoogleSearchParameters(query=query, num=search_params.num)
                    fallback_api_params = {
                        "key": settings.GOOGLE_API_KEY,
                        "cx": settings.GOOGLE_CSE_ID,
                        **fallback_params.to_params()
                    }
                    response = await self.client.get(url, params=fallback_api_params, timeout=10.0)
                    data = response.json()
                    if "items" not in data:
                        return []
                else:
                    return []
            
            # 7. ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ë° ìŠ¤ì½”ì–´ë§
            items = data.get("items", [])
            for item in items[:max_results]:
                # URL ê²€ì¦ (ìœ íš¨í•œ HTTP/HTTPS URLì¸ì§€ í™•ì¸)
                link = item.get("link", "")
                if not link.startswith(("http://", "https://")):
                    continue
                
                # ë„ë©”ì¸ ì¶”ì¶œ
                domain = item.get('displayLink', 'unknown')
                
                # í–¥ìƒëœ source ì •ë³´
                source_info = f"google_{domain}"
                if search_params.searchType:
                    source_info += f"_{search_params.searchType}"
                if search_params.dateRestrict:
                    source_info += f"_recent"
                
                # í•œêµ­ ë„ë©”ì¸ ë³´ë„ˆìŠ¤ ì ìˆ˜
                korea_bonus = 0.1 if any(tld in domain for tld in ['.co.kr', '.go.kr', '.ac.kr', '.or.kr']) else 0
                
                # ì‹œê°„ ê¸°ë°˜ ë³´ë„ˆìŠ¤ ì ìˆ˜
                time_bonus = 0.15 if search_params.dateRestrict else 0
                
                # ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°
                base_score = 0.95 - (len(results) * 0.03)  # ìˆœì„œì— ë”°ë¥¸ ì ìˆ˜ (ë” ì„¸ë°€í•˜ê²Œ)
                final_score = min(1.0, base_score + korea_bonus + time_bonus)
                
                result = SearchResult(
                    title=item.get("title", "").strip(),
                    url=link,
                    snippet=item.get("snippet", "").strip(),
                    source=source_info,
                    score=final_score,
                    timestamp=datetime.now().isoformat()  # ê²€ìƒ‰ ì‹œì  ê¸°ë¡
                )
                
                # ë¹ˆ ì œëª©ì´ë‚˜ ìŠ¤ë‹ˆí«ì´ ìˆëŠ” ê²°ê³¼ í•„í„°ë§
                if result.title and result.snippet:
                    results.append(result)
            
            # 8. ê²°ê³¼ í’ˆì§ˆ í‰ê°€
            avg_score = sum(r.score for r in results) / len(results) if results else 0
            quality_level = "ë†’ìŒ" if avg_score > 0.8 else "ë³´í†µ" if avg_score > 0.6 else "ë‚®ìŒ"
            
            print(f"âœ… Google ê²€ìƒ‰ ì™„ë£Œ: {query} -> {len(results)}ê°œ ê²°ê³¼ (í’ˆì§ˆ: {quality_level}, í‰ê·  ì ìˆ˜: {avg_score:.2f})")
            return results
            
        except httpx.TimeoutException:
            print(f"â±ï¸ Google ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query}")
            return []
        except httpx.HTTPStatusError as e:
            print(f"ğŸš« Google ê²€ìƒ‰ HTTP ì˜¤ë¥˜: {e.response.status_code}")
            if e.response.status_code == 429:
                print("   API í• ë‹¹ëŸ‰ ì´ˆê³¼ - ì ì‹œ í›„ ì¬ì‹œë„ í•„ìš”")
            elif e.response.status_code == 403:
                print("   API í‚¤ ê¶Œí•œ ë¬¸ì œ - ì„¤ì • í™•ì¸ í•„ìš”")
            return []
        except Exception as e:
            print(f"âŒ Google ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _execute_general_search(
        self,
        query: str,
        max_results: int = 5,
        session: Optional[AsyncSession] = None,
        **kwargs
    ) -> List[SearchResult]:
        """ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰ (ê· í˜• ê²€ìƒ‰ìš©)"""
        results = []
        
        try:
            # 1. Google Custom Searchë¥¼ ë©”ì¸ ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ì‚¬ìš©
            if settings.GOOGLE_API_KEY and settings.GOOGLE_CSE_ID:
                try:
                    google_results = await self.search_google(
                        query, max_results, SearchType.WEB, session=session, **kwargs
                    )
                    results.extend(google_results)
                except Exception as google_error:
                    print(f"âŒ ì¼ë°˜ Google ê²€ìƒ‰ ì‹¤íŒ¨: {google_error}")
            
            # 2. DuckDuckGoë¡œ ì¶”ê°€ ê²°ê³¼ ë³´ì™„
            if len(results) < max_results:
                remaining = max_results - len(results)
                try:
                    duckduckgo_results = await self.search_duckduckgo(query, remaining, **kwargs)
                    results.extend(duckduckgo_results)
                except Exception as duckduckgo_error:
                    print(f"âŒ DuckDuckGo ë³´ì™„ ê²€ìƒ‰ ì‹¤íŒ¨: {duckduckgo_error}")
        
        except Exception as e:
            print(f"ğŸ’¥ ì¼ë°˜ ê²€ìƒ‰ ì „ì²´ ì˜¤ë¥˜: {e}")
        
        return results[:max_results]

    async def search_web(
        self,
        query: str,
        max_results: int = 5,
        use_cache: bool = True,
        session: Optional[AsyncSession] = None,
        search_type: SearchType = SearchType.WEB,
        enable_meta_search: bool = True,
        **kwargs
    ) -> List[SearchResult]:
        """
        ì›¹ ê²€ìƒ‰ ì‹¤í–‰ (ê· í˜•ì¡íŒ ê²€ìƒ‰ + ë©”íƒ€ ê²€ìƒ‰ ì§€ì›)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            search_type: ê²€ìƒ‰ íƒ€ì… (WEB, NEWS, ACADEMIC ë“±)
            enable_meta_search: ë©”íƒ€ ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€
            **kwargs: ì¶”ê°€ ê²€ìƒ‰ ì˜µì…˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ê²€ìƒ‰ íƒ€ì…ì— ë”°ë¥¸ ì§€ëŠ¥í˜• ìºì‹œ TTL ì„¤ì •
        dynamic_ttl = self._get_dynamic_cache_ttl(search_type, query)
        
        # 1. ì¹´í…Œê³ ë¦¬ ê°ì§€ (ë³´ìˆ˜ì  ì ‘ê·¼)
        category, confidence = self.balanced_strategy.detect_category_with_confidence(query)
        
        # 2. ë©”íƒ€ ê²€ìƒ‰ ì ìš© ì—¬ë¶€ íŒë‹¨
        should_use_meta_search = (
            enable_meta_search and 
            self._should_use_meta_search(query, category, confidence)
        )
        
        # ìºì‹œ í‚¤ ìƒì„± (ì¹´í…Œê³ ë¦¬ ë° ë©”íƒ€ ê²€ìƒ‰ ì •ë³´ í¬í•¨)
        cache_key = self._generate_cache_key(
            query, 
            max_results=max_results, 
            search_type=search_type.value,
            category=category if category else "none",
            meta_search=should_use_meta_search,
            **kwargs
        )
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached_results = await cache_manager.get(cache_key, session)
            if cached_results:
                search_method = "ë©”íƒ€ ê²€ìƒ‰" if should_use_meta_search else "ê· í˜• ê²€ìƒ‰" if category else "ì¼ë°˜ ê²€ìƒ‰"
                print(f"ğŸ“¦ ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¡°íšŒ: {query} ({search_method})")
                return [SearchResult.from_dict(result) for result in cached_results]
        
        # 3. ê²€ìƒ‰ ì‹¤í–‰
        if should_use_meta_search:
            # ë©”íƒ€ ê²€ìƒ‰ ì‹¤í–‰ (LLM ê¸°ë°˜ ì‚¬ì´íŠ¸ ë°œê²¬ + ì‚¬ì´íŠ¸ë³„ ê²€ìƒ‰)
            print(f"ğŸ¯ ë©”íƒ€ ê²€ìƒ‰ ì‹¤í–‰: '{query}' (ì „ë¬¸ ì‚¬ì´íŠ¸ ë°œê²¬ í›„ ê²€ìƒ‰)")
            results = await self.meta_strategy.execute_meta_search(
                query, max_results, session
            )
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for result in results:
                if not hasattr(result, 'source_type'):
                    result.source_type = "meta_search"
                result.search_method = "meta"
                result.category_detected = category
                result.category_confidence = confidence
                
        elif category and confidence >= self.balanced_strategy.confidence_threshold:
            # ê· í˜•ì¡íŒ ê²€ìƒ‰ ì‹¤í–‰
            print(f"âš–ï¸ ê· í˜• ê²€ìƒ‰ ì‹¤í–‰: '{query}' (ì¹´í…Œê³ ë¦¬: {category}, ì‹ ë¢°ë„: {confidence:.2f})")
            results = await self.balanced_strategy.execute_balanced_search(
                query, category, max_results, session
            )
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for result in results:
                if not hasattr(result, 'source_type'):
                    result.source_type = "balanced"
                result.search_method = "balanced"
                result.category_detected = category
                result.category_confidence = confidence
        else:
            # ì¼ë°˜ ê²€ìƒ‰ë§Œ ì‹¤í–‰
            print(f"ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ì‹¤í–‰: '{query}' (ì¹´í…Œê³ ë¦¬ ê°ì§€ ì•ˆë¨ ë˜ëŠ” ì‹ ë¢°ë„ ë‚®ìŒ)")
            results = await self._execute_general_search(query, max_results, session, **kwargs)
            
            # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for result in results:
                result.source_type = "general"
                result.search_method = "general"
                result.category_detected = None
                result.category_confidence = 0.0
        
        # 4. ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì§€ëŠ¥í˜• TTLë¡œ ìºì‹œì— ì €ì¥
        if results and use_cache and session:
            cache_data = [result.to_dict() for result in results]
            await cache_manager.set(cache_key, cache_data, session, ttl_seconds=dynamic_ttl)
            print(f"ğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œì— ì €ì¥: {query} ({len(results)}ê°œ ê²°ê³¼, TTL: {dynamic_ttl}ì´ˆ)")
        
        return results[:max_results]
    
    def _should_use_meta_search(self, query: str, category: Optional[str], confidence: float) -> bool:
        """ë©”íƒ€ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ íŒë‹¨"""
        
        # 1. íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° ë©”íƒ€ ê²€ìƒ‰ í™œì„±í™”
        meta_search_keywords = [
            "how to", "tutorial", "guide", "best practices", "example", "documentation",
            "ì‚¬ìš©ë²•", "ë°©ë²•", "ê°€ì´ë“œ", "íŠœí† ë¦¬ì–¼", "ì˜ˆì œ", "ë¬¸ì„œ", "ê³µì‹", "ì„¤ëª…ì„œ"
        ]
        
        query_lower = query.lower()
        has_meta_keywords = any(keyword in query_lower for keyword in meta_search_keywords)
        
        # 2. ì¹´í…Œê³ ë¦¬ ì‹ ë¢°ë„ê°€ ë†’ì€ ê²½ìš°
        has_strong_category = category and confidence >= 0.5
        
        # 3. ê¸°ìˆ ì  ì§ˆë¬¸ì¸ ê²½ìš°
        technical_keywords = [
            "error", "bug", "fix", "install", "config", "setup", "deploy",
            "ì—ëŸ¬", "ì˜¤ë¥˜", "ì„¤ì¹˜", "ì„¤ì •", "ë°°í¬", "ë¬¸ì œ", "í•´ê²°"
        ]
        is_technical = any(keyword in query_lower for keyword in technical_keywords)
        
        # ë©”íƒ€ ê²€ìƒ‰ ì‚¬ìš© ì¡°ê±´
        return has_meta_keywords or has_strong_category or is_technical
    
    async def get_search_suggestions(self, query: str) -> List[str]:
        """ê²€ìƒ‰ ì œì•ˆì–´ ìƒì„±"""
        suggestions = []
        
        # ì¿¼ë¦¬ ê¸°ë°˜ ì œì•ˆì–´ ìƒì„±
        base_suggestions = [
            f"{query} ì‚¬ìš©ë²•",
            f"{query} ì˜ˆì œ",
            f"{query} íŠœí† ë¦¬ì–¼",
            f"{query} ìµœì‹  ë™í–¥",
            f"{query} vs"
        ]
        
        # í”„ë¡œê·¸ë˜ë° ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
        programming_keywords = ["python", "javascript", "react", "api", "database", "ì›¹ê°œë°œ", "ì•±ê°œë°œ"]
        if any(keyword in query.lower() for keyword in programming_keywords):
            suggestions.extend([
                f"{query} ë¼ì´ë¸ŒëŸ¬ë¦¬",
                f"{query} í”„ë ˆì„ì›Œí¬",
                f"{query} ì—ëŸ¬ í•´ê²°",
                f"{query} ì„±ëŠ¥ ìµœì í™”"
            ])
        
        # AI ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
        ai_keywords = ["ai", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "llm", "chatgpt", "claude"]
        if any(keyword in query.lower() for keyword in ai_keywords):
            suggestions.extend([
                f"{query} ëª¨ë¸",
                f"{query} í™œìš© ì‚¬ë¡€",
                f"{query} í”„ë¡¬í”„íŠ¸",
                f"{query} í•œê³„"
            ])
        
        return suggestions[:5]
    
    async def summarize_results(
        self,
        query: str,
        results: List[SearchResult],
        session: Optional[AsyncSession] = None
    ) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        if not results:
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary_parts = [
            f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):\n"
        ]
        
        for i, result in enumerate(results, 1):
            summary_parts.append(
                f"{i}. **{result.title}**\n"
                f"   {result.snippet[:150]}{'...' if len(result.snippet) > 150 else ''}\n"
                f"   ğŸ”— {result.url}\n"
                f"   ğŸ“Š ì¶œì²˜: {result.source} | ì‹ ë¢°ë„: {result.score:.0%}\n"
            )
        
        summary = "\n".join(summary_parts)
        
        # ìš”ì•½ë„ ìºì‹œì— ì €ì¥
        if session:
            cache_key = f"summary:{self._generate_cache_key(query)}"
            await cache_manager.set(cache_key, summary, session, ttl_seconds=self.cache_ttl)
        
        return summary
    
    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
        await self.client.aclose()


# ì „ì—­ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
search_service = SearchService()