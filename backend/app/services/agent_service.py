"""
AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤
"""

from typing import Dict, Any, List, AsyncGenerator
import logging
import asyncio
from datetime import datetime
from app.utils.timezone import now_kst

from app.agents.base import AgentInput
from app.agents.supervisor import supervisor_agent
from app.agents.workers.web_search import web_search_agent
from app.agents.workers.canvas import canvas_agent
from app.services.conversation_context_service import universal_context_analyzer

logger = logging.getLogger(__name__)


class AgentService:
    """AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.supervisor = supervisor_agent
        self.agents = {
            "supervisor": supervisor_agent,
            "web_search": web_search_agent,
            "canvas": canvas_agent,
        }
    
    async def execute_chat(
        self, 
        message: str, 
        model: str = "auto",
        agent_type: str = "auto",
        user_id: str = "default_user",
        session_id: str = None,
        context: Dict[str, Any] = None,
        progress_callback = None
    ) -> Dict[str, Any]:
        """
        ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì§€ì›)
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì… (autoëŠ” supervisorê°€ ìë™ ì„ íƒ)
            user_id: ì‚¬ìš©ì ID
            session_id: ëŒ€í™” ì„¸ì…˜ ID
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°±
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            from app.services.conversation_history_service import conversation_history_service
            from app.db.session import AsyncSessionLocal
            from app.db.models.conversation import MessageRole
            
            # ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì‚¬ìš©
            async with AsyncSessionLocal() as db:
                if session_id:
                    # ê¸°ì¡´ ëŒ€í™” í™•ì¸
                    conversation_detail = await conversation_history_service.get_conversation_detail(
                        conversation_id=session_id,
                        user_id=user_id,
                        session=db
                    )
                    if not conversation_detail:
                        # ëŒ€í™”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        conversation = await conversation_history_service.create_conversation(
                            user_id=user_id,
                            title=f"ëŒ€í™” {now_kst().strftime('%Y-%m-%d %H:%M')}",
                            session=db,
                            model=model,
                            agent_type=agent_type
                        )
                        session_id = conversation['id']
                else:
                    # ìƒˆ ëŒ€í™” ìƒì„± - ì„ì‹œ ì œëª©ìœ¼ë¡œ ìƒì„±
                    conversation = await conversation_history_service.create_conversation(
                        user_id=user_id,
                        title=f"ëŒ€í™” {now_kst().strftime('%Y-%m-%d %H:%M')}",
                        session=db,
                        model=model,
                        agent_type=agent_type
                    )
                    session_id = conversation['id']
                
                # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                await conversation_history_service.add_message(
                    conversation_id=session_id,
                    user_id=user_id,
                    role=MessageRole.USER,
                    content=message,
                    session=db
                )
            
            # LLM ë¼ìš°í„°ë¥¼ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ
            from app.agents.llm_router import llm_router
            
            if model == "auto":
                task_type_mapping = {
                    "web_search": "speed",
                    "supervisor": "reasoning",
                    "auto": "general"
                }
                task_type = task_type_mapping.get(agent_type, "general")
                selected_model = llm_router.get_optimal_model(task_type, len(message))
            else:
                selected_model = model
            
            # ëŒ€í™” ë§¥ë½ ì¶”ì¶œ (ìƒˆë¡œìš´ ConversationContext ì„œë¹„ìŠ¤ ì‚¬ìš©)
            conversation_context = None
            conversation_context_text = ""
            logger.info(f"ğŸ” ëŒ€í™” ë§¥ë½ ì¶”ì¶œ ì‹œì‘ - session_id: {session_id}, message: {message[:100]}...")
            async with AsyncSessionLocal() as db:
                conversation_context = await universal_context_analyzer.extract_conversation_context(
                    session_id=session_id,
                    current_query=message,
                    db_session=db,
                    model=selected_model
                )
                logger.info(f"ğŸ” ë§¥ë½ ì¶”ì¶œ ì™„ë£Œ - domain: {conversation_context.domain if conversation_context else 'None'}")
                
                # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í…ìŠ¤íŠ¸ í˜•íƒœ ì»¨í…ìŠ¤íŠ¸ë„ ìƒì„±
                if conversation_context and conversation_context.recent_messages:
                    context_messages = []
                    for msg in conversation_context.recent_messages[-4:]:  # ìµœê·¼ 4ê°œë§Œ ì‚¬ìš©
                        role = "ì‚¬ìš©ì" if msg['role'] == 'user' else "ì–´ì‹œìŠ¤í„´íŠ¸"
                        context_messages.append(f"{role}: {msg['content']}")
                    if context_messages:
                        conversation_context_text = "\n".join(context_messages)
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ì— í¬í•¨ (ê¸°ì¡´ í˜¸í™˜ì„±)
            enhanced_message = message
            if conversation_context_text:
                enhanced_message = f"ëŒ€í™” ê¸°ë¡:\n{conversation_context_text}\n\ní˜„ì¬ ì§ˆë¬¸: {message}"
            
            # ì…ë ¥ ë°ì´í„° ìƒì„±
            agent_input = AgentInput(
                query=enhanced_message,
                context=context or {"has_conversation_context": bool(conversation_context)},
                user_id=user_id,
                session_id=session_id,
                conversation_context=conversation_context  # ìƒˆë¡œìš´ ë§¥ë½ ì •ë³´ ì¶”ê°€
            )
            
            # ì—ì´ì „íŠ¸ ì„ íƒ ë° ì‹¤í–‰ - ëª¨ë“  ìš”ì²­ì€ Supervisorë¥¼ í†µí•´ ìë™ ì •ë³´ ë¶„ì„
            if agent_type == "none" or agent_type == "auto" or agent_type == "supervisor":
                # Supervisorê°€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ì—ì´ì „íŠ¸ ì„ íƒ
                result = await self.supervisor.execute(agent_input, selected_model, progress_callback)
            else:
                # íŠ¹ì • ì—ì´ì „íŠ¸ ì§ì ‘ ì‹¤í–‰
                agent = self.agents.get(agent_type)
                if not agent:
                    raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì—ì´ì „íŠ¸ íƒ€ì…: {agent_type}")
                result = await agent.execute(agent_input, selected_model, progress_callback)
            
            # AI ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (citations/sources ë©”íƒ€ë°ì´í„° í¬í•¨)
            response_metadata = {}
            
            # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ citationsì™€ sources ì •ë³´ ì¶”ì¶œ
            if hasattr(result, 'citations') and result.citations:
                response_metadata['citations'] = result.citations
            if hasattr(result, 'sources') and result.sources:
                response_metadata['sources'] = result.sources
            if hasattr(result, 'search_results') and result.search_results:
                response_metadata['search_results'] = result.search_results
            if hasattr(result, 'metadata') and result.metadata:
                response_metadata.update(result.metadata)
                
            async with AsyncSessionLocal() as db:
                await conversation_history_service.add_message(
                    conversation_id=session_id,
                    user_id=user_id,
                    role=MessageRole.ASSISTANT,
                    content=result.result,
                    session=db,
                    model=result.model_used,
                    metadata_=response_metadata if response_metadata else None
                )
            
            # ìƒˆ ëŒ€í™”ì¸ ê²½ìš° ì œëª© ìë™ ìƒì„± (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸)
            is_new_conversation = False
            try:
                async with AsyncSessionLocal() as db:
                    # ëŒ€í™”ì˜ ë©”ì‹œì§€ ê°œìˆ˜ í™•ì¸ (ì‚¬ìš©ì ë©”ì‹œì§€ + AI ì‘ë‹µ = 2ê°œë©´ ìƒˆ ëŒ€í™”)
                    conversation_detail = await conversation_history_service.get_conversation_detail(
                        conversation_id=session_id,
                        user_id=user_id,
                        session=db
                    )
                    if conversation_detail and len(conversation_detail.get('messages', [])) == 2:
                        is_new_conversation = True
                        
                        # ì œëª© ìë™ ìƒì„±
                        await self._generate_conversation_title(
                            session_id=session_id,
                            user_message=message,
                            model=model,
                            user_id=user_id
                        )
            except Exception as e:
                logger.error(f"ì œëª© ìƒì„± ì‹¤íŒ¨: {e}")
                # ì œëª© ìƒì„± ì‹¤íŒ¨í•´ë„ ì±„íŒ…ì€ ê³„ì† ì§„í–‰
                pass
            
            # ëŒ€í™” ë©”ì‹œì§€ ì¡°íšŒ (ì‚¬ìš©ì ë©”ì‹œì§€ + AI ì‘ë‹µ í¬í•¨)
            async with AsyncSessionLocal() as db:
                conversation_detail = await conversation_history_service.get_conversation_detail(
                    conversation_id=session_id,
                    user_id=user_id,
                    session=db
                )
                messages = conversation_detail.get('messages', []) if conversation_detail else []
                logger.info(f"ğŸ” ë©”ì‹œì§€ ì¡°íšŒ ì™„ë£Œ - session_id: {session_id}, ë©”ì‹œì§€ ìˆ˜: {len(messages)}")
                if messages:
                    logger.info(f"ğŸ” ë§ˆì§€ë§‰ ë©”ì‹œì§€: {messages[-1]}")
                else:
                    logger.warning(f"âš ï¸ ë©”ì‹œì§€ê°€ ì¡°íšŒë˜ì§€ ì•ŠìŒ - conversation_detail: {conversation_detail}")
            
            return {
                "response": result.result,
                "agent_used": result.agent_id,
                "model_used": result.model_used,
                "timestamp": result.timestamp,
                "user_id": user_id,
                "session_id": session_id,  # ëŒ€í™” ID ë°˜í™˜
                "metadata": result.metadata,
                "execution_time_ms": result.execution_time_ms,
                "citations": getattr(result, 'citations', []),  # citations ì¶”ê°€
                "sources": getattr(result, 'sources', []),  # sources ì¶”ê°€
                "messages": messages,  # ì „ì²´ ëŒ€í™” ë©”ì‹œì§€ í¬í•¨
                "user_message": message  # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ë„ í¬í•¨ (í”„ë¡ íŠ¸ì—”ë“œ ì°¸ì¡°ìš©)
            }
            
        except Exception as e:
            import traceback
            logger.error(f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return {
                "response": f"ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "agent_used": "error_handler",
                "model_used": "system",
                "timestamp": "2024-01-01T00:00:00Z",
                "user_id": user_id,
                "metadata": {"error": str(e)},
                "execution_time_ms": 0
            }
    
    def get_agent_info(self, agent_id: str = None) -> List[Dict[str, Any]]:
        """
        ì—ì´ì „íŠ¸ ì •ë³´ ì¡°íšŒ
        
        Args:
            agent_id: íŠ¹ì • ì—ì´ì „íŠ¸ ID (Noneì´ë©´ ì „ì²´ ëª©ë¡)
            
        Returns:
            ì—ì´ì „íŠ¸ ì •ë³´ ëª©ë¡
        """
        if agent_id:
            agent = self.agents.get(agent_id)
            if not agent:
                return []
            
            return [{
                "id": agent.agent_id,
                "name": agent.name,
                "description": agent.description,
                "capabilities": agent.get_capabilities(),
                "supported_models": agent.get_supported_models(),
                "is_enabled": True
            }]
        
        # ì „ì²´ ì—ì´ì „íŠ¸ ëª©ë¡
        agent_info_list = []
        for agent_id, agent in self.agents.items():
            if agent_id == "supervisor":
                continue  # SupervisorëŠ” ë‚´ë¶€ ì—ì´ì „íŠ¸ì´ë¯€ë¡œ ëª©ë¡ì—ì„œ ì œì™¸
                
            agent_info_list.append({
                "id": agent.agent_id,
                "name": agent.name,
                "description": agent.description,
                "capabilities": agent.get_capabilities(),
                "supported_models": agent.get_supported_models(),
                "is_enabled": True
            })
        
        # ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì€ ì—ì´ì „íŠ¸ë“¤ë„ ì¶”ê°€ (ë¹„í™œì„±í™” ìƒíƒœ)
        planned_agents = [
            {
                "id": "deep_research",
                "name": "ì‹¬ì¸µ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸",
                "description": "íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ì‹¬ì¸µì ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤",
                "capabilities": ["ì‹¬ì¸µ ë¶„ì„", "ë³´ê³ ì„œ ìƒì„±", "ë‹¤ì¤‘ ì†ŒìŠ¤ ì¢…í•©"],
                "supported_models": ["claude", "gemini", "claude-haiku", "gemini-flash"],
                "is_enabled": False
            },
            {
                "id": "multimodal_rag",
                "name": "ë©€í‹°ëª¨ë‹¬ RAG ì—ì´ì „íŠ¸",
                "description": "ë¬¸ì„œì™€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤",
                "capabilities": ["ë¬¸ì„œ ë¶„ì„", "ì´ë¯¸ì§€ ì´í•´", "RAG ê²€ìƒ‰"],
                "supported_models": ["claude", "gemini", "claude-haiku", "gemini-flash"],
                "is_enabled": False
            }
        ]
        
        agent_info_list.extend(planned_agents)
        return agent_info_list
    
    async def execute_agent_directly(
        self,
        agent_id: str,
        input_data: Dict[str, Any],
        model: str = "auto"
    ) -> Dict[str, Any]:
        """
        íŠ¹ì • ì—ì´ì „íŠ¸ ì§ì ‘ ì‹¤í–‰
        
        Args:
            agent_id: ì—ì´ì „íŠ¸ ID
            input_data: ì…ë ¥ ë°ì´í„°
            model: ì‚¬ìš©í•  ëª¨ë¸
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        try:
            agent = self.agents.get(agent_id)
            if not agent:
                raise ValueError(f"ì—ì´ì „íŠ¸ '{agent_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # AgentInput ìƒì„±
            agent_input = AgentInput(
                query=input_data.get("query", ""),
                context=input_data.get("context", {}),
                user_id=input_data.get("user_id", "default_user")
            )
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await agent.execute(agent_input, model)
            
            return {
                "agent_id": result.agent_id,
                "result": {
                    "response": result.result,
                    "metadata": result.metadata
                },
                "execution_time_ms": result.execution_time_ms,
                "model_used": result.model_used
            }
            
        except Exception as e:
            logger.error(f"ì—ì´ì „íŠ¸ '{agent_id}' ì§ì ‘ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "agent_id": agent_id,
                "result": {
                    "response": f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "metadata": {"error": str(e)}
                },
                "execution_time_ms": 0,
                "model_used": model
            }
    
    async def execute_chat_stream(
        self, 
        message: str, 
        model: str = "auto",
        agent_type: str = "auto",
        user_id: str = "default_user",
        session_id: str = None,
        context: Dict[str, Any] = None,
        progress_callback = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì§€ì›)
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì… (autoëŠ” supervisorê°€ ìë™ ì„ íƒ)
            user_id: ì‚¬ìš©ì ID
            session_id: ëŒ€í™” ì„¸ì…˜ ID
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ ìƒíƒœ ì½œë°±
            
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì´ë²¤íŠ¸ë“¤
        """
        try:
            from app.services.conversation_history_service import conversation_history_service
            from app.db.session import AsyncSessionLocal
            from app.db.models.conversation import MessageRole
            from app.agents.llm_router import llm_router
            
            # ëŒ€í™” ìƒì„± ë˜ëŠ” ê¸°ì¡´ ëŒ€í™” ì‚¬ìš© (execute_chatê³¼ ë™ì¼í•œ ë¡œì§)
            async with AsyncSessionLocal() as db:
                if session_id:
                    # ê¸°ì¡´ ëŒ€í™” í™•ì¸
                    conversation_detail = await conversation_history_service.get_conversation_detail(
                        conversation_id=session_id,
                        user_id=user_id,
                        session=db
                    )
                    if not conversation_detail:
                        # ëŒ€í™”ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        conversation = await conversation_history_service.create_conversation(
                            user_id=user_id,
                            title=f"ëŒ€í™” {now_kst().strftime('%Y-%m-%d %H:%M')}",
                            session=db,
                            model=model,
                            agent_type=agent_type
                        )
                        session_id = conversation['id']
                else:
                    # ìƒˆ ëŒ€í™” ìƒì„± - ì„ì‹œ ì œëª©ìœ¼ë¡œ ìƒì„±
                    conversation = await conversation_history_service.create_conversation(
                        user_id=user_id,
                        title=f"ëŒ€í™” {now_kst().strftime('%Y-%m-%d %H:%M')}",
                        session=db,
                        model=model,
                        agent_type=agent_type
                    )
                    session_id = conversation['id']
                
                # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                await conversation_history_service.add_message(
                    conversation_id=session_id,
                    user_id=user_id,
                    role=MessageRole.USER,
                    content=message,
                    session=db
                )
                
            yield {"type": "start", "data": {"session_id": session_id, "message": "ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘..."}}
            
            # LLM ë¼ìš°í„°ë¥¼ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ
            if model == "auto":
                task_type_mapping = {
                    "web_search": "speed",
                    "supervisor": "reasoning",
                    "auto": "general"
                }
                task_type = task_type_mapping.get(agent_type, "general")
                selected_model = llm_router.get_optimal_model(task_type, len(message))
            else:
                selected_model = model
            
            # ëŒ€í™” ë§¥ë½ ì¶”ì¶œ (execute_chatê³¼ ë™ì¼í•œ ë¡œì§)
            conversation_context = None
            conversation_context_text = ""
            logger.info(f"ğŸ” ìŠ¤íŠ¸ë¦¬ë° ëŒ€í™” ë§¥ë½ ì¶”ì¶œ ì‹œì‘ - session_id: {session_id}, message: {message[:100]}...")
            async with AsyncSessionLocal() as db:
                conversation_context = await universal_context_analyzer.extract_conversation_context(
                    session_id=session_id,
                    current_query=message,
                    db_session=db,
                    model=selected_model
                )
                logger.info(f"ğŸ” ìŠ¤íŠ¸ë¦¬ë° ë§¥ë½ ì¶”ì¶œ ì™„ë£Œ - domain: {conversation_context.domain if conversation_context else 'None'}")
                
                # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í…ìŠ¤íŠ¸ í˜•íƒœ ì»¨í…ìŠ¤íŠ¸ë„ ìƒì„±
                if conversation_context and conversation_context.recent_messages:
                    context_messages = []
                    for msg in conversation_context.recent_messages[-4:]:  # ìµœê·¼ 4ê°œë§Œ ì‚¬ìš©
                        role = "ì‚¬ìš©ì" if msg['role'] == 'user' else "ì–´ì‹œìŠ¤í„´íŠ¸"
                        context_messages.append(f"{role}: {msg['content']}")
                    if context_messages:
                        conversation_context_text = "\n".join(context_messages)
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ì— í¬í•¨ (ê¸°ì¡´ í˜¸í™˜ì„±)
            enhanced_message = message
            if conversation_context_text:
                enhanced_message = f"ëŒ€í™” ê¸°ë¡:\n{conversation_context_text}\n\ní˜„ì¬ ì§ˆë¬¸: {message}"
            
            yield {"type": "context", "data": {
                "has_context": bool(conversation_context_text),
                "original_message": message,
                "enhanced_message": enhanced_message if conversation_context_text else None,
                "domain": conversation_context.domain if conversation_context else None
            }}
            
            # ì…ë ¥ ë°ì´í„° ìƒì„± (execute_chatê³¼ ë™ì¼í•œ ë¡œì§)
            agent_input = AgentInput(
                query=enhanced_message,
                context=context or {"has_conversation_context": bool(conversation_context)},
                user_id=user_id,
                session_id=session_id,
                conversation_context=conversation_context  # ìƒˆë¡œìš´ ë§¥ë½ ì •ë³´ ì¶”ê°€
            )
            
            # ì—ì´ì „íŠ¸ ì„ íƒ ë° ì‹¤í–‰ (execute_chatê³¼ ë™ì¼í•œ ë¡œì§ ì ìš©)
            full_response = ""
            chunk_count = 0
            citations = []
            sources = []
            
            if agent_type == "none" or agent_type == "auto" or agent_type == "supervisor":
                # Supervisorê°€ ìë™ìœ¼ë¡œ ì ì ˆí•œ ì—ì´ì „íŠ¸ ì„ íƒ
                result = await self.supervisor.execute(agent_input, selected_model, progress_callback)
                
                # ê²°ê³¼ì—ì„œ citations/sources ì¶”ì¶œ
                citations = getattr(result, 'citations', [])
                sources = getattr(result, 'sources', [])
                full_response = result.result
                
                # ì‘ë‹µì„ ì²­í¬ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
                import asyncio
                words = full_response.split()
                for i, word in enumerate(words):
                    chunk = word + (" " if i < len(words) - 1 else "")
                    chunk_count += 1
                    
                    yield {"type": "chunk", "data": {
                        "text": chunk,
                        "index": chunk_count - 1,
                        "is_final": False
                    }}
                    await asyncio.sleep(0.03)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
                    
            else:
                # íŠ¹ì • ì—ì´ì „íŠ¸ ì§ì ‘ ì‹¤í–‰
                agent = self.agents.get(agent_type)
                if agent:
                    result = await agent.execute(agent_input, selected_model, progress_callback)
                    
                    # ê²°ê³¼ì—ì„œ citations/sources ì¶”ì¶œ
                    citations = getattr(result, 'citations', [])
                    sources = getattr(result, 'sources', [])
                    full_response = result.result
                    
                    # ì‘ë‹µì„ ì²­í¬ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
                    import asyncio
                    words = full_response.split()
                    for i, word in enumerate(words):
                        chunk = word + (" " if i < len(words) - 1 else "")
                        chunk_count += 1
                        
                        yield {"type": "chunk", "data": {
                            "text": chunk,
                            "index": chunk_count - 1,
                            "is_final": False
                        }}
                        await asyncio.sleep(0.03)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼
                else:
                    # ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ fallback
                    async for chunk in llm_router.stream_response(selected_model, enhanced_message):
                        chunk_count += 1
                        full_response += chunk
                        
                        yield {"type": "chunk", "data": {
                            "text": chunk,
                            "index": chunk_count - 1,
                            "is_final": False
                        }}
            
            # ë§ˆì§€ë§‰ ì²­í¬ í‘œì‹œ
            if chunk_count > 0:
                yield {"type": "chunk", "data": {
                    "text": "",
                    "index": chunk_count,
                    "is_final": True
                }}
            
            # AI ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (citations/sources ë©”íƒ€ë°ì´í„° í¬í•¨)
            response_metadata = {"streaming": True, "chunk_count": chunk_count}
            
            # ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ citationsì™€ sources ì •ë³´ ì¶”ê°€
            if citations:
                response_metadata['citations'] = citations
            if sources:
                response_metadata['sources'] = sources
                
            async with AsyncSessionLocal() as db:
                await conversation_history_service.add_message(
                    conversation_id=session_id,
                    user_id=user_id,
                    role=MessageRole.ASSISTANT,
                    content=full_response,
                    session=db,
                    model=selected_model,
                    metadata_=response_metadata
                )
            
            # ìƒˆ ëŒ€í™”ì¸ ê²½ìš° ì œëª© ìë™ ìƒì„± (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸)
            is_new_conversation = False
            try:
                async with AsyncSessionLocal() as db:
                    # ëŒ€í™”ì˜ ë©”ì‹œì§€ ê°œìˆ˜ í™•ì¸ (ì‚¬ìš©ì ë©”ì‹œì§€ + AI ì‘ë‹µ = 2ê°œë©´ ìƒˆ ëŒ€í™”)
                    conversation_detail = await conversation_history_service.get_conversation_detail(
                        conversation_id=session_id,
                        user_id=user_id,
                        session=db
                    )
                    if conversation_detail and len(conversation_detail.get('messages', [])) == 2:
                        is_new_conversation = True
                        
                        # ì œëª© ìë™ ìƒì„±
                        await self._generate_conversation_title(
                            session_id=session_id,
                            user_message=message,
                            model=model,
                            user_id=user_id
                        )
            except Exception as e:
                logger.error(f"ì œëª© ìƒì„± ì‹¤íŒ¨: {e}")
                # ì œëª© ìƒì„± ì‹¤íŒ¨í•´ë„ ì±„íŒ…ì€ ê³„ì† ì§„í–‰
                pass
            
            # ë©”íƒ€ë°ì´í„° ì „ì†¡
            citation_stats = None
            if citations:
                citation_stats = {
                    "total_citations": len(citations),
                    "unique_sources": len(set(c.get('source', '') for c in citations if c.get('source')))
                }
            
            # ì‹¤ì œ ì‚¬ìš©ëœ ì—ì´ì „íŠ¸ ê²°ì •
            actual_agent_used = agent_type  # ìš”ì²­ëœ ì—ì´ì „íŠ¸ íƒ€ì…ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
            
            if agent_type == "none" or agent_type == "auto" or agent_type == "supervisor":
                # Supervisorê°€ ì‹¤í–‰ë˜ì–´ result ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                if 'result' in locals() and hasattr(result, 'metadata'):
                    delegated_to = result.metadata.get('delegated_to')
                    supervisor_decision = result.metadata.get('supervisor_decision')
                    
                    if delegated_to:
                        # supervisorê°€ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„í•œ ê²½ìš°
                        actual_agent_used = delegated_to
                    elif supervisor_decision == 'information_request':
                        actual_agent_used = "information_analysis"
                    elif supervisor_decision in ['web_search', 'deep_research', 'canvas']:
                        # supervisorê°€ ì‘ì—… ìœ í˜•ì„ ê²°ì •í•œ ê²½ìš°
                        actual_agent_used = supervisor_decision
                    else:
                        actual_agent_used = "supervisor"
                else:
                    actual_agent_used = "supervisor"
            elif agent_type == "web_search":
                # ì›¹ê²€ìƒ‰ ëª¨ë“œëŠ” ëª…ì‹œì ìœ¼ë¡œ web_searchë¡œ ì„¤ì •
                actual_agent_used = "web_search"
            
            yield {"type": "metadata", "data": {
                "agent_used": actual_agent_used,
                "model_used": selected_model,
                "timestamp": now_kst().isoformat(),
                "user_id": user_id,
                "session_id": session_id,
                "citations": citations,
                "sources": sources,
                "citation_stats": citation_stats,
                "metadata": response_metadata,
                "context_applied": bool(conversation_context_text)
            }}
            
            # ìµœì¢… ì™„ë£Œ ê²°ê³¼ ì „ì†¡
            yield {"type": "result", "data": {
                "response": full_response,
                "agent_used": actual_agent_used,
                "model_used": selected_model,
                "timestamp": now_kst().isoformat(),
                "user_id": user_id,
                "session_id": session_id,
                "citations": citations,
                "sources": sources,
                "citation_stats": citation_stats,
                "metadata": response_metadata
            }}
            
            yield {"type": "end", "data": {"message": "ëŒ€í™” ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}}
            
        except Exception as e:
            import traceback
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            yield {"type": "error", "data": {
                "message": f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e)
            }}
    
    async def stream_response(
        self,
        query: str,
        model: str = "auto",
        agent_type: str = "general",
        conversation_id: str = None,
        user_id: str = "default_user"
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ì—†ëŠ” ê°„ë‹¨í•œ ë²„ì „)
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            model: ì‚¬ìš©í•  ëª¨ë¸
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì…
            conversation_id: ëŒ€í™” ID
            user_id: ì‚¬ìš©ì ID
            
        Yields:
            ì‘ë‹µ í…ìŠ¤íŠ¸ ì²­í¬
        """
        try:
            # LLM ë¼ìš°í„°ë¥¼ í†µí•œ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°
            from app.agents.llm_router import llm_router
            
            # LLM ë¼ìš°í„°ë¥¼ í†µí•œ ìµœì  ëª¨ë¸ ì„ íƒ
            if model == "auto":
                # ì—ì´ì „íŠ¸ ìœ í˜•ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ
                task_type_mapping = {
                    "web_search": "speed",
                    "technical": "reasoning", 
                    "creative": "creative",
                    "general": "general"
                }
                task_type = task_type_mapping.get(agent_type, "general")
                normalized_model = llm_router.get_optimal_model(task_type, len(query))
            else:
                # ëª¨ë¸ ì´ë¦„ ì •ê·œí™” (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)
                model_mapping = {
                    "claude-3-haiku": "claude-haiku",
                    "claude-3-sonnet": "claude", 
                    "claude-3-5-sonnet": "claude-3.5",
                    "claude-3.5": "claude-3.5",
                    "gemini": "gemini-pro",  # ê¸°ì¡´ geminië¥¼ gemini-proë¡œ ë§¤í•‘
                    "gemini-1.5-pro": "gemini-pro",
                    "gemini-1.0-pro": "gemini-1.0",
                    "gemini-flash": "gemini-flash"
                }
                normalized_model = model_mapping.get(model, model)
            
            # ì—ì´ì „íŠ¸ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if agent_type == "none":
                # ì¼ë°˜ ì±„íŒ… ëª¨ë“œ - ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
                prompt = query
            elif agent_type == "web_search":
                prompt = f"ì›¹ ê²€ìƒ‰ ìš”ì²­: {query}\n\nìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
            elif agent_type == "technical":
                prompt = f"ê¸°ìˆ  ì§ˆë¬¸: {query}\n\nê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
            else:
                prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
            if conversation_id:
                prompt += f"\n\n[ëŒ€í™” ID: {conversation_id}]"
            if user_id:
                prompt += f"\n[ì‚¬ìš©ì ID: {user_id}]"
            
            # LLM ë¼ìš°í„°ë¥¼ í†µí•œ ìŠ¤íŠ¸ë¦¬ë°
            async for chunk in llm_router.stream_response(normalized_model, prompt):
                yield chunk
                
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _generate_conversation_title(
        self,
        session_id: str,
        user_message: str, 
        model: str,
        user_id: str
    ):
        """
        ëŒ€í™” ì œëª© ìë™ ìƒì„±
        
        Args:
            session_id: ì„¸ì…˜ ID
            user_message: ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€
            model: ì‚¬ìš©í•  ëª¨ë¸
            user_id: ì‚¬ìš©ì ID
        """
        try:
            from app.agents.llm_router import llm_router
            
            # ì œëª© ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            title_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€í™”ì˜ ì œëª©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ì œëª© ìƒì„± ê·œì¹™:
1. 50ì ì´ë‚´ë¡œ ì‘ì„±
2. êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì‘ì„±
3. í•œêµ­ì–´ë¡œ ì‘ì„±
4. ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹´ì•„ì„œ ì‘ì„±
5. "ëŒ€í™”", "ì±„íŒ…" ê°™ì€ ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” í”¼í•˜ê³  êµ¬ì²´ì ì¸ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±

ì œëª©ë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            # LLMì„ í†µí•´ ì œëª© ìƒì„± (ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ ì œì™¸)
            response_content, used_model = await llm_router.generate_response(
                model_name=model,
                prompt=title_prompt,
                user_id=user_id,
                conversation_id=None,
                include_datetime=False  # ì œëª© ìƒì„±ì‹œì—ëŠ” ë‚ ì§œ ì •ë³´ ë¶ˆí•„ìš”
            )
            
            # ìƒì„±ëœ ì œëª© ì •ë¦¬
            generated_title = response_content.strip()
            
            # ë”°ì˜´í‘œ ì œê±°
            if generated_title.startswith('"') and generated_title.endswith('"'):
                generated_title = generated_title[1:-1]
            
            # ê¸¸ì´ ì œí•œ
            if len(generated_title) > 50:
                generated_title = generated_title[:47] + "..."
            
            # ì œëª© ì—…ë°ì´íŠ¸
            from app.db.session import AsyncSessionLocal
            from app.services.conversation_history_service import conversation_history_service
            
            async with AsyncSessionLocal() as db:
                await conversation_history_service.update_conversation_title(
                    conversation_id=session_id,
                    user_id=user_id,
                    title=generated_title,
                    session=db
                )
                
            logger.info(f"ëŒ€í™” ì œëª© ìë™ ìƒì„± ì™„ë£Œ: {generated_title}")
            
        except Exception as e:
            logger.error(f"ì œëª© ìë™ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì œëª©ìœ¼ë¡œ í´ë°±
            try:
                fallback_title = user_message[:30] + ("..." if len(user_message) > 30 else "")
                from app.db.session import AsyncSessionLocal
                from app.services.conversation_history_service import conversation_history_service
                
                async with AsyncSessionLocal() as db:
                    await conversation_history_service.update_conversation_title(
                        conversation_id=session_id,
                        user_id=user_id,
                        title=fallback_title,
                        session=db
                    )
                logger.info(f"ê¸°ë³¸ ì œëª©ìœ¼ë¡œ í´ë°±: {fallback_title}")
            except Exception as fallback_error:
                logger.error(f"í´ë°± ì œëª© ì„¤ì • ì‹¤íŒ¨: {fallback_error}")


# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
agent_service = AgentService()