"""
ëŒ€í™” ì´ë ¥ ì „ìš© 3-Tier ìºì‹± ì‹œìŠ¤í…œ
L1: ë©”ëª¨ë¦¬ ìºì‹œ (ìµœê·¼ 10ê°œ ëŒ€í™”, ìµœê·¼ 100ê°œ ë©”ì‹œì§€)
L2: PostgreSQL ìºì‹œ í…Œì´ë¸” (ìµœê·¼ 100ê°œ ëŒ€í™”, ìµœê·¼ 1000ê°œ ë©”ì‹œì§€)  
L3: ë©”ì¸ í…Œì´ë¸” (ì „ì²´ ì´ë ¥)
"""

from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from collections import OrderedDict
import json
import hashlib
import asyncio
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text
from app.services.cache_manager import cache_manager
from app.db.models.conversation import Conversation, Message, MessageRole, ConversationStatus

logger = logging.getLogger(__name__)


class ConversationCacheManager:
    """ëŒ€í™” ì´ë ¥ ì „ìš© ê³ ë„í™”ëœ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_conversations: int = 10, max_messages: int = 100):
        self.base_cache = cache_manager
        
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ (ëŒ€í™”ë³„ êµ¬ë¶„)
        self.conversation_cache: OrderedDict = OrderedDict()  # ìµœê·¼ ëŒ€í™” ëª©ë¡
        self.message_cache: Dict[str, OrderedDict] = {}  # ëŒ€í™”ë³„ ë©”ì‹œì§€ ìºì‹œ
        
        # ìºì‹œ í¬ê¸° ì œí•œ
        self.max_conversations = max_conversations
        self.max_messages_per_conversation = max_messages
        
        # í†µê³„
        self.stats = {
            'conversation_hits': 0,
            'conversation_misses': 0,
            'message_hits': 0,
            'message_misses': 0
        }
    
    async def get_user_conversations(
        self, 
        user_id: str, 
        session: AsyncSession,
        limit: int = 20,
        skip: int = 0
    ) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì ëŒ€í™” ëª©ë¡ ì¡°íšŒ (3-tier ìºì‹±)"""
        
        cache_key = f"user_conversations:{user_id}:{skip}:{limit}"
        
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (ìµœê·¼ ì¡°íšŒëœ ëª©ë¡)
        if cache_key in self.conversation_cache:
            self.stats['conversation_hits'] += 1
            # LRU ì—…ë°ì´íŠ¸
            conversations = self.conversation_cache.pop(cache_key)
            self.conversation_cache[cache_key] = conversations
            return conversations
        
        # L2: PostgreSQL ìºì‹œ í…Œì´ë¸” í™•ì¸
        cached_data = await self.base_cache.get(cache_key, session)
        if cached_data:
            self.stats['conversation_hits'] += 1
            # L1ì—ë„ ì €ì¥
            self._update_conversation_cache(cache_key, cached_data)
            return cached_data
        
        # L3: ë©”ì¸ í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        self.stats['conversation_misses'] += 1
        
        query = text("""
            SELECT 
                c.id,
                c.title,
                c.model,
                c.agent_type,
                c.status,
                c.created_at,
                c.updated_at,
                COUNT(m.id) as message_count,
                MAX(m.created_at) as last_message_at,
                SUBSTRING(
                    COALESCE(
                        (SELECT content FROM messages 
                         WHERE conversation_id = c.id 
                         ORDER BY created_at DESC LIMIT 1), 
                        ''
                    ), 1, 100
                ) as last_message_preview
            FROM conversations c
            LEFT JOIN messages m ON c.id = m.conversation_id
            WHERE c.user_id = :user_id AND c.status = :status
            GROUP BY c.id, c.title, c.model, c.agent_type, c.status, c.created_at, c.updated_at
            ORDER BY c.updated_at DESC
            LIMIT :limit OFFSET :skip
        """)
        
        result = await session.execute(
            query, 
            {
                'user_id': user_id, 
                'status': ConversationStatus.ACTIVE.value,
                'limit': limit, 
                'skip': skip
            }
        )
        
        conversations = []
        for row in result:
            conversation_data = {
                'id': str(row.id),
                'title': row.title,
                'model': row.model,
                'agent_type': row.agent_type,
                'status': str(row.status),
                'created_at': row.created_at.isoformat() if row.created_at else None,
                'updated_at': row.updated_at.isoformat() if row.updated_at else None,
                'message_count': row.message_count,
                'last_message_at': row.last_message_at.isoformat() if row.last_message_at else None,
                'last_message_preview': row.last_message_preview
            }
            conversations.append(conversation_data)
        
        # ìºì‹œì— ì €ì¥ (L1 + L2)
        await self.base_cache.set(cache_key, conversations, session, ttl_seconds=300)  # 5ë¶„
        self._update_conversation_cache(cache_key, conversations)
        
        return conversations
    
    async def get_conversation_messages(
        self,
        conversation_id: str,
        session: AsyncSession,
        limit: int = 50,
        skip: int = 0
    ) -> List[Dict[str, Any]]:
        """ëŒ€í™” ë©”ì‹œì§€ ì¡°íšŒ (3-tier ìºì‹±)"""
        
        cache_key = f"conversation_messages:{conversation_id}:{skip}:{limit}"
        
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if conversation_id in self.message_cache:
            if cache_key in self.message_cache[conversation_id]:
                self.stats['message_hits'] += 1
                messages = self.message_cache[conversation_id].pop(cache_key)
                self.message_cache[conversation_id][cache_key] = messages
                return messages
        
        # L2: PostgreSQL ìºì‹œ í…Œì´ë¸” í™•ì¸
        cached_data = await self.base_cache.get(cache_key, session)
        if cached_data:
            self.stats['message_hits'] += 1
            # L1ì—ë„ ì €ì¥
            self._update_message_cache(conversation_id, cache_key, cached_data)
            return cached_data
        
        # L3: ë©”ì¸ í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        self.stats['message_misses'] += 1
        
        query = text("""
            SELECT 
                m.id,
                m.role,
                m.content,
                m.model,
                m.tokens_input,
                m.tokens_output,
                m.latency_ms,
                m.metadata_,
                m.attachments,
                m.created_at,
                m.updated_at
            FROM messages m
            WHERE m.conversation_id = :conversation_id
            ORDER BY m.created_at ASC
            LIMIT :limit OFFSET :skip
        """)
        
        result = await session.execute(
            query,
            {
                'conversation_id': conversation_id,
                'limit': limit,
                'skip': skip
            }
        )
        
        messages = []
        for row in result:
            # metadataì—ì„œ canvas_data ì¶”ì¶œ
            metadata = row.metadata_ or {}
            canvas_data = metadata.get('canvas_data', None)
            
            # Canvas ë°ì´í„° ì¡°íšŒ ë¡œê¹…
            if canvas_data:
                logger.info(f"ğŸ” Canvas ë°ì´í„° ì¡°íšŒ ì„±ê³µ - message_id: {row.id}, íƒ€ì…: {canvas_data.get('type', 'unknown')}")
                logger.debug(f"ğŸ” ì¡°íšŒëœ Canvas ë°ì´í„° ìƒì„¸: {canvas_data}")
            elif metadata:
                logger.debug(f"ğŸ” ë©”íƒ€ë°ì´í„° ìˆì§€ë§Œ Canvas ë°ì´í„° ì—†ìŒ - message_id: {row.id}, metadata í‚¤: {list(metadata.keys())}")
            else:
                logger.debug(f"ğŸ” ë©”íƒ€ë°ì´í„° ì—†ìŒ - message_id: {row.id}")
            
            message_data = {
                'id': str(row.id),
                'role': (row.role.value if hasattr(row.role, 'value') else str(row.role)).upper(),
                'content': row.content,
                'model': row.model,
                'tokens_input': row.tokens_input,
                'tokens_output': row.tokens_output,
                'latency_ms': row.latency_ms,
                'metadata_': row.metadata_,
                'attachments': row.attachments,
                'created_at': row.created_at.isoformat() if row.created_at else None,
                'updated_at': row.updated_at.isoformat() if row.updated_at else None,
                'canvas_data': canvas_data  # Canvas ë°ì´í„°ë¥¼ ë³„ë„ í•„ë“œë¡œ ì œê³µ
            }
            messages.append(message_data)
        
        # ë°˜í™˜ ì „ Canvas ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
        canvas_messages = [msg for msg in messages if msg.get('canvas_data')]
        if canvas_messages:
            logger.info(f"ğŸ“¤ ë©”ì‹œì§€ ë°˜í™˜ - Canvas ë°ì´í„° í¬í•¨ëœ ë©”ì‹œì§€ ìˆ˜: {len(canvas_messages)}/{len(messages)}")
            for msg in canvas_messages:
                logger.debug(f"ğŸ“¤ Canvas ë©”ì‹œì§€ ë°˜í™˜: ID={msg['id']}, canvas_data íƒ€ì…={msg['canvas_data'].get('type', 'unknown')}")
        else:
            logger.debug(f"ğŸ“¤ ë©”ì‹œì§€ ë°˜í™˜ - Canvas ë°ì´í„° ì—†ìŒ ({len(messages)}ê°œ ë©”ì‹œì§€)")
        
        # ìºì‹œì— ì €ì¥ (L1 + L2)
        await self.base_cache.set(cache_key, messages, session, ttl_seconds=600)  # 10ë¶„
        self._update_message_cache(conversation_id, cache_key, messages)
        
        return messages
    
    async def search_conversations(
        self,
        user_id: str,
        query: str,
        session: AsyncSession,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """ëŒ€í™” ì „ë¬¸ê²€ìƒ‰ (ìºì‹± ì ìš©)"""
        
        cache_key = f"search_conversations:{user_id}:{hashlib.md5(query.encode()).hexdigest()}:{limit}"
        
        # ê²€ìƒ‰ ê²°ê³¼ëŠ” L2 ìºì‹œì—ì„œ í™•ì¸ (ì§§ì€ TTL)
        cached_data = await self.base_cache.get(cache_key, session)
        if cached_data:
            return cached_data
        
        # ì „ë¬¸ê²€ìƒ‰ ì‹¤í–‰
        search_query = text("""
            SELECT DISTINCT
                c.id,
                c.title,
                c.model,
                c.agent_type,
                c.created_at,
                c.updated_at,
                ts_rank(to_tsvector('korean', m.content), plainto_tsquery('korean', :query)) as rank,
                ts_headline('korean', m.content, plainto_tsquery('korean', :query)) as highlight
            FROM conversations c
            JOIN messages m ON c.id = m.conversation_id
            WHERE c.user_id = :user_id 
                AND c.status = :status
                AND (
                    to_tsvector('korean', m.content) @@ plainto_tsquery('korean', :query)
                    OR to_tsvector('english', m.content) @@ plainto_tsquery('english', :query)
                    OR c.title ILIKE :title_query
                )
            ORDER BY rank DESC, c.updated_at DESC
            LIMIT :limit
        """)
        
        result = await session.execute(
            search_query,
            {
                'user_id': user_id,
                'query': query,
                'title_query': f'%{query}%',
                'status': ConversationStatus.ACTIVE.value,
                'limit': limit
            }
        )
        
        search_results = []
        for row in result:
            result_data = {
                'id': str(row.id),
                'title': row.title,
                'model': row.model,
                'agent_type': row.agent_type,
                'created_at': row.created_at.isoformat() if row.created_at else None,
                'updated_at': row.updated_at.isoformat() if row.updated_at else None,
                'rank': float(row.rank) if row.rank else 0.0,
                'highlight': row.highlight
            }
            search_results.append(result_data)
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± (ì§§ì€ TTL - 2ë¶„)
        await self.base_cache.set(cache_key, search_results, session, ttl_seconds=120)
        
        return search_results
    
    def _update_conversation_cache(self, key: str, data: List[Dict[str, Any]]):
        """L1 ëŒ€í™” ìºì‹œ ì—…ë°ì´íŠ¸"""
        self.conversation_cache[key] = data
        
        # í¬ê¸° ì œí•œ í™•ì¸
        if len(self.conversation_cache) > self.max_conversations:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            self.conversation_cache.popitem(last=False)
    
    def _update_message_cache(self, conversation_id: str, key: str, data: List[Dict[str, Any]]):
        """L1 ë©”ì‹œì§€ ìºì‹œ ì—…ë°ì´íŠ¸"""
        if conversation_id not in self.message_cache:
            self.message_cache[conversation_id] = OrderedDict()
        
        self.message_cache[conversation_id][key] = data
        
        # í¬ê¸° ì œí•œ í™•ì¸
        if len(self.message_cache[conversation_id]) > self.max_messages_per_conversation:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            self.message_cache[conversation_id].popitem(last=False)
    
    async def invalidate_conversation_cache(
        self, 
        user_id: str, 
        conversation_id: str, 
        session: Optional[AsyncSession] = None
    ):
        """ëŒ€í™” ìºì‹œ ë¬´íš¨í™”"""
        
        # L1 ìºì‹œ ë¬´íš¨í™”
        keys_to_remove = []
        for key in self.conversation_cache.keys():
            if f"user_conversations:{user_id}" in key:
                keys_to_remove.append(key)
        
        for key in keys_to_remove:
            self.conversation_cache.pop(key, None)
        
        # ë©”ì‹œì§€ ìºì‹œ ë¬´íš¨í™”
        if conversation_id in self.message_cache:
            self.message_cache.pop(conversation_id)
        
        # L2 ìºì‹œ ë¬´íš¨í™”
        if session:
            await self.base_cache.invalidate_pattern(f"user_conversations:{user_id}", session)
            await self.base_cache.invalidate_pattern(f"conversation_messages:{conversation_id}", session)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_conversation_requests = self.stats['conversation_hits'] + self.stats['conversation_misses']
        total_message_requests = self.stats['message_hits'] + self.stats['message_misses']
        
        conversation_hit_rate = (
            (self.stats['conversation_hits'] / total_conversation_requests * 100) 
            if total_conversation_requests > 0 else 0
        )
        
        message_hit_rate = (
            (self.stats['message_hits'] / total_message_requests * 100) 
            if total_message_requests > 0 else 0
        )
        
        return {
            'conversation_cache': {
                'size': len(self.conversation_cache),
                'max_size': self.max_conversations,
                'hits': self.stats['conversation_hits'],
                'misses': self.stats['conversation_misses'],
                'hit_rate': f"{conversation_hit_rate:.2f}%"
            },
            'message_cache': {
                'conversations_cached': len(self.message_cache),
                'total_cache_entries': sum(len(cache) for cache in self.message_cache.values()),
                'hits': self.stats['message_hits'],
                'misses': self.stats['message_misses'],
                'hit_rate': f"{message_hit_rate:.2f}%"
            },
            'base_cache': self.base_cache.get_stats()
        }


# ì „ì—­ ëŒ€í™” ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
conversation_cache_manager = ConversationCacheManager()