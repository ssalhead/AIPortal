"""
ëŒ€í™” ë§¥ë½ ì¶”ì¶œ ë° ë¶„ì„ ì„œë¹„ìŠ¤
"""

from typing import Dict, Any, List, Optional
import json
import re
import logging
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, desc

from app.agents.base import ConversationContext
from app.agents.llm_router import llm_router

logger = logging.getLogger(__name__)


class UniversalContextAnalyzer:
    """ë²”ìš© ëŒ€í™” ë§¥ë½ ì¶”ì¶œ ë° ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.max_recent_messages = 10  # ë” ë§ì€ ë§¥ë½ í™•ë³´ë¥¼ ìœ„í•´ ì¦ê°€
        
        # í•˜ë“œì½”ë”©ëœ íŒ¨í„´ ì œê±° - ì™„ì „ LLM ê¸°ë°˜ìœ¼ë¡œ ì „í™˜
        # ì´ì œ ëª¨ë“  ë„ë©”ì¸ê³¼ ì˜ë„ ë¶„ë¥˜ë¥¼ LLMì´ ë™ì ìœ¼ë¡œ ìˆ˜í–‰
        
        # ë„ë©”ì¸ ë¶„ë¥˜ ì‹ ë¢°ë„ ì„ê³„ê°’
        self.domain_confidence_threshold = 0.7
        
        # ë™ì  í•™ìŠµ ë„ë©”ì¸ ìºì‹œ (ì„±ëŠ¥ ìµœì í™” + ì§€ì‹ ì¶•ì )
        self.domain_cache = {}
        self.cache_max_size = 1000
        self.domain_learning_enabled = True
        
        # ì‹¤ì‹œê°„ ë„ë©”ì¸ í†µê³„
        self.domain_stats = {
            "total_classifications": 0,
            "high_confidence_count": 0,
            "discovered_domains": set(),
            "domain_frequency": {},
            "confidence_distribution": []
        }
    
    async def extract_conversation_context(
        self, 
        session_id: str, 
        current_query: str,
        db_session: AsyncSession,
        model: str = "gemini"
    ) -> ConversationContext:
        """
        ëŒ€í™” ì„¸ì…˜ì—ì„œ ë§¥ë½ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            session_id: ëŒ€í™” ì„¸ì…˜ ID
            current_query: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸
            db_session: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            model: LLM ëª¨ë¸ëª…
            
        Returns:
            ì¶”ì¶œëœ ëŒ€í™” ë§¥ë½ ì •ë³´
        """
        try:
            # 1. ìµœê·¼ ë©”ì‹œì§€ ì¡°íšŒ
            recent_messages = await self._get_recent_messages(session_id, db_session)
            logger.info(f"ğŸ” ë©”ì‹œì§€ ì¡°íšŒ ì™„ë£Œ - session_id: {session_id}, ì¡°íšŒëœ ë©”ì‹œì§€ ìˆ˜: {len(recent_messages)}")
            
            # 2. ë§¥ë½ ë¶„ì„ (ì²« ë©”ì‹œì§€ì´ê±°ë‚˜ ë©”ì‹œì§€ê°€ 1ê°œì¼ ë•Œë„ ì‹œë„)
            if len(recent_messages) <= 1:
                logger.info(f"ğŸ” ë©”ì‹œì§€ ìˆ˜ê°€ ì ìŒ ({len(recent_messages)}ê°œ) - ê¸°ë³¸ ë§¥ë½ìœ¼ë¡œ ë¶„ì„ ì‹œë„")
                # ì²« ë©”ì‹œì§€ê±°ë‚˜ ë©”ì‹œì§€ê°€ 1ê°œë©´ ê¸°ë³¸ ë§¥ë½ë§Œ ìƒì„±
                return ConversationContext(
                    recent_messages=recent_messages,
                    conversation_topics=[],
                    mentioned_entities=[],
                    previous_search_queries=[],
                    conversation_flow="ì²« ë²ˆì§¸ ëŒ€í™” ë˜ëŠ” ì´ˆê¸° ë‹¨ê³„",
                    current_focus_topic=None,
                    question_depth_level="basic"
                )
            
            # 3. ì „ì²´ ë§¥ë½ ë¶„ì„ (2ê°œ ì´ìƒ ë©”ì‹œì§€ì¼ ë•Œ)
            logger.info(f"ğŸ” ë§¥ë½ ë¶„ì„ ì‹œì‘ - {len(recent_messages)}ê°œ ë©”ì‹œì§€ë¡œ ë¶„ì„")
            context = await self._analyze_conversation_context(
                recent_messages, 
                current_query, 
                model
            )
            
            return context
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ë§¥ë½ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ConversationContext()
    
    async def _get_recent_messages(
        self, 
        session_id: str, 
        db_session: AsyncSession
    ) -> List[Dict[str, Any]]:
        """ìµœê·¼ ë©”ì‹œì§€ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            logger.info(f"ğŸ” DB ë©”ì‹œì§€ ì¡°íšŒ ì‹œì‘ - session_id: {session_id}, limit: {self.max_recent_messages}")
            
            # messages í…Œì´ë¸”ì—ì„œ ìµœê·¼ ë©”ì‹œì§€ ì¡°íšŒ
            query = text("""
                SELECT id, role, content, created_at, metadata_
                FROM messages 
                WHERE conversation_id = :session_id
                ORDER BY created_at DESC
                LIMIT :limit
            """)
            
            result = await db_session.execute(
                query, 
                {"session_id": session_id, "limit": self.max_recent_messages}
            )
            
            logger.info(f"ğŸ” DB ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ - session_id: {session_id}")
            
            messages = []
            for row in result:
                # metadataëŠ” ì´ë¯¸ dict íƒ€ì…ì´ë¯€ë¡œ JSON íŒŒì‹± ë¶ˆí•„ìš”
                metadata = row.metadata_ if row.metadata_ else {}
                if isinstance(metadata, str):
                    # ë§Œì•½ ë¬¸ìì—´ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´ JSON íŒŒì‹±
                    try:
                        metadata = json.loads(metadata)
                    except (json.JSONDecodeError, TypeError):
                        metadata = {}
                        
                message_data = {
                    'id': str(row.id),
                    'role': row.role,
                    'content': row.content,
                    'created_at': row.created_at.isoformat() if row.created_at else None,
                    'metadata': metadata
                }
                messages.append(message_data)
            
            # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
            messages.reverse()
            return messages
            
        except Exception as e:
            logger.error(f"ìµœê·¼ ë©”ì‹œì§€ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def _analyze_conversation_context(
        self, 
        recent_messages: List[Dict[str, Any]], 
        current_query: str,
        model: str
    ) -> ConversationContext:
        """LLMì„ í™œìš©í•˜ì—¬ ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        
        # ë©”ì‹œì§€ ë‚´ìš©ë§Œ ì¶”ì¶œ
        message_contents = []
        search_queries = []
        
        for msg in recent_messages:
            message_contents.append(f"{msg['role']}: {msg['content']}")
            
            # ì´ì „ ê²€ìƒ‰ì–´ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ì—ì„œ)
            if msg.get('metadata', {}).get('agent_type') == 'web_search':
                queries = msg.get('metadata', {}).get('search_queries', [])
                search_queries.extend(queries)
        
        # ì™„ì „ ë™ì  LLM ë§¥ë½ ë¶„ì„ í”„ë¡¬í”„íŠ¸  
        prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ AIì˜ ì™„ì „í•œ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤. ì „ì²´ ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ í˜„ì¬ ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì˜ë„ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.

=== ì „ì²´ ëŒ€í™” ê¸°ë¡ (ì§ˆë¬¸ + AI ë‹µë³€ í¬í•¨) ===
{chr(10).join(message_contents)}

=== í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ ===
"{current_query}"

=== ì´ì „ ê²€ìƒ‰ì–´ ===
{', '.join(search_queries[-3:]) if search_queries else 'ì—†ìŒ'}

ì „ì²´ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "domain": "ëŒ€í™” ì „ì²´ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë„ë©”ì¸ì„ ììœ ë¡­ê²Œ ì •ì˜ (ì˜ˆ: ìš°ì£¼í•­ê³µê³µí•™, í‘¸ë“œí…Œí¬, ë°˜ë ¤ë™ë¬¼í–‰ë™í•™ ë“±)",
  "domain_confidence": 0.9,
  "main_domain": "ì£¼ìš” ë„ë©”ì¸",
  "sub_domains": ["ì„¸ë¶€ ë„ë©”ì¸1", "ì„¸ë¶€ ë„ë©”ì¸2"],
  "conversation_topics": ["ëŒ€í™”ì—ì„œ ë‹¤ë¤„ì§„ í•µì‹¬ ì£¼ì œë“¤ì„ ììœ ë¡­ê²Œ ì¶”ì¶œ"],
  "topic_evolution": ["ì£¼ì œê°€ ì–´ë–»ê²Œ ì§„í™”í–ˆëŠ”ì§€ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´"],
  "mentioned_entities": ["êµ¬ì²´ì  ì—”í‹°í‹°ë“¤: ì œí’ˆëª…, ê¸°ìˆ ëª…, ì¥ì†Œëª… ë“±ì„ ììœ ë¡­ê²Œ ì¶”ì¶œ"],
  "user_intent": "ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ììœ ë¡­ê²Œ ì •ì˜ (ì •ë³´ìˆ˜ì§‘, ë¬¸ì œí•´ê²°, ì¶”ì²œìš”ì²­, ë¹„êµë¶„ì„, ìµœì‹ ë™í–¥íŒŒì•…, í•™ìŠµëª©ì  ë“±)",
  "context_connection": "í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…",
  "search_focus": "ê²€ìƒ‰ì—ì„œ ì¤‘ì ì ìœ¼ë¡œ ì°¾ì•„ì•¼ í•  ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ",
  "optimal_search_queries": [
    "ì²« ë²ˆì§¸: ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì™„ì „íˆ í†µí•©í•œ ê°€ì¥ í•µì‹¬ì ì¸ ê²€ìƒ‰ì–´",
    "ë‘ ë²ˆì§¸: ìœ„ì™€ ë‹¤ë¥¸ ê°ë„ì˜ ë³´ì™„ì  ê²€ìƒ‰ì–´", 
    "ì„¸ ë²ˆì§¸: í™•ì¥ëœ ê´€ì ì˜ ì¶”ê°€ ê²€ìƒ‰ì–´"
  ],
  "conversation_flow": "ëŒ€í™” ì „ì²´ íë¦„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "current_focus_topic": "í˜„ì¬ ê°€ì¥ ì§‘ì¤‘í•˜ê³  ìˆëŠ” ì£¼ì œ",
  "question_depth_level": "basic|intermediate|advanced",
  "dynamic_categories": {{
    "complexity": "simple|moderate|complex",
    "urgency": "low|medium|high", 
    "scope": "narrow|broad|comprehensive",
    "novelty": "familiar|emerging|cutting_edge"
  }}
}}

**ì™„ì „ ë™ì  ë¶„ì„ ì›ì¹™**:
1. **ë„ë©”ì¸ ììœ  ì •ì˜**: ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ì— ì–½ë§¤ì´ì§€ ë§ê³  ëŒ€í™” ë‚´ìš©ì— ê°€ì¥ ì í•©í•œ ë„ë©”ì¸ëª…ì„ ì°½ì˜ì ìœ¼ë¡œ ì •ì˜
2. **ë§¥ë½ ì—°ê²°ì„± ì¤‘ì‹œ**: í˜„ì¬ ì§ˆë¬¸ê³¼ ì´ì „ ëŒ€í™”ì˜ ì—°ê´€ì„±ì„ ê¹Šì´ ë¶„ì„
3. **ì§€ëŠ¥ì  ê²€ìƒ‰ì–´ ìƒì„±**: ë‹¨ìˆœíˆ í˜„ì¬ ì§ˆë¬¸ë§Œ ë³´ì§€ ë§ê³ , ì „ì²´ ëŒ€í™” ë§¥ë½ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìê°€ ì •ë§ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê²€ìƒ‰ì–´ ìƒì„±
4. **ì„¸ë¶„í™”**: ë³µì¡í•œ ì£¼ì œëŠ” ë©”ì¸/ì„œë¸Œ ë„ë©”ì¸ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì •í™•ì„± í–¥ìƒ

**íŠ¹ë³„ ì¼€ì´ìŠ¤ ì²˜ë¦¬**:
- "ì¶”ì²œ", "ì‹ ê°„", "ìµœì‹ ", "íŒë§¤ì¤‘", "ì„œì " ë“±ì´ ë‚˜ì˜¤ë©´ ì´ì „ ëŒ€í™” ì£¼ì œì™€ ê²°í•©
- "ì´ê±°", "ê·¸ê±°", "ê´€ë ¨ëœ", "ì¬í’ˆ" ë“± ì§€ì‹œì–´ê°€ ë‚˜ì˜¤ë©´ ì§ì „ ì–¸ê¸‰ëœ ì£¼ì œ ì°¸ì¡°
- ì „ë¬¸ ìš©ì–´ë‚˜ ê³ ìœ ëª…ì‚¬ê°€ ë‚˜ì˜¤ë©´ í•´ë‹¹ ë¶„ì•¼ë¡œ ë„ë©”ì¸ íŠ¹ì •

**ğŸš¨ ê·¹ë„ë¡œ ì¤‘ìš”í•œ ìµœì  ê²€ìƒ‰ì–´ ìƒì„± ê·œì¹™**:
1. optimal_search_queriesëŠ” ë°˜ë“œì‹œ 3ê°œì˜ êµ¬ì²´ì  ê²€ìƒ‰ì–´ ë°°ì—´ì´ì–´ì•¼ í•¨
2. í˜„ì¬ ì§ˆë¬¸ "{current_query}"ë§Œ ë³´ì§€ ë§ê³ , ì „ì²´ ëŒ€í™” ë§¥ë½ì„ ì¢…í•©í•´ì„œ ìƒì„±
3. ì˜ˆì‹œ: í† ë§ˆí†  â†’ ë™í™” â†’ ì„œì  íŒë§¤ = ["í† ë§ˆí†  ê´€ë ¨ ë™í™”ì±… ì„œì  íŒë§¤", "í† ë§ˆí†  ë™í™”ì±… ìµœì‹  ì‹ ê°„", "í† ë§ˆí†  í…Œë§ˆ ì•„ë™ë„ì„œ êµ¬ë§¤"]
4. ë‹¨ìˆœíˆ í˜„ì¬ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , ë§¥ë½ì„ í†µí•©í•œ ìƒˆë¡œìš´ ê²€ìƒ‰ì–´ ìƒì„±

JSONë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            logger.info(f"ğŸ” ë§¥ë½ ë¶„ì„ ì‹œì‘ - í˜„ì¬ ì§ˆë¬¸: {current_query}")
            logger.info(f"ğŸ” ëŒ€í™” ê¸°ë¡ ìˆ˜: {len(recent_messages)}")
            
            response, _ = await llm_router.generate_response(model, prompt)
            
            # JSON íŒŒì‹±
            clean_response = self._clean_json_response(response)
            logger.info(f"ğŸ” LLM ì‘ë‹µ: {clean_response[:500]}...")
            context_data = json.loads(clean_response)
            
            logger.info(f"ğŸ” ë¶„ì„ëœ ë„ë©”ì¸: {context_data.get('domain', 'N/A')}")
            logger.info(f"ğŸ” ìµœì  ê²€ìƒ‰ì–´: {context_data.get('optimal_search_queries', [])}")
            
            # ìƒˆë¡œìš´ ë™ì  ConversationContext ê°ì²´ ìƒì„±
            context = ConversationContext(
                recent_messages=recent_messages,
                conversation_topics=context_data.get("conversation_topics", []),
                mentioned_entities=context_data.get("mentioned_entities", []),
                previous_search_queries=search_queries[-5:],  # ìµœê·¼ 5ê°œë§Œ
                conversation_flow=context_data.get("conversation_flow", ""),
                current_focus_topic=context_data.get("current_focus_topic"),
                question_depth_level=context_data.get("question_depth_level", "basic"),
                
                # ë™ì  ë„ë©”ì¸ ë¶„ë¥˜ í•„ë“œë“¤
                domain=context_data.get("domain", "general"),
                domain_confidence=context_data.get("domain_confidence", 0.5),
                main_domain=context_data.get("main_domain", context_data.get("domain", "general")),
                sub_domains=context_data.get("sub_domains", []),
                topic_evolution=context_data.get("topic_evolution", []),
                user_intent=context_data.get("user_intent", "ì •ë³´ìˆ˜ì§‘"),
                context_connection=context_data.get("context_connection", ""),
                search_focus=context_data.get("search_focus", ""),
                optimal_search_queries=context_data.get("optimal_search_queries", []),
                
                # ë‹¤ì°¨ì› ë™ì  ì¹´í…Œê³ ë¦¬
                dynamic_categories=context_data.get("dynamic_categories", {
                    "complexity": "simple",
                    "urgency": "low", 
                    "scope": "narrow",
                    "novelty": "familiar"
                })
            )
            
            # ë„ë©”ì¸ í•™ìŠµ ë° í†µê³„ ì—…ë°ì´íŠ¸
            if self.domain_learning_enabled:
                await self._update_domain_learning(context, current_query)
            
            return context
            
        except Exception as e:
            logger.error(f"LLM ë§¥ë½ ë¶„ì„ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ì ì¸ íŒ¨í„´ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ í´ë°±
            return self._extract_basic_context(recent_messages, current_query, search_queries)
    
    def _clean_json_response(self, response: str) -> str:
        """LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        response = response.strip()
        
        # ```json ì œê±°
        if response.startswith('```json'):
            response = response[7:]
        if response.endswith('```'):
            response = response[:-3]
        
        return response.strip()
    
    async def _update_domain_learning(self, context, current_query: str):
        """ë„ë©”ì¸ í•™ìŠµ ë° í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            domain = context.domain
            confidence = context.domain_confidence
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.domain_stats["total_classifications"] += 1
            
            if confidence >= self.domain_confidence_threshold:
                self.domain_stats["high_confidence_count"] += 1
            
            # ìƒˆë¡œìš´ ë„ë©”ì¸ ë°œê²¬
            if domain not in self.domain_stats["discovered_domains"]:
                self.domain_stats["discovered_domains"].add(domain)
                logger.info(f"ğŸ†• ìƒˆë¡œìš´ ë„ë©”ì¸ ë°œê²¬: {domain} (ì‹ ë¢°ë„: {confidence:.2f})")
            
            # ë„ë©”ì¸ ë¹ˆë„ ì—…ë°ì´íŠ¸
            if domain not in self.domain_stats["domain_frequency"]:
                self.domain_stats["domain_frequency"][domain] = 0
            self.domain_stats["domain_frequency"][domain] += 1
            
            # ì‹ ë¢°ë„ ë¶„í¬ ì—…ë°ì´íŠ¸
            self.domain_stats["confidence_distribution"].append({
                "domain": domain,
                "confidence": confidence,
                "query": current_query[:50] + "..." if len(current_query) > 50 else current_query,
                "timestamp": datetime.now().isoformat()
            })
            
            # ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.domain_stats["confidence_distribution"]) > 100:
                self.domain_stats["confidence_distribution"] = self.domain_stats["confidence_distribution"][-50:]
            
            # ê³ í’ˆì§ˆ ë„ë©”ì¸ ë¶„ë¥˜ ìºì‹± (ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ)
            if confidence >= self.domain_confidence_threshold:
                cache_key = hash(current_query[:100])  # ì¿¼ë¦¬ ê¸°ë°˜ ìºì‹œ í‚¤
                self.domain_cache[cache_key] = {
                    "domain": domain,
                    "context": context,
                    "confidence": confidence,
                    "cached_at": datetime.now().isoformat()
                }
                
                # ìºì‹œ í¬ê¸° ì œí•œ
                if len(self.domain_cache) > self.cache_max_size:
                    # ì˜¤ë˜ëœ ìºì‹œ ì œê±° (FIFO)
                    oldest_key = min(self.domain_cache.keys(), 
                                   key=lambda k: self.domain_cache[k]["cached_at"])
                    del self.domain_cache[oldest_key]
            
            # ì£¼ê¸°ì  ë¡œê¹… (ë§¤ 10ë²ˆì§¸ ë¶„ë¥˜ë§ˆë‹¤)
            if self.domain_stats["total_classifications"] % 10 == 0:
                logger.info(f"""
ğŸ“Š ë„ë©”ì¸ í•™ìŠµ í˜„í™©:
- ì´ ë¶„ë¥˜ íšŸìˆ˜: {self.domain_stats['total_classifications']}
- ê³ ì‹ ë¢°ë„ ë¶„ë¥˜: {self.domain_stats['high_confidence_count']}
- ë°œê²¬ëœ ë„ë©”ì¸: {len(self.domain_stats['discovered_domains'])}
- ìƒìœ„ ë„ë©”ì¸: {dict(sorted(self.domain_stats['domain_frequency'].items(), key=lambda x: x[1], reverse=True)[:5])}
""")
                
        except Exception as e:
            logger.error(f"ë„ë©”ì¸ í•™ìŠµ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def get_domain_statistics(self) -> Dict[str, Any]:
        """ë„ë©”ì¸ í•™ìŠµ í†µê³„ ì¡°íšŒ"""
        return {
            "total_classifications": self.domain_stats["total_classifications"],
            "high_confidence_count": self.domain_stats["high_confidence_count"],
            "high_confidence_rate": (
                self.domain_stats["high_confidence_count"] / self.domain_stats["total_classifications"]
                if self.domain_stats["total_classifications"] > 0 else 0
            ),
            "discovered_domains": list(self.domain_stats["discovered_domains"]),
            "domain_count": len(self.domain_stats["discovered_domains"]),
            "domain_frequency": self.domain_stats["domain_frequency"],
            "recent_classifications": self.domain_stats["confidence_distribution"][-10:],
            "cache_size": len(self.domain_cache)
        }
    
    def _extract_basic_context(
        self, 
        recent_messages: List[Dict[str, Any]], 
        current_query: str,
        search_queries: List[str]
    ) -> ConversationContext:
        """LLM ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ì¸ ë™ì  ë§¥ë½ ì¶”ì¶œ"""
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_content = ' '.join([msg['content'] for msg in recent_messages])
        
        # ë™ì  í‚¤ì›Œë“œ ì¶”ì¶œ (íŒ¨í„´ ë§¤ì¹­ ì—†ì´)
        import re
        korean_keywords = re.findall(r'[ê°€-í£]{2,}', all_content)
        english_keywords = re.findall(r'\b[A-Za-z]{3,}\b', all_content)
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        from collections import Counter
        all_keywords = korean_keywords + english_keywords
        keyword_freq = Counter(all_keywords)
        top_keywords = [word for word, _ in keyword_freq.most_common(5)]
        
        # ì—°ì†ì„± ì§€ì‹œì–´ ê°ì§€ (ë™ì )
        continuation_patterns = ['ê´€ë ¨ëœ', 'ì´ì™€', 'ê·¸ê²ƒ', 'ê·¸ê±°', 'ì¶”ì²œ', 'ìµœì‹ ', 'ì‹ ê°„', 'ë”', 'ìì„¸íˆ']
        has_continuation = any(pattern in current_query for pattern in continuation_patterns)
        
        # ì§ˆë¬¸ ê¹Šì´ ìˆ˜ì¤€ íŒë‹¨ (ë™ì )
        depth_level = "basic"
        if any(word in current_query for word in ['êµ¬ì²´ì ', 'ìì„¸íˆ', 'ì‹¬í™”', 'ì „ë¬¸ì ']):
            depth_level = "advanced"
        elif any(word in current_query for word in ['ë¹„êµ', 'ì°¨ì´', 'ë¶„ì„']):
            depth_level = "intermediate"
        
        return ConversationContext(
            recent_messages=recent_messages,
            conversation_topics=top_keywords[:3],
            mentioned_entities=top_keywords,
            previous_search_queries=search_queries[-5:],
            conversation_flow="ë™ì  íŒ¨í„´ ë¶„ì„ ê²°ê³¼",
            current_focus_topic=top_keywords[0] if top_keywords else None,
            question_depth_level=depth_level,
            # ë™ì  ë„ë©”ì¸ ë¶„ë¥˜ í•„ë“œë“¤ (ê¸°ë³¸ê°’)
            domain="general",
            domain_confidence=0.3,  # íŒ¨í„´ ê¸°ë°˜ì´ë¯€ë¡œ ë‚®ì€ ì‹ ë¢°ë„
            main_domain="general",
            sub_domains=top_keywords[:2] if len(top_keywords) > 1 else [],
            topic_evolution=top_keywords[:3] if top_keywords else [],
            user_intent="ì •ë³´ìˆ˜ì§‘" if not has_continuation else "ì¶”ê°€ì •ë³´ìš”ì²­",
            context_connection="ì´ì „ ëŒ€í™”ì™€ ì—°ê´€" if has_continuation else "ìƒˆë¡œìš´ ì£¼ì œ",
            search_focus="ì£¼ìš” í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰",
            optimal_search_queries=[current_query],  # ê¸°ë³¸ì ìœ¼ë¡œëŠ” í˜„ì¬ ì§ˆë¬¸ ì‚¬ìš©
            # ë‹¤ì°¨ì› ë™ì  ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’)
            dynamic_categories={
                "complexity": "simple",
                "urgency": "low", 
                "scope": "narrow",
                "novelty": "familiar"
            }
        )


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
universal_context_analyzer = UniversalContextAnalyzer()