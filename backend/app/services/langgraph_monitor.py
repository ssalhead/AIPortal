"""
LangGraph ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ - Legacy vs LangGraph ì„±ëŠ¥ ë¹„êµ ë° ì¶”ì 
"""

import time
import json
import asyncio
import statistics
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import logging

from app.core.config import settings

logger = logging.getLogger(__name__)


class AgentType(Enum):
    """ì—ì´ì „íŠ¸ ìœ í˜•"""
    LEGACY = "legacy"
    LANGGRAPH = "langgraph"


class ExecutionStatus(Enum):
    """ì‹¤í–‰ ìƒíƒœ"""
    SUCCESS = "success"
    ERROR = "error"
    TIMEOUT = "timeout"
    FALLBACK = "fallback"


@dataclass
class ExecutionMetric:
    """ì‹¤í–‰ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    agent_type: AgentType
    agent_name: str
    execution_time: float
    memory_usage: Optional[float]
    status: ExecutionStatus
    user_id: Optional[str]
    query: str
    response_length: int
    timestamp: datetime
    error_message: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class PerformanceReport:
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""
    period_start: datetime
    period_end: datetime
    legacy_metrics: Dict[str, Any]
    langgraph_metrics: Dict[str, Any]
    comparison: Dict[str, Any]
    recommendations: List[str]


class LangGraphMonitor:
    """LangGraph vs Legacy ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        # ë©”íŠ¸ë¦­ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ê¸°ë°˜, ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” DB ì €ì¥ ê¶Œì¥)
        self.execution_metrics: List[ExecutionMetric] = []
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
        self.performance_thresholds = {
            "response_time_warning": 5.0,     # 5ì´ˆ ì´ìƒ ê²½ê³ 
            "response_time_critical": 10.0,   # 10ì´ˆ ì´ìƒ ì‹¬ê°
            "error_rate_warning": 0.05,       # 5% ì´ìƒ ê²½ê³ 
            "error_rate_critical": 0.10,      # 10% ì´ìƒ ì‹¬ê°
            "memory_usage_warning": 512,      # 512MB ì´ìƒ ê²½ê³ 
            "fallback_rate_warning": 0.02     # 2% ì´ìƒ fallback ê²½ê³ 
        }
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ (ìµœê·¼ 1ì‹œê°„)
        self.realtime_window = timedelta(hours=1)
        
        # ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ìºì‹œ
        self.report_cache = {}
        self.cache_ttl = timedelta(minutes=5)
        
        # ì„±ëŠ¥ ìµœì í™” ì—°ë™
        self.optimization_enabled = True
        self.performance_optimizer = None  # ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ ì§€ì—° ë¡œë”©
        
        logger.info("ğŸ” LangGraph ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def start_execution(self, agent_name: str) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ ì¶”ì 
        
        Args:
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            
        Returns:
            ì‹œì‘ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        """
        start_time = time.time()
        context = {
            "agent_name": agent_name,
            "start_time": start_time,
            "timestamp": datetime.now()
        }
        logger.debug(f"ğŸš€ {agent_name} ì‹¤í–‰ ì‹œì‘ ì¶”ì ")
        return context
    
    async def track_execution(
        self,
        agent_type: AgentType,
        agent_name: str,
        execution_time: float,
        status: ExecutionStatus,
        query: str,
        response_length: int,
        user_id: Optional[str] = None,
        memory_usage: Optional[float] = None,
        error_message: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs  # í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì¸ìˆ˜ ì§€ì›
    ) -> None:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰ ë©”íŠ¸ë¦­ ì¶”ì 
        
        Args:
            agent_type: ì—ì´ì „íŠ¸ ìœ í˜• (Legacy/LangGraph)
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
            execution_time: ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            status: ì‹¤í–‰ ìƒíƒœ
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            response_length: ì‘ë‹µ ê¸¸ì´
            user_id: ì‚¬ìš©ì ID
            memory_usage: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        """
        
        metric = ExecutionMetric(
            agent_type=agent_type,
            agent_name=agent_name,
            execution_time=execution_time,
            memory_usage=memory_usage,
            status=status,
            user_id=user_id,
            query=query[:100],  # ì¿¼ë¦¬ëŠ” 100ìê¹Œì§€ë§Œ ì €ì¥
            response_length=response_length,
            timestamp=datetime.now(),
            error_message=error_message,
            metadata=metadata or {}
        )
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.execution_metrics.append(metric)
        
        # ì„ê³„ê°’ í™•ì¸ ë° ì•ŒëŒ
        await self._check_thresholds(metric)
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìµœê·¼ 24ì‹œê°„ ë°ì´í„°ë§Œ ìœ ì§€)
        self._cleanup_old_metrics()
        
        logger.debug(f"ğŸ“Š ë©”íŠ¸ë¦­ ì¶”ê°€: {agent_type.value} {agent_name} - {execution_time:.2f}s ({status.value})")
    
    async def _check_thresholds(self, metric: ExecutionMetric) -> None:
        """ì„±ëŠ¥ ì„ê³„ê°’ í™•ì¸ ë° ì•ŒëŒ"""
        
        alerts = []
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        if metric.execution_time > self.performance_thresholds["response_time_critical"]:
            alerts.append(f"ğŸš¨ CRITICAL: {metric.agent_name} ì‘ë‹µ ì‹œê°„ {metric.execution_time:.2f}s (ì„ê³„ê°’: {self.performance_thresholds['response_time_critical']}s)")
        elif metric.execution_time > self.performance_thresholds["response_time_warning"]:
            alerts.append(f"âš ï¸ WARNING: {metric.agent_name} ì‘ë‹µ ì‹œê°„ {metric.execution_time:.2f}s (ê²½ê³ ê°’: {self.performance_thresholds['response_time_warning']}s)")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if metric.memory_usage and metric.memory_usage > self.performance_thresholds["memory_usage_warning"]:
            alerts.append(f"âš ï¸ WARNING: {metric.agent_name} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ {metric.memory_usage:.1f}MB")
        
        # ì—ëŸ¬ ìƒíƒœ í™•ì¸
        if metric.status == ExecutionStatus.ERROR:
            alerts.append(f"âŒ ERROR: {metric.agent_name} ì‹¤í–‰ ì‹¤íŒ¨ - {metric.error_message}")
        elif metric.status == ExecutionStatus.FALLBACK:
            alerts.append(f"ğŸ”„ FALLBACK: {metric.agent_name} Legacyë¡œ fallback")
        
        # ì•ŒëŒ ë°œì†¡
        for alert in alerts:
            logger.warning(alert)
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì•ŒëŒ ë°œì†¡
    
    def _cleanup_old_metrics(self) -> None:
        """ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ë°ì´í„° ì •ë¦¬ (24ì‹œê°„ ì´ìƒ)"""
        cutoff_time = datetime.now() - timedelta(hours=24)
        original_count = len(self.execution_metrics)
        
        self.execution_metrics = [
            metric for metric in self.execution_metrics
            if metric.timestamp > cutoff_time
        ]
        
        removed_count = original_count - len(self.execution_metrics)
        if removed_count > 0:
            logger.debug(f"ğŸ§¹ ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ {removed_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
    
    async def get_realtime_metrics(self) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ (ìµœê·¼ 1ì‹œê°„)"""
        
        cutoff_time = datetime.now() - self.realtime_window
        recent_metrics = [
            metric for metric in self.execution_metrics
            if metric.timestamp > cutoff_time
        ]
        
        if not recent_metrics:
            return {
                "status": "no_data",
                "message": "ìµœê·¼ 1ì‹œê°„ ë‚´ ì‹¤í–‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
                "timestamp": datetime.now().isoformat()
            }
        
        # Legacy vs LangGraph ë¶„ë¦¬
        legacy_metrics = [m for m in recent_metrics if m.agent_type == AgentType.LEGACY]
        langgraph_metrics = [m for m in recent_metrics if m.agent_type == AgentType.LANGGRAPH]
        
        return {
            "period": {
                "start": cutoff_time.isoformat(),
                "end": datetime.now().isoformat(),
                "duration_minutes": 60
            },
            "summary": {
                "total_executions": len(recent_metrics),
                "legacy_executions": len(legacy_metrics),
                "langgraph_executions": len(langgraph_metrics),
                "langgraph_adoption_rate": len(langgraph_metrics) / len(recent_metrics) * 100 if recent_metrics else 0
            },
            "legacy": self._calculate_agent_metrics(legacy_metrics),
            "langgraph": self._calculate_agent_metrics(langgraph_metrics),
            "comparison": self._compare_metrics(legacy_metrics, langgraph_metrics),
            "timestamp": datetime.now().isoformat()
        }
    
    def _calculate_agent_metrics(self, metrics: List[ExecutionMetric]) -> Dict[str, Any]:
        """íŠ¹ì • ì—ì´ì „íŠ¸ ìœ í˜•ì˜ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        if not metrics:
            return {
                "execution_count": 0,
                "avg_response_time": 0,
                "success_rate": 0,
                "error_rate": 0,
                "fallback_rate": 0
            }
        
        # ì‹¤í–‰ ì‹œê°„ í†µê³„
        execution_times = [m.execution_time for m in metrics]
        
        # ìƒíƒœë³„ ì¹´ìš´íŠ¸
        success_count = len([m for m in metrics if m.status == ExecutionStatus.SUCCESS])
        error_count = len([m for m in metrics if m.status == ExecutionStatus.ERROR])
        fallback_count = len([m for m in metrics if m.status == ExecutionStatus.FALLBACK])
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
        memory_usages = [m.memory_usage for m in metrics if m.memory_usage is not None]
        
        return {
            "execution_count": len(metrics),
            "avg_response_time": statistics.mean(execution_times),
            "median_response_time": statistics.median(execution_times),
            "p95_response_time": self._percentile(execution_times, 95),
            "min_response_time": min(execution_times),
            "max_response_time": max(execution_times),
            "success_rate": success_count / len(metrics) * 100,
            "error_rate": error_count / len(metrics) * 100,
            "fallback_rate": fallback_count / len(metrics) * 100,
            "avg_memory_usage": statistics.mean(memory_usages) if memory_usages else None,
            "avg_response_length": statistics.mean([m.response_length for m in metrics])
        }
    
    def _compare_metrics(self, legacy_metrics: List[ExecutionMetric], langgraph_metrics: List[ExecutionMetric]) -> Dict[str, Any]:
        """Legacy vs LangGraph ë©”íŠ¸ë¦­ ë¹„êµ"""
        
        if not legacy_metrics or not langgraph_metrics:
            return {"status": "insufficient_data"}
        
        legacy_stats = self._calculate_agent_metrics(legacy_metrics)
        langgraph_stats = self._calculate_agent_metrics(langgraph_metrics)
        
        # ì„±ëŠ¥ ê°œì„ ìœ¨ ê³„ì‚°
        response_time_improvement = self._calculate_improvement(
            legacy_stats["avg_response_time"],
            langgraph_stats["avg_response_time"],
            lower_is_better=True
        )
        
        success_rate_improvement = self._calculate_improvement(
            legacy_stats["success_rate"],
            langgraph_stats["success_rate"],
            lower_is_better=False
        )
        
        return {
            "response_time": {
                "legacy_avg": legacy_stats["avg_response_time"],
                "langgraph_avg": langgraph_stats["avg_response_time"],
                "improvement_percent": response_time_improvement,
                "winner": "langgraph" if response_time_improvement > 0 else "legacy"
            },
            "success_rate": {
                "legacy": legacy_stats["success_rate"],
                "langgraph": langgraph_stats["success_rate"],
                "improvement_percent": success_rate_improvement,
                "winner": "langgraph" if success_rate_improvement > 0 else "legacy"
            },
            "error_rate": {
                "legacy": legacy_stats["error_rate"],
                "langgraph": langgraph_stats["error_rate"],
                "winner": "langgraph" if langgraph_stats["error_rate"] < legacy_stats["error_rate"] else "legacy"
            },
            "overall_recommendation": self._get_overall_recommendation(legacy_stats, langgraph_stats)
        }
    
    def _calculate_improvement(self, baseline: float, new_value: float, lower_is_better: bool = True) -> float:
        """ì„±ëŠ¥ ê°œì„ ìœ¨ ê³„ì‚°"""
        
        if baseline == 0:
            return 0
        
        if lower_is_better:
            # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (ì‘ë‹µ ì‹œê°„, ì—ëŸ¬ìœ¨ ë“±)
            improvement = (baseline - new_value) / baseline * 100
        else:
            # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (ì„±ê³µë¥  ë“±)
            improvement = (new_value - baseline) / baseline * 100
        
        return round(improvement, 2)
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        if not data:
            return 0
        
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower = sorted_data[int(index)]
            upper = sorted_data[int(index) + 1]
            return lower + (upper - lower) * (index - int(index))
    
    def _get_overall_recommendation(self, legacy_stats: Dict[str, Any], langgraph_stats: Dict[str, Any]) -> str:
        """ì „ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì‘ë‹µ ì‹œê°„ ë¹„êµ
        if langgraph_stats["avg_response_time"] < legacy_stats["avg_response_time"]:
            improvement = self._calculate_improvement(
                legacy_stats["avg_response_time"],
                langgraph_stats["avg_response_time"],
                lower_is_better=True
            )
            recommendations.append(f"LangGraphê°€ ì‘ë‹µ ì‹œê°„ {improvement:.1f}% ê°œì„ ")
        
        # ì„±ê³µë¥  ë¹„êµ
        if langgraph_stats["success_rate"] > legacy_stats["success_rate"]:
            recommendations.append("LangGraphì˜ ì„±ê³µë¥ ì´ ë” ë†’ìŒ")
        
        # ì—ëŸ¬ìœ¨ ë¹„êµ
        if langgraph_stats["error_rate"] < legacy_stats["error_rate"]:
            recommendations.append("LangGraphì˜ ì—ëŸ¬ìœ¨ì´ ë” ë‚®ìŒ")
        
        # Fallback ë¹„ìœ¨ í™•ì¸
        if langgraph_stats["fallback_rate"] > 5:
            recommendations.append("âš ï¸ LangGraph fallback ë¹„ìœ¨ì´ ë†’ìŒ - ì•ˆì •ì„± ì ê²€ í•„ìš”")
        
        if not recommendations:
            return "ì„±ëŠ¥ìƒ í° ì°¨ì´ ì—†ìŒ - ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”"
        
        return " | ".join(recommendations)
    
    async def get_daily_report(self, date: Optional[datetime] = None) -> PerformanceReport:
        """ì¼ì¼ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        if date is None:
            date = datetime.now().date()
        
        # ìºì‹œ í™•ì¸
        cache_key = f"daily_report_{date.isoformat()}"
        if cache_key in self.report_cache:
            cached_report, cache_time = self.report_cache[cache_key]
            if datetime.now() - cache_time < self.cache_ttl:
                return cached_report
        
        # í•´ë‹¹ ë‚ ì§œì˜ ë©”íŠ¸ë¦­ í•„í„°ë§
        start_time = datetime.combine(date, datetime.min.time())
        end_time = start_time + timedelta(days=1)
        
        daily_metrics = [
            metric for metric in self.execution_metrics
            if start_time <= metric.timestamp < end_time
        ]
        
        if not daily_metrics:
            return PerformanceReport(
                period_start=start_time,
                period_end=end_time,
                legacy_metrics={},
                langgraph_metrics={},
                comparison={},
                recommendations=["í•´ë‹¹ ë‚ ì§œì— ì‹¤í–‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"]
            )
        
        # Legacy vs LangGraph ë¶„ë¦¬
        legacy_metrics = [m for m in daily_metrics if m.agent_type == AgentType.LEGACY]
        langgraph_metrics = [m for m in daily_metrics if m.agent_type == AgentType.LANGGRAPH]
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = PerformanceReport(
            period_start=start_time,
            period_end=end_time,
            legacy_metrics=self._calculate_agent_metrics(legacy_metrics),
            langgraph_metrics=self._calculate_agent_metrics(langgraph_metrics),
            comparison=self._compare_metrics(legacy_metrics, langgraph_metrics),
            recommendations=self._generate_recommendations(legacy_metrics, langgraph_metrics)
        )
        
        # ìºì‹œ ì €ì¥
        self.report_cache[cache_key] = (report, datetime.now())
        
        return report
    
    def _generate_recommendations(self, legacy_metrics: List[ExecutionMetric], langgraph_metrics: List[ExecutionMetric]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        if not langgraph_metrics:
            recommendations.append("LangGraph í™œìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. Feature Flag ë¹„ìœ¨ì„ ë†’ì—¬ë³´ì„¸ìš”.")
            return recommendations
        
        if not legacy_metrics:
            recommendations.append("Legacy ë©”íŠ¸ë¦­ì´ ì—†ì–´ ë¹„êµê°€ ì–´ë µìŠµë‹ˆë‹¤.")
            return recommendations
        
        legacy_stats = self._calculate_agent_metrics(legacy_metrics)
        langgraph_stats = self._calculate_agent_metrics(langgraph_metrics)
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if langgraph_stats["avg_response_time"] < legacy_stats["avg_response_time"]:
            improvement = self._calculate_improvement(
                legacy_stats["avg_response_time"],
                langgraph_stats["avg_response_time"],
                lower_is_better=True
            )
            if improvement > 20:
                recommendations.append(f"ğŸš€ LangGraph ì„±ëŠ¥ì´ {improvement:.1f}% ìš°ìˆ˜í•¨ - Feature Flag ë¹„ìœ¨ ì¦ê°€ ê¶Œì¥")
            else:
                recommendations.append(f"âœ… LangGraph ì„±ëŠ¥ì´ {improvement:.1f}% ê°œì„ ë¨")
        
        # ì•ˆì •ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if langgraph_stats["error_rate"] > legacy_stats["error_rate"]:
            recommendations.append("âš ï¸ LangGraph ì—ëŸ¬ìœ¨ì´ ë†’ìŒ - ì•ˆì •ì„± ê°œì„  í•„ìš”")
        
        if langgraph_stats["fallback_rate"] > 5:
            recommendations.append("ğŸ”„ Fallback ë¹„ìœ¨ì´ ë†’ìŒ - LangGraph ì•ˆì •ì„± ì ê²€ í•„ìš”")
        
        # íŠ¸ë˜í”½ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        total_executions = len(legacy_metrics) + len(langgraph_metrics)
        langgraph_ratio = len(langgraph_metrics) / total_executions * 100
        
        if langgraph_ratio < 10:
            recommendations.append("ğŸ“ˆ LangGraph íŠ¸ë˜í”½ ë¹„ìœ¨ì´ ë‚®ìŒ - ì ì§„ì  ì¦ê°€ ê³ ë ¤")
        elif langgraph_ratio > 50:
            recommendations.append("ğŸ¯ LangGraph í™œìš©ë¥ ì´ ë†’ìŒ - ì™„ì „ ì „í™˜ ì¤€ë¹„ ê²€í† ")
        
        return recommendations
    
    async def export_metrics(self, format: str = "json", period_hours: int = 24) -> str:
        """ë©”íŠ¸ë¦­ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        
        cutoff_time = datetime.now() - timedelta(hours=period_hours)
        export_metrics = [
            metric for metric in self.execution_metrics
            if metric.timestamp > cutoff_time
        ]
        
        if format == "json":
            return json.dumps([asdict(metric) for metric in export_metrics], default=str, indent=2)
        elif format == "csv":
            # CSV í˜•íƒœë¡œ ë³€í™˜ (ê°„ë‹¨ êµ¬í˜„)
            lines = ["timestamp,agent_type,agent_name,execution_time,status,response_length"]
            for metric in export_metrics:
                lines.append(f"{metric.timestamp},{metric.agent_type.value},{metric.agent_name},{metric.execution_time},{metric.status.value},{metric.response_length}")
            return "\n".join(lines)
        else:
            raise ValueError(f"Unsupported format: {format}")
    
    def _get_performance_optimizer(self):
        """ì„±ëŠ¥ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ì§€ì—° ë¡œë”©"""
        if self.performance_optimizer is None and self.optimization_enabled:
            try:
                from app.services.performance_optimizer import performance_optimizer
                self.performance_optimizer = performance_optimizer
            except ImportError:
                logger.warning("ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                self.optimization_enabled = False
        return self.performance_optimizer
    
    async def trigger_performance_analysis(self):
        """ì„±ëŠ¥ ë¶„ì„ íŠ¸ë¦¬ê±°"""
        optimizer = self._get_performance_optimizer()
        if optimizer:
            try:
                report = optimizer.get_performance_report()
                
                # ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œ ê°ì§€ ì‹œ ì•Œë¦¼
                if report.get("performance_level") in ["critical", "poor"]:
                    logger.warning(f"ì„±ëŠ¥ ë¬¸ì œ ê°ì§€: {report.get('performance_level')} - ì ìˆ˜: {report.get('performance_score')}")
                    
                    # ìë™ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
                    recommendations = report.get("recommendations", [])
                    if recommendations:
                        logger.info(f"ìë™ ìµœì í™” ê¶Œì¥ì‚¬í•­: {', '.join(recommendations)}")
                
                return report
            except Exception as e:
                logger.error(f"ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None
    
    async def get_comprehensive_report(self) -> Dict[str, Any]:
        """LangGraph ëª¨ë‹ˆí„°ë§ + ì„±ëŠ¥ ìµœì í™” í†µí•© ë¦¬í¬íŠ¸"""
        
        # ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ë°ì´í„°
        monitoring_report = await self.get_realtime_metrics()
        
        # ì„±ëŠ¥ ìµœì í™” ë°ì´í„°
        performance_report = await self.trigger_performance_analysis()
        
        # í†µí•© ë¦¬í¬íŠ¸ êµ¬ì„±
        comprehensive_report = {
            "timestamp": datetime.now().isoformat(),
            "monitoring": monitoring_report,
            "performance": performance_report or {"status": "optimizer_unavailable"},
            "system_health": self._assess_system_health(monitoring_report, performance_report),
            "recommendations": self._generate_comprehensive_recommendations(monitoring_report, performance_report)
        }
        
        return comprehensive_report
    
    def _assess_system_health(self, monitoring_report: Dict[str, Any], performance_report: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì „ì²´ ê±´ê°•ë„ í‰ê°€"""
        
        health_score = 100
        issues = []
        
        # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë°˜ í‰ê°€
        if monitoring_report.get("status") != "no_data":
            langgraph_metrics = monitoring_report.get("langgraph", {})
            
            # ì—ëŸ¬ìœ¨ í™•ì¸
            error_rate = langgraph_metrics.get("error_rate", 0)
            if error_rate > 0.1:  # 10% ì´ìƒ
                health_score -= 30
                issues.append("ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€")
            elif error_rate > 0.05:  # 5% ì´ìƒ
                health_score -= 15
                issues.append("ì—ëŸ¬ìœ¨ ì£¼ì˜ í•„ìš”")
            
            # ì‘ë‹µì‹œê°„ í™•ì¸
            avg_response_time = langgraph_metrics.get("avg_response_time", 0)
            if avg_response_time > 10:  # 10ì´ˆ ì´ìƒ
                health_score -= 25
                issues.append("ì‘ë‹µì‹œê°„ ì‹¬ê°")
            elif avg_response_time > 5:  # 5ì´ˆ ì´ìƒ
                health_score -= 10
                issues.append("ì‘ë‹µì‹œê°„ ëŠë¦¼")
        
        # ì„±ëŠ¥ ìµœì í™” ë°ì´í„° ê¸°ë°˜ í‰ê°€
        if performance_report:
            performance_level = performance_report.get("performance_level")
            if performance_level == "critical":
                health_score -= 40
                issues.append("ì‹œìŠ¤í…œ ì„±ëŠ¥ ìœ„í—˜")
            elif performance_level == "poor":
                health_score -= 20
                issues.append("ì‹œìŠ¤í…œ ì„±ëŠ¥ ì €í•˜")
            elif performance_level == "moderate":
                health_score -= 10
                issues.append("ì„±ëŠ¥ ê°œì„  ì—¬ì§€")
        
        # ê±´ê°•ë„ ë“±ê¸‰ ê²°ì •
        if health_score >= 90:
            health_grade = "excellent"
        elif health_score >= 70:
            health_grade = "good"
        elif health_score >= 50:
            health_grade = "moderate"
        elif health_score >= 30:
            health_grade = "poor"
        else:
            health_grade = "critical"
        
        return {
            "health_score": max(0, health_score),
            "health_grade": health_grade,
            "issues": issues,
            "status": "healthy" if health_score >= 70 else "attention_needed"
        }
    
    def _generate_comprehensive_recommendations(self, monitoring_report: Dict[str, Any], performance_report: Optional[Dict[str, Any]]) -> List[str]:
        """í†µí•© ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ëª¨ë‹ˆí„°ë§ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if monitoring_report.get("status") != "no_data":
            comparison = monitoring_report.get("comparison", {})
            if comparison.get("performance_improvement", 0) < 0:
                recommendations.append("LangGraph ì„±ëŠ¥ì´ Legacyë³´ë‹¤ ë‚®ìŒ - ìµœì í™” í•„ìš”")
            
            langgraph_metrics = monitoring_report.get("langgraph", {})
            if langgraph_metrics.get("error_rate", 0) > 0.05:
                recommendations.append("LangGraph ì—ëŸ¬ìœ¨ ê°œì„  í•„ìš”")
        
        # ì„±ëŠ¥ ìµœì í™” ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if performance_report:
            perf_recommendations = performance_report.get("recommendations", [])
            recommendations.extend(perf_recommendations)
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("í˜„ì¬ ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ ì¤‘ì…ë‹ˆë‹¤")
        
        return recommendations


# ì „ì—­ ëª¨ë‹ˆí„°ë§ ì¸ìŠ¤í„´ìŠ¤
langgraph_monitor = LangGraphMonitor()


# í¸ì˜ í•¨ìˆ˜ë“¤
async def track_legacy_execution(agent_name: str, execution_time: float, status: ExecutionStatus, query: str, response_length: int, **kwargs):
    """Legacy ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¶”ì """
    await langgraph_monitor.track_execution(
        agent_type=AgentType.LEGACY,
        agent_name=agent_name,
        execution_time=execution_time,
        status=status,
        query=query,
        response_length=response_length,
        **kwargs
    )


async def track_langgraph_execution(agent_name: str, execution_time: float, status: ExecutionStatus, query: str, response_length: int, **kwargs):
    """LangGraph ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¶”ì """
    await langgraph_monitor.track_execution(
        agent_type=AgentType.LANGGRAPH,
        agent_name=agent_name,
        execution_time=execution_time,
        status=status,
        query=query,
        response_length=response_length,
        **kwargs
    )


class PerformanceTracker:
    """ì„±ëŠ¥ ì¶”ì ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    
    def __init__(self, agent_type: AgentType, agent_name: str, query: str, user_id: Optional[str] = None):
        self.agent_type = agent_type
        self.agent_name = agent_name
        self.query = query
        self.user_id = user_id
        self.start_time = None
        self.response_length = 0
        self.error_message = None
    
    async def __aenter__(self):
        self.start_time = time.time()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        execution_time = time.time() - self.start_time
        
        if exc_type is None:
            status = ExecutionStatus.SUCCESS
        else:
            status = ExecutionStatus.ERROR
            self.error_message = str(exc_val)
        
        await langgraph_monitor.track_execution(
            agent_type=self.agent_type,
            agent_name=self.agent_name,
            execution_time=execution_time,
            status=status,
            query=self.query,
            response_length=self.response_length,
            user_id=self.user_id,
            error_message=self.error_message
        )
    
    def set_response_length(self, length: int):
        """ì‘ë‹µ ê¸¸ì´ ì„¤ì •"""
        self.response_length = length