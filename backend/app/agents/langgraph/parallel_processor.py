"""
LangGraph ê¸°ë°˜ ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ

ìµœì‹  LangGraph StateGraphë¥¼ í™œìš©í•˜ì—¬ Fan-out/Fan-in íŒ¨í„´ê³¼ Reducerë¥¼ êµ¬í˜„í•œ
ìµœê³  ì„±ëŠ¥ì˜ ë³‘ë ¬ ì²˜ë¦¬ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import time
import asyncio
import json
import uuid
from typing import Dict, Any, List, Optional, TypedDict, Union, Annotated, Callable
from datetime import datetime
from enum import Enum
from dataclasses import dataclass
import logging
import operator
from concurrent.futures import ThreadPoolExecutor, as_completed
import functools

# LangGraph í•µì‹¬ imports
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.types import Send
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# ê¸°ì¡´ ì‹œìŠ¤í…œ imports
from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.core.config import settings
from app.core.feature_flags import is_langgraph_enabled, LangGraphFeatureFlags
from app.services.langgraph_monitor import langgraph_monitor
from app.services.search_service import search_service
from app.services.web_crawler import web_crawler

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìœ í˜•"""
    SEARCH = "search"
    WEB_CRAWL = "web_crawl"
    LLM_ANALYSIS = "llm_analysis"
    DATA_PROCESSING = "data_processing"
    CONTENT_GENERATION = "content_generation"
    SIMILARITY_ANALYSIS = "similarity_analysis"


class ProcessingStrategy(Enum):
    """ì²˜ë¦¬ ì „ëµ"""
    CONCURRENT_ALL = "concurrent_all"        # ëª¨ë“  ì‘ì—… ë™ì‹œ ì‹¤í–‰
    BATCHED_PARALLEL = "batched_parallel"    # ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
    ADAPTIVE_LOAD = "adaptive_load"          # ë¶€í•˜ ì ì‘í˜• ì²˜ë¦¬
    PRIORITY_BASED = "priority_based"        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬


@dataclass
class ParallelTask:
    """ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ì •ì˜"""
    task_id: str
    task_type: TaskType
    task_data: Dict[str, Any]
    priority: int = 1
    timeout: float = 30.0
    retry_count: int = 3
    dependencies: List[str] = None


@dataclass
class TaskResult:
    """ì‘ì—… ê²°ê³¼"""
    task_id: str
    success: bool
    result: Any
    execution_time: float
    error: Optional[str] = None
    retry_attempt: int = 0
    metadata: Dict[str, Any] = None


class ParallelProcessingState(TypedDict):
    """LangGraph ë³‘ë ¬ ì²˜ë¦¬ ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    original_query: str
    user_id: str
    session_id: Optional[str]
    model: str
    
    # ì‘ì—… ê³„íš
    processing_strategy: Optional[str]
    task_breakdown: Optional[List[Dict[str, Any]]]
    dependency_graph: Optional[Dict[str, List[str]]]
    
    # ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ (Reducer ì‚¬ìš©)
    search_results: Annotated[List[Dict[str, Any]], operator.add]
    analysis_results: Annotated[List[Dict[str, Any]], operator.add]
    generation_results: Annotated[List[Dict[str, Any]], operator.add]
    processing_results: Annotated[List[Dict[str, Any]], operator.add]
    
    # í†µí•© ê²°ê³¼
    aggregated_results: Optional[Dict[str, Any]]
    final_output: Optional[str]
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    execution_metadata: Dict[str, Any]
    performance_metrics: Dict[str, Any]
    parallel_efficiency: float
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: Annotated[List[str], operator.add]
    task_failures: Annotated[List[Dict[str, Any]], operator.add]
    should_fallback: bool


class LangGraphParallelProcessor(BaseAgent):
    """LangGraph ê¸°ë°˜ ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        super().__init__(
            agent_id="langgraph_parallel_processor",
            name="LangGraph ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ",
            description="LangGraph StateGraphë¡œ êµ¬í˜„ëœ ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬ ì—ì´ì „íŠ¸"
        )
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()
        
        # PostgreSQL ì²´í¬í¬ì¸í„° ì„¤ì •
        if settings.DATABASE_URL:
            self.checkpointer = PostgresSaver.from_conn_string(
                settings.DATABASE_URL,
                
            )
        else:
            self.checkpointer = None
            logger.warning("DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì²´í¬í¬ì¸í„° ë¹„í™œì„±í™”")
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        self.max_concurrent_tasks = 10
        self.batch_size = 5
        self.thread_pool = ThreadPoolExecutor(max_workers=20)

    def _build_workflow(self) -> StateGraph:
        """LangGraph ë³‘ë ¬ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        
        # StateGraph ìƒì„± (Reducer ì‚¬ìš©)
        workflow = StateGraph(ParallelProcessingState)
        
        # ë…¸ë“œ ì •ì˜ - ë³‘ë ¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        workflow.add_node("plan_parallel_tasks", self._plan_parallel_tasks_node)
        workflow.add_node("execute_search_tasks", self._execute_search_tasks_node)
        workflow.add_node("execute_analysis_tasks", self._execute_analysis_tasks_node)
        workflow.add_node("execute_generation_tasks", self._execute_generation_tasks_node)
        workflow.add_node("execute_processing_tasks", self._execute_processing_tasks_node)
        workflow.add_node("aggregate_results", self._aggregate_results_node)
        workflow.add_node("optimize_output", self._optimize_output_node)
        
        # ì—£ì§€ ì •ì˜ - Fan-outì—ì„œ Fan-in íŒ¨í„´
        workflow.set_entry_point("plan_parallel_tasks")
        
        # Fan-out: ê³„íšì—ì„œ ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ
        workflow.add_edge("plan_parallel_tasks", "execute_search_tasks")
        workflow.add_edge("plan_parallel_tasks", "execute_analysis_tasks")
        workflow.add_edge("plan_parallel_tasks", "execute_generation_tasks")
        workflow.add_edge("plan_parallel_tasks", "execute_processing_tasks")
        
        # Fan-in: ë³‘ë ¬ ì‹¤í–‰ì—ì„œ í†µí•©ìœ¼ë¡œ
        workflow.add_edge("execute_search_tasks", "aggregate_results")
        workflow.add_edge("execute_analysis_tasks", "aggregate_results")
        workflow.add_edge("execute_generation_tasks", "aggregate_results")
        workflow.add_edge("execute_processing_tasks", "aggregate_results")
        
        # ìµœì¢… ì¶œë ¥
        workflow.add_edge("aggregate_results", "optimize_output")
        workflow.add_edge("optimize_output", END)
        
        # ì¡°ê±´ë¶€ ì—£ì§€ (ì—ëŸ¬ ì²˜ë¦¬ ë° ìµœì í™”)
        workflow.add_conditional_edges(
            "plan_parallel_tasks",
            self._should_continue,
            {
                "continue": "execute_search_tasks",
                "fallback": END
            }
        )
        
        workflow.add_conditional_edges(
            "aggregate_results",
            self._should_retry_failed_tasks,
            {
                "retry": "execute_search_tasks",
                "continue": "optimize_output"
            }
        )
        
        return workflow

    async def _plan_parallel_tasks_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ë³‘ë ¬ ì‘ì—… ê³„íš ìˆ˜ë¦½ ë…¸ë“œ"""
        try:
            logger.info(f"ğŸš€ LangGraph ë³‘ë ¬ ì²˜ë¦¬: ì‘ì—… ê³„íš ìˆ˜ë¦½ ì¤‘... (query: {state['original_query'][:50]})")
            
            model = self._get_llm_model(state["model"])
            
            planning_prompt = ChatPromptTemplate.from_messages([
                ("system", """ì „ë¬¸ì ì¸ ë³‘ë ¬ ì²˜ë¦¬ ê³„íšê°€ë¡œì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë³‘ë ¬ ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

ë¶„ì„ í•­ëª©:
1. ì²˜ë¦¬ ì „ëµ (concurrent_all/batched_parallel/adaptive_load/priority_based)
2. ì‘ì—… ë¶„í•´ (ê²€ìƒ‰/ë¶„ì„/ìƒì„±/ì²˜ë¦¬ ì‘ì—…ë“¤)
3. ìš°ì„ ìˆœìœ„ ì„¤ì • (1-5, 1ì´ ìµœê³  ìš°ì„ ìˆœìœ„)
4. ì˜ì¡´ì„± ê´€ê³„ íŒŒì•…
5. ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ë° ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‘ì—…ë“¤:
- ê²€ìƒ‰ ì‘ì—…: ë‹¤ì¤‘ ê²€ìƒ‰ ì—”ì§„, í‚¤ì›Œë“œ ì¡°í•©
- ë¶„ì„ ì‘ì—…: ë‚´ìš© ë¶„ì„, ê´€ë ¨ì„± í‰ê°€, í’ˆì§ˆ ë¶„ì„
- ìƒì„± ì‘ì—…: ìš”ì•½ ìƒì„±, ë‹µë³€ ìƒì„±, ì¶”ì²œ ìƒì„±
- ì²˜ë¦¬ ì‘ì—…: ë°ì´í„° ì •ì œ, êµ¬ì¡°í™”, ìµœì í™”

JSON í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•œ ë³‘ë ¬ ì²˜ë¦¬ ê³„íšì„ ì œê³µí•˜ì„¸ìš”."""),
                ("human", """ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ì´ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ìµœì ì˜ ë³‘ë ¬ ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.""")
            ])
            
            response = await model.ainvoke(planning_prompt.format_messages(query=state["original_query"]))
            
            try:
                plan_data = json.loads(response.content)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê³„íš
                plan_data = {
                    "processing_strategy": "concurrent_all",
                    "task_breakdown": [
                        {
                            "task_id": f"search_{uuid.uuid4().hex[:8]}",
                            "task_type": "search",
                            "task_data": {"query": state["original_query"], "max_results": 10},
                            "priority": 1
                        }
                    ],
                    "dependency_graph": {},
                    "estimated_time": 5.0
                }
            
            return {
                "processing_strategy": plan_data.get("processing_strategy", "concurrent_all"),
                "task_breakdown": plan_data.get("task_breakdown", []),
                "dependency_graph": plan_data.get("dependency_graph", {}),
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "planning_completed_at": time.time(),
                    "planned_tasks_count": len(plan_data.get("task_breakdown", []))
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ë³‘ë ¬ ì‘ì—… ê³„íš ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì‘ì—… ê³„íš ì‹¤íŒ¨: {str(e)}"],
                "should_fallback": True
            }

    async def _execute_search_tasks_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ"""
        try:
            logger.info("ğŸ” LangGraph ë³‘ë ¬ ì²˜ë¦¬: ê²€ìƒ‰ ì‘ì—… ì‹¤í–‰ ì¤‘...")
            
            task_breakdown = state.get("task_breakdown", [])
            search_tasks = [task for task in task_breakdown if task.get("task_type") == "search"]
            
            if not search_tasks:
                return {"search_results": []}
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            search_results = []
            async_tasks = []
            
            for task in search_tasks[:self.max_concurrent_tasks]:  # ìµœëŒ€ ë™ì‹œ ì‘ì—… ì œí•œ
                async_tasks.append(self._execute_single_search_task(task))
            
            # ë³‘ë ¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            results = await asyncio.gather(*async_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ê²€ìƒ‰ ì‘ì—… {i} ì‹¤íŒ¨: {result}")
                    search_results.append({
                        "task_id": search_tasks[i].get("task_id", f"search_{i}"),
                        "success": False,
                        "error": str(result),
                        "execution_time": 0
                    })
                else:
                    search_results.append(result)
            
            return {
                "search_results": search_results,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "search_tasks_completed_at": time.time(),
                    "search_tasks_count": len(search_results)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ê²€ìƒ‰ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"],
                "search_results": []
            }

    async def _execute_analysis_tasks_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ë¶„ì„ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ"""
        try:
            logger.info("ğŸ§  LangGraph ë³‘ë ¬ ì²˜ë¦¬: ë¶„ì„ ì‘ì—… ì‹¤í–‰ ì¤‘...")
            
            task_breakdown = state.get("task_breakdown", [])
            analysis_tasks = [task for task in task_breakdown if task.get("task_type") == "llm_analysis"]
            
            if not analysis_tasks:
                return {"analysis_results": []}
            
            # ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
            analysis_results = []
            async_tasks = []
            
            for task in analysis_tasks[:self.max_concurrent_tasks]:
                async_tasks.append(self._execute_single_analysis_task(task, state))
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*async_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ë¶„ì„ ì‘ì—… {i} ì‹¤íŒ¨: {result}")
                    analysis_results.append({
                        "task_id": analysis_tasks[i].get("task_id", f"analysis_{i}"),
                        "success": False,
                        "error": str(result),
                        "execution_time": 0
                    })
                else:
                    analysis_results.append(result)
            
            return {
                "analysis_results": analysis_results,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "analysis_tasks_completed_at": time.time(),
                    "analysis_tasks_count": len(analysis_results)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ë¶„ì„ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"],
                "analysis_results": []
            }

    async def _execute_generation_tasks_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ìƒì„± ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ"""
        try:
            logger.info("âœ¨ LangGraph ë³‘ë ¬ ì²˜ë¦¬: ìƒì„± ì‘ì—… ì‹¤í–‰ ì¤‘...")
            
            task_breakdown = state.get("task_breakdown", [])
            generation_tasks = [task for task in task_breakdown if task.get("task_type") == "content_generation"]
            
            if not generation_tasks:
                return {"generation_results": []}
            
            # ë³‘ë ¬ ìƒì„± ì‹¤í–‰
            generation_results = []
            async_tasks = []
            
            for task in generation_tasks[:self.max_concurrent_tasks]:
                async_tasks.append(self._execute_single_generation_task(task, state))
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*async_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ìƒì„± ì‘ì—… {i} ì‹¤íŒ¨: {result}")
                    generation_results.append({
                        "task_id": generation_tasks[i].get("task_id", f"generation_{i}"),
                        "success": False,
                        "error": str(result),
                        "execution_time": 0
                    })
                else:
                    generation_results.append(result)
            
            return {
                "generation_results": generation_results,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "generation_tasks_completed_at": time.time(),
                    "generation_tasks_count": len(generation_results)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„± ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ìƒì„± ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"],
                "generation_results": []
            }

    async def _execute_processing_tasks_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ë°ì´í„° ì²˜ë¦¬ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ"""
        try:
            logger.info("âš¡ LangGraph ë³‘ë ¬ ì²˜ë¦¬: ë°ì´í„° ì²˜ë¦¬ ì‘ì—… ì‹¤í–‰ ì¤‘...")
            
            task_breakdown = state.get("task_breakdown", [])
            processing_tasks = [task for task in task_breakdown if task.get("task_type") == "data_processing"]
            
            if not processing_tasks:
                return {"processing_results": []}
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            processing_results = []
            async_tasks = []
            
            for task in processing_tasks[:self.max_concurrent_tasks]:
                async_tasks.append(self._execute_single_processing_task(task, state))
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*async_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ì²˜ë¦¬ ì‘ì—… {i} ì‹¤íŒ¨: {result}")
                    processing_results.append({
                        "task_id": processing_tasks[i].get("task_id", f"processing_{i}"),
                        "success": False,
                        "error": str(result),
                        "execution_time": 0
                    })
                else:
                    processing_results.append(result)
            
            return {
                "processing_results": processing_results,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "processing_tasks_completed_at": time.time(),
                    "processing_tasks_count": len(processing_results)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì²˜ë¦¬ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"],
                "processing_results": []
            }

    async def _aggregate_results_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ê²°ê³¼ í†µí•© ë…¸ë“œ - Fan-in íŒ¨í„´"""
        try:
            logger.info("ğŸ”„ LangGraph ë³‘ë ¬ ì²˜ë¦¬: ê²°ê³¼ í†µí•© ì¤‘...")
            
            # ëª¨ë“  ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ ìˆ˜ì§‘ (Reducerë¡œ ìë™ ì§‘ê³„ë¨)
            search_results = state.get("search_results", [])
            analysis_results = state.get("analysis_results", [])
            generation_results = state.get("generation_results", [])
            processing_results = state.get("processing_results", [])
            
            # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
            all_results = search_results + analysis_results + generation_results + processing_results
            successful_tasks = [r for r in all_results if r.get("success", False)]
            failed_tasks = [r for r in all_results if not r.get("success", False)]
            
            # ë³‘ë ¬ íš¨ìœ¨ì„± ê³„ì‚°
            total_execution_time = sum(r.get("execution_time", 0) for r in all_results)
            parallel_efficiency = self._calculate_parallel_efficiency(all_results, state)
            
            # ê²°ê³¼ í†µí•© ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
            aggregated_results = {
                "search_data": self._extract_search_data(search_results),
                "analysis_insights": self._extract_analysis_insights(analysis_results),
                "generated_content": self._extract_generated_content(generation_results),
                "processed_data": self._extract_processed_data(processing_results),
                "statistics": {
                    "total_tasks": len(all_results),
                    "successful_tasks": len(successful_tasks),
                    "failed_tasks": len(failed_tasks),
                    "success_rate": len(successful_tasks) / len(all_results) * 100 if all_results else 0
                }
            }
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            performance_metrics = {
                "parallel_efficiency": parallel_efficiency,
                "total_execution_time": total_execution_time,
                "avg_task_time": total_execution_time / len(all_results) if all_results else 0,
                "concurrency_benefit": self._calculate_concurrency_benefit(all_results),
                "resource_utilization": min(100, len(all_results) / self.max_concurrent_tasks * 100)
            }
            
            return {
                "aggregated_results": aggregated_results,
                "parallel_efficiency": parallel_efficiency,
                "performance_metrics": performance_metrics,
                "task_failures": failed_tasks,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "aggregation_completed_at": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {str(e)}"]
            }

    async def _optimize_output_node(self, state: ParallelProcessingState) -> Dict[str, Any]:
        """ì¶œë ¥ ìµœì í™” ë…¸ë“œ"""
        try:
            logger.info("ğŸ¯ LangGraph ë³‘ë ¬ ì²˜ë¦¬: ì¶œë ¥ ìµœì í™” ì¤‘...")
            
            aggregated_results = state.get("aggregated_results", {})
            performance_metrics = state.get("performance_metrics", {})
            
            model = self._get_llm_model(state["model"])
            
            # ìµœì í™”ëœ ìµœì¢… ì‘ë‹µ ìƒì„±
            optimization_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ ìµœì í™” ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ëª¨ë“  ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ ìƒì„±
2. ì¤‘ë³µëœ ì •ë³´ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
3. ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì¡°ë¡œ ì •ë¦¬
4. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì™€ ë¶ˆí™•ì‹¤í•œ ì •ë³´ êµ¬ë¶„
5. ì„±ëŠ¥ ìµœì í™” í˜œíƒ ê°„ëµ ì–¸ê¸‰

í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""),
                ("human", """ì›ë³¸ ì§ˆë¬¸: "{query}"

ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼:
- ê²€ìƒ‰ ë°ì´í„°: {search_data}
- ë¶„ì„ ì¸ì‚¬ì´íŠ¸: {analysis_insights}
- ìƒì„±ëœ ì½˜í…ì¸ : {generated_content}
- ì²˜ë¦¬ëœ ë°ì´í„°: {processed_data}

ì„±ëŠ¥ ë©”íŠ¸ë¦­:
- ë³‘ë ¬ íš¨ìœ¨ì„±: {parallel_efficiency:.1f}%
- ì„±ê³µë¥ : {success_rate:.1f}%

ì´ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì í™”ëœ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.""")
            ])
            
            search_data = aggregated_results.get("search_data", {})
            analysis_insights = aggregated_results.get("analysis_insights", {})
            generated_content = aggregated_results.get("generated_content", {})
            processed_data = aggregated_results.get("processed_data", {})
            statistics = aggregated_results.get("statistics", {})
            
            response = await model.ainvoke(optimization_prompt.format_messages(
                query=state["original_query"],
                search_data=json.dumps(search_data, ensure_ascii=False)[:500],
                analysis_insights=json.dumps(analysis_insights, ensure_ascii=False)[:500],
                generated_content=json.dumps(generated_content, ensure_ascii=False)[:500],
                processed_data=json.dumps(processed_data, ensure_ascii=False)[:500],
                parallel_efficiency=performance_metrics.get("parallel_efficiency", 0),
                success_rate=statistics.get("success_rate", 0)
            ))
            
            final_output = response.content
            
            return {
                "final_output": final_output,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "optimization_completed_at": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¶œë ¥ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "final_output": "ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆì§€ë§Œ, ìµœì¢… ê²°ê³¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "errors": [f"ì¶œë ¥ ìµœì í™” ì‹¤íŒ¨: {str(e)}"]
            }

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤

    def _should_continue(self, state: ParallelProcessingState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ê³„ì† ì§„í–‰ ì—¬ë¶€ ê²°ì •"""
        if state.get("should_fallback", False) or len(state.get("errors", [])) > 5:
            return "fallback"
        return "continue"

    def _should_retry_failed_tasks(self, state: ParallelProcessingState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ì‹¤íŒ¨í•œ ì‘ì—… ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
        task_failures = state.get("task_failures", [])
        critical_failures = [f for f in task_failures if f.get("priority", 3) <= 2]
        
        if len(critical_failures) > 0 and state.get("execution_metadata", {}).get("retry_count", 0) < 2:
            return "retry"
        return "continue"

    def _get_llm_model(self, model_name: str):
        """LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if "claude" in model_name.lower():
            return ChatAnthropic(
                model_name=model_name,
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.3
            )
        elif "gemini" in model_name.lower():
            return ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=settings.GOOGLE_API_KEY,
                temperature=0.3
            )
        else:
            return ChatAnthropic(
                model_name="claude-3-sonnet-20240229",
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.3
            )

    async def _execute_single_search_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ê²€ìƒ‰ ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        try:
            task_data = task.get("task_data", {})
            query = task_data.get("query", "")
            max_results = task_data.get("max_results", 10)
            
            # ê²€ìƒ‰ ì„œë¹„ìŠ¤ í˜¸ì¶œ
            results = await search_service.search(
                query=query,
                max_results=max_results,
                language="ko"
            )
            
            execution_time = time.time() - start_time
            
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": True,
                "result": results,
                "execution_time": execution_time,
                "metadata": {
                    "query": query,
                    "results_count": len(results)
                }
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ê²€ìƒ‰ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": False,
                "result": None,
                "execution_time": execution_time,
                "error": str(e)
            }

    async def _execute_single_analysis_task(self, task: Dict[str, Any], state: ParallelProcessingState) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¶„ì„ ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        try:
            task_data = task.get("task_data", {})
            content = task_data.get("content", "")
            analysis_type = task_data.get("analysis_type", "general")
            
            model = self._get_llm_model(state["model"])
            
            analysis_prompt = ChatPromptTemplate.from_messages([
                ("system", f"""{analysis_type} ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ì£¼ì–´ì§„ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”."""),
                ("human", "ë¶„ì„í•  ë‚´ìš©: {content}")
            ])
            
            response = await model.ainvoke(analysis_prompt.format_messages(content=content[:1000]))
            
            try:
                analysis_result = json.loads(response.content)
            except json.JSONDecodeError:
                analysis_result = {"analysis": response.content, "type": analysis_type}
            
            execution_time = time.time() - start_time
            
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": True,
                "result": analysis_result,
                "execution_time": execution_time,
                "metadata": {"analysis_type": analysis_type}
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ë¶„ì„ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": False,
                "result": None,
                "execution_time": execution_time,
                "error": str(e)
            }

    async def _execute_single_generation_task(self, task: Dict[str, Any], state: ParallelProcessingState) -> Dict[str, Any]:
        """ë‹¨ì¼ ìƒì„± ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        try:
            task_data = task.get("task_data", {})
            prompt = task_data.get("prompt", "")
            generation_type = task_data.get("generation_type", "general")
            
            model = self._get_llm_model(state["model"])
            
            generation_prompt = ChatPromptTemplate.from_messages([
                ("system", f"""{generation_type} ì½˜í…ì¸  ìƒì„± ì „ë¬¸ê°€ë¡œì„œ ìš”ì²­ëœ ë‚´ìš©ì„ ìƒì„±í•˜ì„¸ìš”."""),
                ("human", "{prompt}")
            ])
            
            response = await model.ainvoke(generation_prompt.format_messages(prompt=prompt))
            
            execution_time = time.time() - start_time
            
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": True,
                "result": {"content": response.content, "type": generation_type},
                "execution_time": execution_time,
                "metadata": {"generation_type": generation_type}
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ìƒì„± ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": False,
                "result": None,
                "execution_time": execution_time,
                "error": str(e)
            }

    async def _execute_single_processing_task(self, task: Dict[str, Any], state: ParallelProcessingState) -> Dict[str, Any]:
        """ë‹¨ì¼ ì²˜ë¦¬ ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        try:
            task_data = task.get("task_data", {})
            data = task_data.get("data", [])
            processing_type = task_data.get("processing_type", "filter")
            
            # ì²˜ë¦¬ ìœ í˜•ë³„ ë¡œì§
            if processing_type == "filter":
                processed_data = [item for item in data if self._should_include_item(item)]
            elif processing_type == "sort":
                processed_data = sorted(data, key=lambda x: x.get("score", 0), reverse=True)
            elif processing_type == "deduplicate":
                processed_data = self._deduplicate_items(data)
            else:
                processed_data = data
            
            execution_time = time.time() - start_time
            
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": True,
                "result": processed_data,
                "execution_time": execution_time,
                "metadata": {"processing_type": processing_type, "processed_count": len(processed_data)}
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ì²˜ë¦¬ ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "task_id": task.get("task_id", "unknown"),
                "success": False,
                "result": None,
                "execution_time": execution_time,
                "error": str(e)
            }

    def _calculate_parallel_efficiency(self, results: List[Dict[str, Any]], state: ParallelProcessingState) -> float:
        """ë³‘ë ¬ íš¨ìœ¨ì„± ê³„ì‚°"""
        try:
            if not results:
                return 0.0
            
            # ì„±ê³µí•œ ì‘ì—…ë“¤ì˜ ì‹¤í–‰ ì‹œê°„
            successful_results = [r for r in results if r.get("success", False)]
            if not successful_results:
                return 0.0
            
            execution_times = [r.get("execution_time", 0) for r in successful_results]
            
            # ìˆœì°¨ ì‹¤í–‰ ì˜ˆìƒ ì‹œê°„ vs ì‹¤ì œ ë³‘ë ¬ ì‹¤í–‰ ì‹œê°„
            sequential_time = sum(execution_times)
            parallel_time = max(execution_times) if execution_times else 0
            
            if parallel_time <= 0:
                return 0.0
            
            # íš¨ìœ¨ì„± = (ìˆœì°¨ì‹œê°„ - ë³‘ë ¬ì‹œê°„) / ìˆœì°¨ì‹œê°„ * 100
            efficiency = (sequential_time - parallel_time) / sequential_time * 100
            return min(100.0, max(0.0, efficiency))
            
        except Exception:
            return 0.0

    def _calculate_concurrency_benefit(self, results: List[Dict[str, Any]]) -> float:
        """ë™ì‹œì„± í˜œíƒ ê³„ì‚°"""
        try:
            if len(results) <= 1:
                return 0.0
            
            execution_times = [r.get("execution_time", 0) for r in results if r.get("success", False)]
            if not execution_times:
                return 0.0
            
            # í‰ê·  ì‘ì—… ì‹œê°„ vs ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„
            avg_task_time = sum(execution_times) / len(execution_times)
            max_parallel_time = max(execution_times)
            
            if max_parallel_time <= 0:
                return 0.0
            
            # ë™ì‹œì„± í˜œíƒ = ì‘ì—… ìˆ˜ * í‰ê·  ì‹œê°„ / ìµœëŒ€ ë³‘ë ¬ ì‹œê°„
            benefit = len(execution_times) * avg_task_time / max_parallel_time
            return min(10.0, max(1.0, benefit))  # 1ë°°~10ë°° ì‚¬ì´ë¡œ ì œí•œ
            
        except Exception:
            return 1.0

    def _extract_search_data(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° ì¶”ì¶œ"""
        successful_searches = [r for r in search_results if r.get("success", False)]
        
        all_results = []
        for search in successful_searches:
            results = search.get("result", [])
            if isinstance(results, list):
                all_results.extend(results)
        
        return {
            "total_results": len(all_results),
            "search_count": len(successful_searches),
            "top_results": all_results[:10]  # ìƒìœ„ 10ê°œë§Œ
        }

    def _extract_analysis_insights(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        successful_analyses = [r for r in analysis_results if r.get("success", False)]
        
        insights = []
        for analysis in successful_analyses:
            result = analysis.get("result", {})
            if isinstance(result, dict):
                insights.append(result)
        
        return {
            "insights_count": len(insights),
            "analysis_types": list(set(i.get("type", "unknown") for i in insights)),
            "insights": insights
        }

    def _extract_generated_content(self, generation_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìƒì„±ëœ ì½˜í…ì¸  ì¶”ì¶œ"""
        successful_generations = [r for r in generation_results if r.get("success", False)]
        
        contents = []
        for generation in successful_generations:
            result = generation.get("result", {})
            if isinstance(result, dict):
                contents.append(result)
        
        return {
            "content_count": len(contents),
            "generation_types": list(set(c.get("type", "unknown") for c in contents)),
            "contents": contents
        }

    def _extract_processed_data(self, processing_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì²˜ë¦¬ëœ ë°ì´í„° ì¶”ì¶œ"""
        successful_processing = [r for r in processing_results if r.get("success", False)]
        
        processed_items = []
        for processing in successful_processing:
            result = processing.get("result", [])
            if isinstance(result, list):
                processed_items.extend(result)
        
        return {
            "processed_count": len(processed_items),
            "processing_types": [p.get("metadata", {}).get("processing_type", "unknown") for p in successful_processing],
            "processed_items": processed_items[:20]  # ìƒìœ„ 20ê°œë§Œ
        }

    def _should_include_item(self, item: Any) -> bool:
        """ì•„ì´í…œ í¬í•¨ ì—¬ë¶€ ê²°ì •"""
        if isinstance(item, dict):
            return item.get("quality_score", 0) > 0.5
        return True

    def _deduplicate_items(self, items: List[Any]) -> List[Any]:
        """ì¤‘ë³µ ì•„ì´í…œ ì œê±°"""
        seen = set()
        unique_items = []
        
        for item in items:
            item_key = str(item) if not isinstance(item, dict) else item.get("id", str(item))
            if item_key not in seen:
                seen.add(item_key)
                unique_items.append(item)
        
        return unique_items

    async def execute_parallel_processing(
        self, 
        input_data: AgentInput, 
        model: str = "claude-sonnet", 
        progress_callback=None
    ) -> AgentOutput:
        """
        ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ LangGraph ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹œì‘ (ì‚¬ìš©ì: {input_data.user_id})")
        
        try:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            await langgraph_monitor.start_execution("langgraph_parallel_processor")
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì • (Reducer ê¸°ë³¸ê°’ ì„¤ì •)
            initial_state = ParallelProcessingState(
                original_query=input_data.query,
                user_id=input_data.user_id,
                session_id=input_data.session_id,
                model=model,
                processing_strategy=None,
                task_breakdown=None,
                dependency_graph=None,
                search_results=[],
                analysis_results=[],
                generation_results=[],
                processing_results=[],
                aggregated_results=None,
                final_output=None,
                execution_metadata={"start_time": start_time},
                performance_metrics={},
                parallel_efficiency=0.0,
                errors=[],
                task_failures=[],
                should_fallback=False
            )
            
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            if self.checkpointer:
                app = self.workflow.compile(checkpointer=self.checkpointer)
                config = {"configurable": {"thread_id": f"parallel_{input_data.user_id}_{input_data.session_id}"}}
                final_state = await app.ainvoke(initial_state, config=config)
            else:
                app = self.workflow.compile()
                final_state = await app.ainvoke(initial_state)
            
            # ê²°ê³¼ ì²˜ë¦¬
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # ì—ëŸ¬ê°€ ìˆê±°ë‚˜ fallbackì´ í•„ìš”í•œ ê²½ìš°
            if final_state.get("should_fallback", False) or len(final_state.get("errors", [])) > 0:
                logger.warning("ğŸ”„ LangGraph ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨")
                return AgentOutput(
                    result="ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœ ì²˜ë¦¬ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
                    metadata={
                        "agent_version": "langgraph_parallel",
                        "processing_failed": True,
                        "errors": final_state.get("errors", [])
                    },
                    execution_time_ms=execution_time_ms,
                    agent_id=self.agent_id,
                    model_used=model,
                    timestamp=datetime.utcnow().isoformat()
                )
            
            # ì„±ê³µì ì¸ ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ ë°˜í™˜
            final_output = final_state.get("final_output", "ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ê²°ê³¼ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            performance_metrics = final_state.get("performance_metrics", {})
            aggregated_results = final_state.get("aggregated_results", {})
            
            await langgraph_monitor.track_execution(
                agent_type="langgraph_parallel_processor",
                execution_time=execution_time_ms / 1000,
                success=True,
                user_id=input_data.user_id
            )
            
            result = AgentOutput(
                result=final_output,
                metadata={
                    "agent_version": "langgraph_parallel_v2",
                    "parallel_processing": True,
                    "parallel_efficiency": final_state.get("parallel_efficiency", 0),
                    "statistics": aggregated_results.get("statistics", {}),
                    **performance_metrics,
                    **final_state.get("execution_metadata", {})
                },
                execution_time_ms=execution_time_ms,
                agent_id=self.agent_id,
                model_used=model,
                timestamp=datetime.utcnow().isoformat()
            )
            
            efficiency = final_state.get("parallel_efficiency", 0)
            logger.info(f"âœ… LangGraph ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ({execution_time_ms}ms, íš¨ìœ¨ì„±: {efficiency:.1f}%)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ LangGraph ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            await langgraph_monitor.track_execution(
                agent_type="langgraph_parallel_processor",
                execution_time=(time.time() - start_time),
                success=False,
                error_message=str(e),
                user_id=input_data.user_id
            )
            
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            return AgentOutput(
                result="ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œì—ì„œ ì¼ì‹œì  ì²˜ë¦¬ ì§€ì—°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                metadata={
                    "agent_version": "langgraph_parallel_v2",
                    "error_occurred": True,
                    "error_handled": True
                },
                execution_time_ms=execution_time_ms,
                agent_id=self.agent_id,
                model_used=model,
                timestamp=datetime.utcnow().isoformat(),
                error=f"LangGraph ë³‘ë ¬ ì²˜ë¦¬ error: {str(e)}"
            )

    def get_capabilities(self) -> List[str]:
        """ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "Fan-out/Fan-in ë³‘ë ¬ ì²˜ë¦¬",
            "Reducer ê¸°ë°˜ ìƒíƒœ ì§‘ê³„",
            "ì ì‘í˜• ë¶€í•˜ ë¶„ì‚°",
            "ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™”",
            "ì˜ì¡´ì„± ê¸°ë°˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§",
            "ë™ì‹œì„± í˜œíƒ ì¸¡ì •",
            "ìë™ ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„"
        ]

    def get_supported_models(self) -> List[str]:
        """ì§€ì› ëª¨ë¸ ëª©ë¡"""
        return [
            "claude-sonnet",
            "claude-haiku", 
            "claude-opus",
            "gemini-pro",
            "gemini-flash"
        ]


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
langgraph_parallel_processor = LangGraphParallelProcessor()