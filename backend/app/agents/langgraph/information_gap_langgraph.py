"""
LangGraph ê¸°ë°˜ Information Gap Analyzer - ì§€ëŠ¥í˜• ì •ë³´ ë¶„ì„ ì‹œìŠ¤í…œ

Context7 ìµœì‹  ë¬¸ì„œë¥¼ ì°¸ì¡°í•œ ê³ ê¸‰ LangGraph StateGraph êµ¬í˜„ìœ¼ë¡œ,
ì •ë³´ ë¶€ì¡± ìƒí™©ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìµœì ì˜ í•´ê²°ì±…ì„ ì œì•ˆí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import time
import asyncio
import json
from typing import Dict, Any, List, Optional, TypedDict, Union
from datetime import datetime
import logging

# LangGraph í•µì‹¬ imports (ìµœì‹  ë²„ì „)
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# ê¸°ì¡´ ì‹œìŠ¤í…œ imports
from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.agents.workers.information_gap_analyzer import information_gap_analyzer
from app.core.config import settings
from app.core.feature_flags import is_langgraph_enabled, LangGraphFeatureFlags
from app.services.langgraph_monitor import langgraph_monitor

logger = logging.getLogger(__name__)


class InformationGapState(TypedDict):
    """LangGraph Information Gap ë¶„ì„ ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    original_query: str
    user_id: str
    session_id: Optional[str]
    model: str
    conversation_context: Optional[Dict[str, Any]]
    
    # 1ë‹¨ê³„: ì¿¼ë¦¬ ì´í•´
    query_understanding: Optional[Dict[str, Any]]
    intent_classification: Optional[Dict[str, Any]]
    complexity_assessment: Optional[Dict[str, Any]]
    
    # 2ë‹¨ê³„: ë„ë©”ì¸ ë¶„ë¥˜
    domain_analysis: Optional[Dict[str, Any]]
    expertise_requirements: Optional[Dict[str, Any]]
    
    # 3ë‹¨ê³„: ì •ë³´ ê²©ì°¨ ë¶„ì„
    information_gaps: Optional[List[Dict[str, Any]]]
    missing_context: Optional[List[str]]
    ambiguity_points: Optional[List[Dict[str, Any]]]
    
    # 4ë‹¨ê³„: í•´ê²° ì „ëµ ìˆ˜ë¦½
    resolution_strategy: Optional[Dict[str, Any]]
    clarification_questions: Optional[List[str]]
    fallback_approaches: Optional[List[Dict[str, Any]]]
    
    # 5ë‹¨ê³„: ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ ê³„íš
    information_gathering_plan: Optional[Dict[str, Any]]
    research_directions: Optional[List[str]]
    
    # 6ë‹¨ê³„: ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€
    answerability_assessment: Optional[Dict[str, Any]]
    confidence_score: Optional[float]
    
    # 7ë‹¨ê³„: ì‚¬ìš©ì ì•ˆë‚´ ìƒì„±
    user_guidance: Optional[Dict[str, Any]]
    recommended_actions: Optional[List[str]]
    
    # 8ë‹¨ê³„: ìµœì¢… ì‘ë‹µ êµ¬ì„±
    final_response: Optional[str]
    metadata_enrichment: Optional[Dict[str, Any]]
    
    # ë©”íƒ€ë°ì´í„°
    execution_metadata: Dict[str, Any]
    performance_metrics: Dict[str, Any]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: List[str]
    error_recovery_attempts: int
    should_fallback: bool


def create_error_safe_node(agent_name: str, node_name: str, node_func):
    """ì—ëŸ¬ ì•ˆì „ ë…¸ë“œ ë˜í¼ - ëª¨ë“  ë…¸ë“œë¥¼ ì—ëŸ¬ ì•ˆì „í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤"""
    async def error_safe_wrapper(state):
        try:
            logger.debug(f"ğŸ” {agent_name}: {node_name} ë…¸ë“œ ì‹¤í–‰ ì¤‘...")
            result = await node_func(state)
            logger.debug(f"âœ… {agent_name}: {node_name} ë…¸ë“œ ì™„ë£Œ")
            return result
        except Exception as e:
            logger.error(f"âŒ {agent_name}: {node_name} ë…¸ë“œ ì—ëŸ¬: {e}")
            
            # ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            error_context = {
                "node_name": node_name,
                "error_type": type(e).__name__,
                "error_message": str(e),
                "timestamp": datetime.now().isoformat(),
                "state_snapshot": {
                    "errors_count": len(state.get("errors", [])),
                    "recovery_attempts": state.get("error_recovery_attempts", 0)
                }
            }
            
            # ì—ëŸ¬ ëˆ„ì  ë° ë³µêµ¬ ì‹œë„ ì¦ê°€
            current_errors = state.get("errors", [])
            current_errors.append(f"{node_name}: {str(e)}")
            
            recovery_attempts = state.get("error_recovery_attempts", 0) + 1
            
            # ë³µêµ¬ ê°€ëŠ¥ì„± í‰ê°€
            should_fallback = False
            if recovery_attempts >= 3:  # 3íšŒ ì´ìƒ ì‹¤íŒ¨ ì‹œ fallback
                should_fallback = True
                logger.warning(f"ğŸš¨ {agent_name}: ë³µêµ¬ ì‹œë„ í•œê³„ ë„ë‹¬ - Legacy fallback ì¤€ë¹„")
            elif len(current_errors) >= 5:  # ì—ëŸ¬ 5ê°œ ì´ìƒ ëˆ„ì  ì‹œ fallback
                should_fallback = True
                logger.warning(f"ğŸš¨ {agent_name}: ì—ëŸ¬ ëˆ„ì  í•œê³„ ë„ë‹¬ - Legacy fallback ì¤€ë¹„")
            
            return {
                "errors": current_errors,
                "error_recovery_attempts": recovery_attempts,
                "should_fallback": should_fallback,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    f"{node_name}_error": error_context
                }
            }
    
    return error_safe_wrapper


class LangGraphInformationGapAnalyzer(BaseAgent):
    """LangGraph ê¸°ë°˜ Information Gap ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_id="langgraph_information_gap",
            name="LangGraph Information Gap Analyzer",
            description="ì§€ëŠ¥í˜• ì •ë³´ ê²©ì°¨ ë¶„ì„ ë° í•´ê²° ì „ëµ ìˆ˜ë¦½ ì‹œìŠ¤í…œ"
        )
        
        # Legacy ì—ì´ì „íŠ¸ (fallbackìš©)
        self.legacy_agent = information_gap_analyzer
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()
        
        # PostgreSQL ì²´í¬í¬ì¸í„° ì„¤ì •
        if settings.DATABASE_URL:
            self.checkpointer = PostgresSaver.from_conn_string(
                settings.DATABASE_URL,
                
            )
        else:
            self.checkpointer = None
            logger.warning("DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì²´í¬í¬ì¸í„° ë¹„í™œì„±í™”")

    def _build_workflow(self) -> StateGraph:
        """LangGraph Information Gap ë¶„ì„ ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        
        # StateGraph ìƒì„±
        workflow = StateGraph(InformationGapState)
        
        # ë…¸ë“œ ì •ì˜ - 8ë‹¨ê³„ ê³ ê¸‰ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ëª¨ë‘ ì—ëŸ¬ ì•ˆì „)
        workflow.add_node("understand_query", 
                         create_error_safe_node("langgraph_information_gap", "understand_query", self._understand_query_node))
        workflow.add_node("classify_domain", 
                         create_error_safe_node("langgraph_information_gap", "classify_domain", self._classify_domain_node))
        workflow.add_node("analyze_gaps", 
                         create_error_safe_node("langgraph_information_gap", "analyze_gaps", self._analyze_gaps_node))
        workflow.add_node("develop_strategy", 
                         create_error_safe_node("langgraph_information_gap", "develop_strategy", self._develop_strategy_node))
        workflow.add_node("plan_information_gathering", 
                         create_error_safe_node("langgraph_information_gap", "plan_information_gathering", self._plan_information_gathering_node))
        workflow.add_node("assess_answerability", 
                         create_error_safe_node("langgraph_information_gap", "assess_answerability", self._assess_answerability_node))
        workflow.add_node("generate_guidance", 
                         create_error_safe_node("langgraph_information_gap", "generate_guidance", self._generate_guidance_node))
        workflow.add_node("construct_response", 
                         create_error_safe_node("langgraph_information_gap", "construct_response", self._construct_response_node))
        
        # ì—£ì§€ ì •ì˜ - ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        workflow.set_entry_point("understand_query")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ - ê° ë‹¨ê³„ì—ì„œ ì—ëŸ¬ ì²´í¬
        workflow.add_conditional_edges(
            "understand_query",
            self._should_continue_or_abort,
            {
                "continue": "classify_domain",
                "abort": END,
                "fallback": END
            }
        )
        
        workflow.add_conditional_edges(
            "classify_domain",
            self._should_continue_or_abort,
            {
                "continue": "analyze_gaps",
                "abort": END,
                "fallback": END
            }
        )
        
        # ì„ í˜• ì§„í–‰ (ì—ëŸ¬ ë°œìƒ ì‹œ ê° ë…¸ë“œì—ì„œ ìì²´ ì²˜ë¦¬)
        workflow.add_edge("analyze_gaps", "develop_strategy")
        workflow.add_edge("develop_strategy", "plan_information_gathering")
        workflow.add_edge("plan_information_gathering", "assess_answerability")
        workflow.add_edge("assess_answerability", "generate_guidance")
        workflow.add_edge("generate_guidance", "construct_response")
        workflow.add_edge("construct_response", END)
        
        return workflow

    def _should_continue_or_abort(self, state: InformationGapState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜ - ì—ëŸ¬ ìƒí™©ì— ë”°ë¥¸ íë¦„ ì œì–´"""
        if state.get("should_fallback", False):
            return "fallback"
        
        errors_count = len(state.get("errors", []))
        recovery_attempts = state.get("error_recovery_attempts", 0)
        
        if errors_count >= 3 or recovery_attempts >= 2:
            logger.warning(f"âš ï¸ Information Gap: ì¡°ê±´ë¶€ ì¤‘ë‹¨ (ì—ëŸ¬: {errors_count}, ë³µêµ¬ì‹œë„: {recovery_attempts})")
            return "abort"
        
        return "continue"

    async def _understand_query_node(self, state: InformationGapState) -> Dict[str, Any]:
        """1ë‹¨ê³„: ì¿¼ë¦¬ ì´í•´ ë…¸ë“œ"""
        logger.info(f"ğŸ” Information Gap: ì¿¼ë¦¬ ì´í•´ ë¶„ì„ ì¤‘... (query: {state['original_query'][:50]})")
        
        model = self._get_llm_model(state["model"])
        
        understanding_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ ì–¸ì–´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ íŒŒì•…í•˜ì„¸ìš”:

1. í•µì‹¬ ì˜ë„ (ì •ë³´ ìˆ˜ì§‘, ë¬¸ì œ í•´ê²°, ë¹„êµ ë¶„ì„, ì°½ì‘ ì§€ì› ë“±)
2. ì§ˆë¬¸ ë³µì¡ë„ (ë‹¨ìˆœ, ë³´í†µ, ë³µí•©, ê³ ë„ ì „ë¬¸)
3. ëª…í™•ì„± ìˆ˜ì¤€ (ë§¤ìš° ëª…í™•, ëª…í™•, ë³´í†µ, ëª¨í˜¸, ë§¤ìš° ëª¨í˜¸)
4. í•„ìš” ì •ë³´ ìœ í˜• (ì‚¬ì‹¤, ë¶„ì„, ì˜ê²¬, ì ˆì°¨, ì°½ì˜ì  ë‚´ìš©)
5. ì‹œê¸‰ì„± ë° ì¤‘ìš”ë„

JSON í˜•ì‹ìœ¼ë¡œ ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”."""),
            ("human", """ë¶„ì„í•  ì§ˆë¬¸: "{query}"

ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì  ì´í•´ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.""")
        ])
        
        response = await model.ainvoke(understanding_prompt.format_messages(query=state["original_query"]))
        
        try:
            understanding_result = json.loads(response.content)
        except json.JSONDecodeError:
            understanding_result = {
                "core_intent": "ì •ë³´_ìˆ˜ì§‘",
                "complexity": "ë³´í†µ",
                "clarity_level": "ë³´í†µ",
                "required_info_type": ["ì‚¬ì‹¤", "ë¶„ì„"],
                "urgency": "ë³´í†µ",
                "importance": "ë³´í†µ"
            }
        
        # ì˜ë„ ë¶„ë¥˜
        intent_classification = {
            "primary_intent": understanding_result.get("core_intent", "ì •ë³´_ìˆ˜ì§‘"),
            "secondary_intents": understanding_result.get("secondary_intents", []),
            "confidence": understanding_result.get("intent_confidence", 0.8)
        }
        
        # ë³µì¡ë„ í‰ê°€
        complexity_assessment = {
            "level": understanding_result.get("complexity", "ë³´í†µ"),
            "factors": understanding_result.get("complexity_factors", []),
            "estimated_effort": understanding_result.get("estimated_effort", "ì¤‘ê°„")
        }
        
        return {
            "query_understanding": understanding_result,
            "intent_classification": intent_classification,
            "complexity_assessment": complexity_assessment,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "query_understood_at": time.time()
            }
        }

    async def _classify_domain_node(self, state: InformationGapState) -> Dict[str, Any]:
        """2ë‹¨ê³„: ë„ë©”ì¸ ë¶„ë¥˜ ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: ë„ë©”ì¸ ë¶„ë¥˜ ì¤‘...")
        
        model = self._get_llm_model(state["model"])
        
        domain_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì „ë¬¸ ë„ë©”ì¸ ë¶„ë¥˜ ì „ë¬¸ê°€ë¡œì„œ ì§ˆë¬¸ì´ ì†í•œ ì˜ì—­ê³¼ í•„ìš”í•œ ì „ë¬¸ì„±ì„ ë¶„ì„í•˜ì„¸ìš”:

1. ì£¼ìš” ë„ë©”ì¸ (ê¸°ìˆ , ë¹„ì¦ˆë‹ˆìŠ¤, ì°½ì‘, í•™ìˆ , ì¼ìƒ, ì „ë¬¸ì§ì—… ë“±)
2. ì„¸ë¶€ ì „ë¬¸ ë¶„ì•¼
3. í•„ìš”í•œ ì „ë¬¸ì„± ìˆ˜ì¤€ (ì¼ë°˜ì¸, ì´ˆê¸‰ì „ë¬¸ê°€, ì¤‘ê¸‰ì „ë¬¸ê°€, ê³ ê¸‰ì „ë¬¸ê°€)
4. ê´€ë ¨ ì§€ì‹ ì˜ì—­ë“¤
5. ë„ë©”ì¸ê°„ ìœµí•© í•„ìš”ì„±

JSON í˜•ì‹ìœ¼ë¡œ ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”."""),
            ("human", """ì§ˆë¬¸: "{query}"
ì§ˆë¬¸ ì´í•´ ê²°ê³¼: {understanding}

ë„ë©”ì¸ ë¶„ë¥˜ ë° ì „ë¬¸ì„± ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.""")
        ])
        
        response = await model.ainvoke(domain_prompt.format_messages(
            query=state["original_query"],
            understanding=json.dumps(state.get("query_understanding", {}), ensure_ascii=False)
        ))
        
        try:
            domain_analysis = json.loads(response.content)
        except json.JSONDecodeError:
            domain_analysis = {
                "primary_domain": "ì¼ë°˜",
                "sub_domains": [],
                "expertise_level": "ì¼ë°˜ì¸",
                "knowledge_areas": [],
                "interdisciplinary": False
            }
        
        # ì „ë¬¸ì„± ìš”êµ¬ì‚¬í•­
        expertise_requirements = {
            "level": domain_analysis.get("expertise_level", "ì¼ë°˜ì¸"),
            "specific_skills": domain_analysis.get("specific_skills", []),
            "knowledge_depth": domain_analysis.get("knowledge_depth", "ê¸°ë³¸"),
            "tools_needed": domain_analysis.get("tools_needed", [])
        }
        
        return {
            "domain_analysis": domain_analysis,
            "expertise_requirements": expertise_requirements,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "domain_classified_at": time.time()
            }
        }

    async def _analyze_gaps_node(self, state: InformationGapState) -> Dict[str, Any]:
        """3ë‹¨ê³„: ì •ë³´ ê²©ì°¨ ë¶„ì„ ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: ì •ë³´ ê²©ì°¨ ë¶„ì„ ì¤‘...")
        
        model = self._get_llm_model(state["model"])
        
        gap_analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì •ë³´ ê²©ì°¨ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ëˆ„ë½ëœ í•µì‹¬ ì •ë³´ ì‹ë³„
2. ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ìš”ì†Œë“¤
3. ê°€ì •ì´ í•„ìš”í•œ ë¶€ë¶„ë“¤
4. ì¶”ê°€ ë§¥ë½ì´ í•„ìš”í•œ ì˜ì—­
5. ì§ˆë¬¸ ë²”ìœ„ì˜ ëª…í™•ì„± í‰ê°€

ê° ì •ë³´ ê²©ì°¨ì— ëŒ€í•´ ì¤‘ìš”ë„ì™€ í•´ê²° ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ì„¸ìš”."""),
            ("human", """ì§ˆë¬¸: "{query}"
ë„ë©”ì¸ ë¶„ì„: {domain}
ì „ë¬¸ì„± ìš”êµ¬ì‚¬í•­: {expertise}

ì •ë³´ ê²©ì°¨ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.""")
        ])
        
        response = await model.ainvoke(gap_analysis_prompt.format_messages(
            query=state["original_query"],
            domain=json.dumps(state.get("domain_analysis", {}), ensure_ascii=False),
            expertise=json.dumps(state.get("expertise_requirements", {}), ensure_ascii=False)
        ))
        
        try:
            gap_result = json.loads(response.content)
            information_gaps = gap_result.get("information_gaps", [])
            missing_context = gap_result.get("missing_context", [])
            ambiguity_points = gap_result.get("ambiguity_points", [])
        except json.JSONDecodeError:
            information_gaps = [{"type": "ì¼ë°˜", "importance": "ì¤‘ê°„", "description": "ì¶”ê°€ ë§¥ë½ í•„ìš”"}]
            missing_context = ["êµ¬ì²´ì  ìƒí™© ì •ë³´"]
            ambiguity_points = [{"point": "ì§ˆë¬¸ ë²”ìœ„", "clarification_needed": True}]
        
        return {
            "information_gaps": information_gaps,
            "missing_context": missing_context,
            "ambiguity_points": ambiguity_points,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "gaps_analyzed_at": time.time()
            }
        }

    async def _develop_strategy_node(self, state: InformationGapState) -> Dict[str, Any]:
        """4ë‹¨ê³„: í•´ê²° ì „ëµ ìˆ˜ë¦½ ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: í•´ê²° ì „ëµ ìˆ˜ë¦½ ì¤‘...")
        
        model = self._get_llm_model(state["model"])
        
        strategy_prompt = ChatPromptTemplate.from_messages([
            ("system", """í•´ê²° ì „ëµ ìˆ˜ë¦½ ì „ë¬¸ê°€ë¡œì„œ ì •ë³´ ê²©ì°¨ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”:

1. ìš°ì„ ìˆœìœ„ë³„ í•´ê²° ì „ëµ
2. ì‚¬ìš©ì í™•ì¸ì´ í•„ìš”í•œ ëª…ë£Œí™” ì§ˆë¬¸ë“¤
3. ì¶”ì •/ê°€ì • ê¸°ë°˜ ì ‘ê·¼ë²•
4. ëŒ€ì•ˆì  ë‹µë³€ ë°©í–¥
5. ë¶€ë¶„ì  ë‹µë³€ ê°€ëŠ¥ì„±

ê° ì „ëµì˜ íš¨ê³¼ì„±ê³¼ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ì„¸ìš”."""),
            ("human", """ì§ˆë¬¸: "{query}"
ì •ë³´ ê²©ì°¨: {gaps}
ëª¨í˜¸í•œ ì ë“¤: {ambiguities}

ìµœì ì˜ í•´ê²° ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.""")
        ])
        
        response = await model.ainvoke(strategy_prompt.format_messages(
            query=state["original_query"],
            gaps=json.dumps(state.get("information_gaps", []), ensure_ascii=False),
            ambiguities=json.dumps(state.get("ambiguity_points", []), ensure_ascii=False)
        ))
        
        try:
            strategy_result = json.loads(response.content)
        except json.JSONDecodeError:
            strategy_result = {
                "primary_strategy": "ë¶€ë¶„_ë‹µë³€_ë°_ëª…ë£Œí™”",
                "clarification_questions": ["ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"],
                "fallback_approaches": [{"type": "ì¼ë°˜ì _ë‹µë³€", "feasibility": "ë†’ìŒ"}]
            }
        
        return {
            "resolution_strategy": strategy_result,
            "clarification_questions": strategy_result.get("clarification_questions", []),
            "fallback_approaches": strategy_result.get("fallback_approaches", []),
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "strategy_developed_at": time.time()
            }
        }

    async def _plan_information_gathering_node(self, state: InformationGapState) -> Dict[str, Any]:
        """5ë‹¨ê³„: ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ ê³„íš ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: ì •ë³´ ìˆ˜ì§‘ ê³„íš ì¤‘...")
        
        # ì •ë³´ ìˆ˜ì§‘ ê³„íš ìƒì„±
        gathering_plan = {
            "immediate_actions": [
                "ì‚¬ìš©ì ëª…ë£Œí™” ìš”ì²­",
                "ê¸°ì¡´ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ ì¤€ë¹„"
            ],
            "research_directions": [
                "ê´€ë ¨ ì£¼ì œ íƒìƒ‰",
                "ìœ ì‚¬ ì‚¬ë¡€ ë¶„ì„"
            ],
            "information_sources": [
                "ì›¹ ê²€ìƒ‰",
                "ì „ë¬¸ ì§€ì‹ ë² ì´ìŠ¤"
            ],
            "priority": "ë†’ìŒ"
        }
        
        # ì—°êµ¬ ë°©í–¥
        research_directions = [
            f"'{state['original_query'][:30]}...' ê´€ë ¨ ì„¸ë¶€ ì •ë³´",
            "ìœ ì‚¬í•œ ìƒí™©ì˜ ëª¨ë²” ì‚¬ë¡€",
            "ì „ë¬¸ê°€ ê¶Œì¥ ì‚¬í•­"
        ]
        
        return {
            "information_gathering_plan": gathering_plan,
            "research_directions": research_directions,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "gathering_planned_at": time.time()
            }
        }

    async def _assess_answerability_node(self, state: InformationGapState) -> Dict[str, Any]:
        """6ë‹¨ê³„: ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€ ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€ ì¤‘...")
        
        # ë‹µë³€ ê°€ëŠ¥ì„± ì ìˆ˜ ê³„ì‚°
        gaps_count = len(state.get("information_gaps", []))
        ambiguity_count = len(state.get("ambiguity_points", []))
        
        # ê¸°ë³¸ ì ìˆ˜ì—ì„œ ê²©ì°¨ì™€ ëª¨í˜¸í•¨ì— ë”°ë¼ ì°¨ê°
        base_score = 0.8
        gap_penalty = min(gaps_count * 0.1, 0.3)
        ambiguity_penalty = min(ambiguity_count * 0.05, 0.2)
        
        confidence_score = max(0.1, base_score - gap_penalty - ambiguity_penalty)
        
        # ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€
        answerability_assessment = {
            "confidence_score": confidence_score,
            "answerability_level": "ë†’ìŒ" if confidence_score > 0.7 else "ë³´í†µ" if confidence_score > 0.4 else "ë‚®ìŒ",
            "limitations": [
                f"ì •ë³´ ê²©ì°¨ {gaps_count}ê°œ",
                f"ëª¨í˜¸í•œ ì  {ambiguity_count}ê°œ"
            ],
            "recommendation": "ë¶€ë¶„_ë‹µë³€_ë°_ëª…ë£Œí™”" if confidence_score < 0.6 else "ì§ì ‘_ë‹µë³€"
        }
        
        return {
            "answerability_assessment": answerability_assessment,
            "confidence_score": confidence_score,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "answerability_assessed_at": time.time()
            }
        }

    async def _generate_guidance_node(self, state: InformationGapState) -> Dict[str, Any]:
        """7ë‹¨ê³„: ì‚¬ìš©ì ì•ˆë‚´ ìƒì„± ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: ì‚¬ìš©ì ì•ˆë‚´ ìƒì„± ì¤‘...")
        
        model = self._get_llm_model(state["model"])
        
        confidence_score = state.get("confidence_score", 0.5)
        clarification_questions = state.get("clarification_questions", [])
        
        guidance_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì‚¬ìš©ì ì•ˆë‚´ ì „ë¬¸ê°€ë¡œì„œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì•ˆë‚´ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

1. í˜„ì¬ ìƒí™© ì„¤ëª… (ì •ë³´ ë¶€ì¡± ë“±)
2. ê°€ëŠ¥í•œ ë„ì›€ ë²”ìœ„ ì•ˆë‚´
3. ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ êµ¬ì²´ì  ìš”ì²­
4. ëŒ€ì•ˆì  ì ‘ê·¼ ë°©ë²• ì œì‹œ
5. ê²©ë ¤ì™€ ì§€ì§€ ë©”ì‹œì§€

ì‚¬ìš©ìê°€ ì‹¤ë§í•˜ì§€ ì•Šê³  ì ê·¹ì ìœ¼ë¡œ í˜‘ë ¥í•  ìˆ˜ ìˆë„ë¡ ê¸ì •ì  í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""),
            ("human", """ì›ë˜ ì§ˆë¬¸: "{query}"
ì‹ ë¢°ë„ ì ìˆ˜: {confidence}
ëª…ë£Œí™” ì§ˆë¬¸ë“¤: {questions}

íš¨ê³¼ì ì¸ ì‚¬ìš©ì ì•ˆë‚´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.""")
        ])
        
        response = await model.ainvoke(guidance_prompt.format_messages(
            query=state["original_query"],
            confidence=confidence_score,
            questions=json.dumps(clarification_questions, ensure_ascii=False)
        ))
        
        # ê¶Œì¥ í–‰ë™
        recommended_actions = [
            "ë” êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ",
            "ëª…ë£Œí™” ì§ˆë¬¸ì— ì‘ë‹µ",
            "ê´€ë ¨ ë§¥ë½ ì •ë³´ ì¶”ê°€"
        ]
        
        user_guidance = {
            "guidance_text": response.content,
            "tone": "ì¹œê·¼í•˜ê³ _ë„ì›€ì´_ë˜ëŠ”",
            "approach": "í˜‘ë ¥ì _ë¬¸ì œí•´ê²°"
        }
        
        return {
            "user_guidance": user_guidance,
            "recommended_actions": recommended_actions,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "guidance_generated_at": time.time()
            }
        }

    async def _construct_response_node(self, state: InformationGapState) -> Dict[str, Any]:
        """8ë‹¨ê³„: ìµœì¢… ì‘ë‹µ êµ¬ì„± ë…¸ë“œ"""
        logger.info("ğŸ” Information Gap: ìµœì¢… ì‘ë‹µ êµ¬ì„± ì¤‘...")
        
        confidence_score = state.get("confidence_score", 0.5)
        user_guidance = state.get("user_guidance", {})
        clarification_questions = state.get("clarification_questions", [])
        
        # ì‘ë‹µ êµ¬ì„±
        response_parts = []
        
        # ìƒí™© ì„¤ëª…
        if confidence_score < 0.6:
            response_parts.append("**ğŸ” ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤**\n")
            response_parts.append("ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ìœ„í•´ ëª‡ ê°€ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\n")
        else:
            response_parts.append("**âœ… ë‹µë³€ ì¤€ë¹„ ì™„ë£Œ**\n")
            response_parts.append("ì œê³µí•´ì£¼ì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        
        # ì‚¬ìš©ì ì•ˆë‚´ ì¶”ê°€
        guidance_text = user_guidance.get("guidance_text", "")
        if guidance_text:
            response_parts.append(f"\n{guidance_text}\n")
        
        # ëª…ë£Œí™” ì§ˆë¬¸ ì¶”ê°€
        if clarification_questions:
            response_parts.append("\n**êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì•Œë ¤ì£¼ì‹œë©´ ë” ë„ì›€ì´ ë©ë‹ˆë‹¤:**\n")
            for i, question in enumerate(clarification_questions[:3], 1):
                response_parts.append(f"{i}. {question}\n")
        
        final_response = "".join(response_parts)
        
        # ë©”íƒ€ë°ì´í„° ê°•í™”
        metadata_enrichment = {
            "analysis_depth": "ìƒì„¸",
            "confidence_level": confidence_score,
            "response_type": "ì •ë³´_ê²©ì°¨_ë¶„ì„",
            "user_interaction_needed": len(clarification_questions) > 0,
            "processing_quality": "ë†’ìŒ"
        }
        
        return {
            "final_response": final_response,
            "metadata_enrichment": metadata_enrichment,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "response_constructed_at": time.time()
            }
        }

    def _get_llm_model(self, model_name: str):
        """LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if "claude" in model_name.lower():
            return ChatAnthropic(
                model_name=model_name,
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.3
            )
        elif "gemini" in model_name.lower():
            return ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=settings.GOOGLE_API_KEY,
                temperature=0.3
            )
        else:
            return ChatAnthropic(
                model_name="claude-3-sonnet-20240229",
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.3
            )

    async def execute(self, input_data: AgentInput, model: str = "claude-sonnet", progress_callback=None) -> AgentOutput:
        """
        LangGraph Information Gap Analyzer ì‹¤í–‰
        100% LangGraph ë²„ì „ (ì—ëŸ¬ ì•ˆì „ ë³´ì¥)
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ LangGraph Information Gap Analyzer ì‹¤í–‰ ì‹œì‘ (ì‚¬ìš©ì: {input_data.user_id})")
        
        try:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (optional)
            try:
                await langgraph_monitor.start_execution("langgraph_information_gap")
            except Exception as monitoring_error:
                logger.warning(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {monitoring_error}")
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = InformationGapState(
                original_query=input_data.query,
                user_id=input_data.user_id,
                session_id=input_data.session_id,
                model=model,
                conversation_context=getattr(input_data, 'conversation_context', None),
                query_understanding=None,
                intent_classification=None,
                complexity_assessment=None,
                domain_analysis=None,
                expertise_requirements=None,
                information_gaps=None,
                missing_context=None,
                ambiguity_points=None,
                resolution_strategy=None,
                clarification_questions=None,
                fallback_approaches=None,
                information_gathering_plan=None,
                research_directions=None,
                answerability_assessment=None,
                confidence_score=None,
                user_guidance=None,
                recommended_actions=None,
                final_response=None,
                metadata_enrichment=None,
                execution_metadata={"start_time": start_time},
                performance_metrics={},
                errors=[],
                error_recovery_attempts=0,
                should_fallback=False
            )
            
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ì—ëŸ¬ ì•ˆì „ ì²˜ë¦¬)
            try:
                if self.checkpointer:
                    app = self.workflow.compile(checkpointer=self.checkpointer)
                    config = {"configurable": {"thread_id": f"info_gap_{input_data.user_id}_{input_data.session_id}"}}
                    final_state = await app.ainvoke(initial_state, config=config)
                else:
                    app = self.workflow.compile()
                    final_state = await app.ainvoke(initial_state)
            except Exception as workflow_error:
                logger.error(f"âŒ LangGraph Information Gap ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {workflow_error}")
                raise workflow_error  # ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ fallback ì²˜ë¦¬
            
            # ê²°ê³¼ ì²˜ë¦¬
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # Fallback í•„ìš” ì‹œ Legacy í˜¸ì¶œ
            if final_state.get("should_fallback", False) or len(final_state.get("errors", [])) >= 3:
                logger.warning(f"ğŸ”„ Information Gap: ì‹¬ê°í•œ ì—ëŸ¬ ë°œìƒ - Legacy ëª¨ë“œë¡œ fallback")
                langgraph_monitor.record_fallback("langgraph_information_gap", f"Errors: {final_state.get('errors', [])}")
                return await self.legacy_agent.execute(input_data, model, progress_callback)
            
            # ì„±ê³µì ì¸ LangGraph ê²°ê³¼ ë°˜í™˜
            try:
                await langgraph_monitor.track_execution(
                    agent_name="langgraph_information_gap",
                    execution_time=execution_time_ms / 1000,
                    status="success",
                    query=input_data.query,
                    response_length=len(final_response) if final_response else 0,
                    user_id=input_data.user_id
                )
            except Exception as monitoring_error:
                logger.warning(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {monitoring_error}")
            
            confidence_score = final_state.get("confidence_score", 0.5)
            final_response = final_state.get("final_response", "ì •ë³´ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
            
            result = AgentOutput(
                result=final_response,
                metadata={
                    "agent_version": "langgraph_v2",
                    "analysis_type": "information_gap",
                    "confidence_score": confidence_score,
                    "needs_clarification": len(final_state.get("clarification_questions", [])) > 0,
                    "information_gaps_count": len(final_state.get("information_gaps", [])),
                    "langgraph_execution": True,
                    **final_state.get("metadata_enrichment", {}),
                    **final_state.get("execution_metadata", {})
                },
                execution_time_ms=execution_time_ms,
                agent_id=self.agent_id,
                model_used=model,
                timestamp=datetime.utcnow().isoformat()
            )
            
            logger.info(f"âœ… LangGraph Information Gap Analyzer ì™„ë£Œ ({execution_time_ms}ms, ì‹ ë¢°ë„: {confidence_score:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ LangGraph Information Gap Analyzer ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            try:
                await langgraph_monitor.track_execution(
                    agent_name="langgraph_information_gap",
                    execution_time=(time.time() - start_time),
                    status="error",
                    query=input_data.query,
                    response_length=0,
                    user_id=input_data.user_id,
                    error_message=str(e)
                )
            except Exception as monitoring_error:
                logger.warning(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {monitoring_error}")
            
            # ì—ëŸ¬ ì‹œ Legacy fallback
            langgraph_monitor.record_fallback("langgraph_information_gap", f"Exception: {str(e)}")
            logger.info("ğŸ”„ ì˜ˆì™¸ ë°œìƒ - Legacy Information Gap Analyzerë¡œ fallback")
            return await self.legacy_agent.execute(input_data, model, progress_callback)

    def get_capabilities(self) -> List[str]:
        """ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "ì§€ëŠ¥í˜• ì¿¼ë¦¬ ì´í•´ ë° ë¶„ì„",
            "ë„ë©”ì¸ ë¶„ë¥˜ ë° ì „ë¬¸ì„± í‰ê°€",
            "ì •ë³´ ê²©ì°¨ ì²´ê³„ì  ë¶„ì„",
            "í•´ê²° ì „ëµ ìˆ˜ë¦½",
            "ì‚¬ìš©ì ë§ì¶¤ ì•ˆë‚´ ìƒì„±",
            "ë‹µë³€ ê°€ëŠ¥ì„± ì •í™•í•œ í‰ê°€",
            "ì—ëŸ¬ ì•ˆì „ ì²˜ë¦¬ ì‹œìŠ¤í…œ"
        ]

    def get_supported_models(self) -> List[str]:
        """ì§€ì› ëª¨ë¸ ëª©ë¡"""
        return [
            "claude-sonnet",
            "claude-haiku", 
            "claude-opus",
            "gemini-pro",
            "gemini-flash"
        ]


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
langgraph_information_gap_analyzer = LangGraphInformationGapAnalyzer()