"""
LangGraph ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œ

ìµœì‹  LangGraph StateGraphë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¬¸ì„œë¥¼ í†µí•© ì²˜ë¦¬í•˜ëŠ”
ê³ ì„±ëŠ¥ ë©€í‹°ëª¨ë‹¬ RAG(Retrieval-Augmented Generation) ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
"""

import time
import asyncio
import json
import uuid
import base64
import io
from typing import Dict, Any, List, Optional, TypedDict, Union, Annotated, BinaryIO
from datetime import datetime
from enum import Enum
from dataclasses import dataclass
import logging
import operator
from pathlib import Path

# LangGraph í•µì‹¬ imports
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# Document processing imports
try:
    import pypdf
    import docx
    from PIL import Image
    import pytesseract
    DOCUMENT_PROCESSING_AVAILABLE = True
except ImportError:
    DOCUMENT_PROCESSING_AVAILABLE = False
    logger.warning("Document processing libraries not available. Installing: pip install pypdf python-docx Pillow pytesseract")

# Embedding and vector store imports
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    VECTOR_PROCESSING_AVAILABLE = True
except ImportError:
    VECTOR_PROCESSING_AVAILABLE = False
    logger.warning("Vector processing libraries not available. Installing: pip install langchain-community faiss-cpu")

# ê¸°ì¡´ ì‹œìŠ¤í…œ imports
from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.core.config import settings
from app.core.feature_flags import is_langgraph_enabled, LangGraphFeatureFlags
from app.services.langgraph_monitor import langgraph_monitor

logger = logging.getLogger(__name__)


class DocumentType(Enum):
    """ë¬¸ì„œ ìœ í˜•"""
    TEXT = "text"
    PDF = "pdf"
    DOCX = "docx"
    IMAGE = "image"
    HTML = "html"
    MARKDOWN = "markdown"
    JSON = "json"


class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ"""
    EXTRACT_ONLY = "extract_only"        # ì¶”ì¶œë§Œ
    SEARCH_AND_EXTRACT = "search_extract" # ê²€ìƒ‰ í›„ ì¶”ì¶œ
    SEMANTIC_SEARCH = "semantic_search"   # ì˜ë¯¸ì  ê²€ìƒ‰
    MULTIMODAL_ANALYSIS = "multimodal"    # ë©€í‹°ëª¨ë‹¬ ë¶„ì„
    COMPREHENSIVE = "comprehensive"       # ì¢…í•© ë¶„ì„


@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬"""
    chunk_id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    source_type: DocumentType = DocumentType.TEXT
    page_number: Optional[int] = None
    position: Optional[Dict[str, Any]] = None


@dataclass
class MultimodalContent:
    """ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸ """
    content_id: str
    text_content: Optional[str]
    image_content: Optional[bytes]
    metadata: Dict[str, Any]
    extracted_features: Optional[Dict[str, Any]] = None
    similarity_scores: Optional[Dict[str, float]] = None


class MultimodalRAGState(TypedDict):
    """LangGraph ë©€í‹°ëª¨ë‹¬ RAG ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    original_query: str
    user_id: str
    session_id: Optional[str]
    model: str
    documents: Optional[List[Dict[str, Any]]]
    processing_mode: Optional[str]
    
    # ë¬¸ì„œ ì²˜ë¦¬
    document_analysis: Optional[Dict[str, Any]]
    extracted_content: Annotated[List[Dict[str, Any]], operator.add]
    processed_chunks: Annotated[List[Dict[str, Any]], operator.add]
    
    # ì„ë² ë”© ë° ë²¡í„°í™”
    embeddings_generated: Optional[Dict[str, Any]]
    vector_store_built: Optional[Dict[str, Any]]
    similarity_search_results: Optional[List[Dict[str, Any]]]
    
    # ë©€í‹°ëª¨ë‹¬ ë¶„ì„
    image_analysis_results: Annotated[List[Dict[str, Any]], operator.add]
    text_analysis_results: Annotated[List[Dict[str, Any]], operator.add]
    cross_modal_correlations: Optional[Dict[str, Any]]
    
    # RAG ê²€ìƒ‰ ë° ìƒì„±
    relevant_context: Optional[Dict[str, Any]]
    augmented_prompt: Optional[str]
    generated_response: Optional[str]
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    execution_metadata: Dict[str, Any]
    processing_stats: Dict[str, Any]
    quality_scores: Dict[str, Any]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: Annotated[List[str], operator.add]
    processing_failures: Annotated[List[Dict[str, Any]], operator.add]
    should_fallback: bool


class LangGraphMultimodalRAGAgent(BaseAgent):
    """LangGraph ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ RAG ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_id="langgraph_multimodal_rag",
            name="LangGraph ë©€í‹°ëª¨ë‹¬ RAG ì—ì´ì „íŠ¸",
            description="LangGraph StateGraphë¡œ êµ¬í˜„ëœ ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œ"
        )
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()
        
        # PostgreSQL ì²´í¬í¬ì¸í„° ì„¤ì •
        if settings.DATABASE_URL:
            self.checkpointer = PostgresSaver.from_conn_string(
                settings.DATABASE_URL,
                
            )
        else:
            self.checkpointer = None
            logger.warning("DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì²´í¬í¬ì¸í„° ë¹„í™œì„±í™”")
        
        # ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
        self.text_splitter = None
        self.embeddings_model = None
        self.vector_store = None
        
        # ì´ˆê¸°í™”
        self._initialize_components()

    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            if VECTOR_PROCESSING_AVAILABLE:
                # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    length_function=len,
                    separators=["\n\n", "\n", " ", ""]
                )
                
                # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (lightweight model)
                self.embeddings_model = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2"
                )
                
                logger.info("âœ… ë©€í‹°ëª¨ë‹¬ RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ ë²¡í„° ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë§Œ ê°€ëŠ¥")
        except Exception as e:
            logger.error(f"âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _build_workflow(self) -> StateGraph:
        """LangGraph ë©€í‹°ëª¨ë‹¬ RAG ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        
        # StateGraph ìƒì„±
        workflow = StateGraph(MultimodalRAGState)
        
        # ë…¸ë“œ ì •ì˜ - 9ë‹¨ê³„ ê³ ë„í™”ëœ RAG íŒŒì´í”„ë¼ì¸
        workflow.add_node("analyze_documents", self._analyze_documents_node)
        workflow.add_node("extract_content", self._extract_content_node)
        workflow.add_node("process_text", self._process_text_node)
        workflow.add_node("process_images", self._process_images_node)
        workflow.add_node("generate_embeddings", self._generate_embeddings_node)
        workflow.add_node("perform_retrieval", self._perform_retrieval_node)
        workflow.add_node("analyze_multimodal", self._analyze_multimodal_node)
        workflow.add_node("augment_generation", self._augment_generation_node)
        workflow.add_node("optimize_response", self._optimize_response_node)
        
        # ì—£ì§€ ì •ì˜ - ë³µí•© íŒŒì´í”„ë¼ì¸
        workflow.set_entry_point("analyze_documents")
        workflow.add_edge("analyze_documents", "extract_content")
        
        # Fan-out: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë³‘ë ¬ ì²˜ë¦¬
        workflow.add_edge("extract_content", "process_text")
        workflow.add_edge("extract_content", "process_images")
        
        # Fan-in: ì„ë² ë”© ìƒì„±ìœ¼ë¡œ ìˆ˜ë ´
        workflow.add_edge("process_text", "generate_embeddings")
        workflow.add_edge("process_images", "generate_embeddings")
        
        # ìˆœì°¨ ì²˜ë¦¬
        workflow.add_edge("generate_embeddings", "perform_retrieval")
        workflow.add_edge("perform_retrieval", "analyze_multimodal")
        workflow.add_edge("analyze_multimodal", "augment_generation")
        workflow.add_edge("augment_generation", "optimize_response")
        workflow.add_edge("optimize_response", END)
        
        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "analyze_documents",
            self._should_continue,
            {
                "continue": "extract_content",
                "fallback": END
            }
        )
        
        workflow.add_conditional_edges(
            "extract_content",
            self._determine_processing_path,
            {
                "text_only": "process_text",
                "image_only": "process_images",
                "multimodal": "process_text"  # ë©€í‹°ëª¨ë‹¬ì€ í…ìŠ¤íŠ¸ë¶€í„° ì‹œì‘
            }
        )
        
        return workflow

    async def _analyze_documents_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ë¬¸ì„œ ë¶„ì„ ë…¸ë“œ - ì…ë ¥ ë¬¸ì„œë“¤ì˜ íŠ¹ì„± íŒŒì•…"""
        try:
            logger.info(f"ğŸ“„ LangGraph MultimodalRAG: ë¬¸ì„œ ë¶„ì„ ì¤‘... (query: {state['original_query'][:50]})")
            
            documents = state.get("documents", [])
            if not documents:
                # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì¿¼ë¦¬ ê¸°ë°˜ ì²˜ë¦¬
                return {
                    "document_analysis": {
                        "document_count": 0,
                        "processing_mode": "query_only",
                        "analysis_result": "ë¬¸ì„œ ì—†ì´ ì¿¼ë¦¬ ê¸°ë°˜ ì²˜ë¦¬"
                    },
                    "execution_metadata": {
                        **state.get("execution_metadata", {}),
                        "document_analysis_completed_at": time.time()
                    }
                }
            
            # ë¬¸ì„œ ìœ í˜•ë³„ ë¶„ì„
            document_types = []
            total_size = 0
            has_images = False
            has_text = False
            
            for doc in documents:
                doc_type = self._detect_document_type(doc)
                document_types.append(doc_type)
                
                if doc_type in [DocumentType.IMAGE]:
                    has_images = True
                else:
                    has_text = True
                
                # í¬ê¸° ì¶”ì •
                content = doc.get("content", "")
                if isinstance(content, str):
                    total_size += len(content.encode('utf-8'))
                elif isinstance(content, bytes):
                    total_size += len(content)
            
            # ì²˜ë¦¬ ëª¨ë“œ ê²°ì •
            if has_images and has_text:
                processing_mode = ProcessingMode.MULTIMODAL_ANALYSIS
            elif has_images:
                processing_mode = ProcessingMode.EXTRACT_ONLY
            else:
                processing_mode = ProcessingMode.SEMANTIC_SEARCH
            
            document_analysis = {
                "document_count": len(documents),
                "document_types": [dt.value for dt in document_types],
                "total_size_bytes": total_size,
                "has_images": has_images,
                "has_text": has_text,
                "processing_mode": processing_mode.value,
                "complexity_score": self._calculate_complexity_score(documents)
            }
            
            return {
                "document_analysis": document_analysis,
                "processing_mode": processing_mode.value,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "document_analysis_completed_at": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"],
                "should_fallback": True
            }

    async def _extract_content_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ì½˜í…ì¸  ì¶”ì¶œ ë…¸ë“œ - ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            logger.info("ğŸ“„ LangGraph MultimodalRAG: ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...")
            
            documents = state.get("documents", [])
            if not documents:
                return {"extracted_content": []}
            
            extracted_content = []
            
            # ë³‘ë ¬ ì¶”ì¶œ ì²˜ë¦¬
            extraction_tasks = []
            for doc in documents:
                task = self._extract_single_document_content(doc)
                extraction_tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*extraction_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ë¬¸ì„œ {i} ì¶”ì¶œ ì‹¤íŒ¨: {result}")
                    extracted_content.append({
                        "doc_id": f"doc_{i}",
                        "success": False,
                        "error": str(result),
                        "content": "",
                        "type": "error"
                    })
                else:
                    extracted_content.append(result)
            
            return {
                "extracted_content": extracted_content,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "content_extraction_completed_at": time.time(),
                    "extracted_documents_count": len([c for c in extracted_content if c.get("success", False)])
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"],
                "extracted_content": []
            }

    async def _process_text_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë…¸ë“œ - í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ì „ì²˜ë¦¬"""
        try:
            logger.info("ğŸ“ LangGraph MultimodalRAG: í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘...")
            
            extracted_content = state.get("extracted_content", [])
            text_contents = [c for c in extracted_content if c.get("type") in ["text", "pdf", "docx", "html"]]
            
            if not text_contents:
                return {"processed_chunks": []}
            
            processed_chunks = []
            
            for content_item in text_contents:
                if not content_item.get("success", False):
                    continue
                
                text = content_item.get("content", "")
                if not text or len(text.strip()) < 10:
                    continue
                
                # í…ìŠ¤íŠ¸ ì²­í‚¹
                if self.text_splitter and VECTOR_PROCESSING_AVAILABLE:
                    chunks = self.text_splitter.split_text(text)
                else:
                    # ê°„ë‹¨í•œ ì²­í‚¹ (fallback)
                    chunk_size = 1000
                    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
                
                # ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ìƒì„±
                for i, chunk in enumerate(chunks):
                    chunk_id = f"{content_item.get('doc_id', 'unknown')}_{i}"
                    processed_chunks.append({
                        "chunk_id": chunk_id,
                        "content": chunk,
                        "metadata": {
                            "source_doc": content_item.get("doc_id"),
                            "chunk_index": i,
                            "chunk_length": len(chunk),
                            "source_type": content_item.get("type"),
                            **content_item.get("metadata", {})
                        },
                        "type": "text_chunk"
                    })
            
            return {
                "processed_chunks": processed_chunks,
                "text_analysis_results": [{
                    "total_chunks": len(processed_chunks),
                    "avg_chunk_length": sum(len(c["content"]) for c in processed_chunks) / len(processed_chunks) if processed_chunks else 0,
                    "processing_method": "recursive_splitter" if self.text_splitter else "simple_chunking"
                }],
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "text_processing_completed_at": time.time(),
                    "text_chunks_created": len(processed_chunks)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"],
                "processed_chunks": []
            }

    async def _process_images_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ë…¸ë“œ - ì´ë¯¸ì§€ ë¶„ì„ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            logger.info("ğŸ–¼ï¸ LangGraph MultimodalRAG: ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
            
            extracted_content = state.get("extracted_content", [])
            image_contents = [c for c in extracted_content if c.get("type") == "image"]
            
            if not image_contents:
                return {"image_analysis_results": []}
            
            image_analysis_results = []
            
            # ë³‘ë ¬ ì´ë¯¸ì§€ ì²˜ë¦¬
            processing_tasks = []
            for image_content in image_contents:
                task = self._process_single_image(image_content)
                processing_tasks.append(task)
            
            results = await asyncio.gather(*processing_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                    image_analysis_results.append({
                        "image_id": f"image_{i}",
                        "success": False,
                        "error": str(result),
                        "extracted_text": "",
                        "analysis": {}
                    })
                else:
                    image_analysis_results.append(result)
            
            return {
                "image_analysis_results": image_analysis_results,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "image_processing_completed_at": time.time(),
                    "images_processed": len(image_analysis_results)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"],
                "image_analysis_results": []
            }

    async def _generate_embeddings_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ì„ë² ë”© ìƒì„± ë…¸ë“œ - í…ìŠ¤íŠ¸ ë²¡í„°í™” ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        try:
            logger.info("ğŸ”¢ LangGraph MultimodalRAG: ì„ë² ë”© ìƒì„± ì¤‘...")
            
            processed_chunks = state.get("processed_chunks", [])
            text_analysis_results = state.get("text_analysis_results", [])
            
            if not processed_chunks and not text_analysis_results:
                return {"embeddings_generated": {"status": "no_content"}}
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì½˜í…ì¸  ìˆ˜ì§‘
            texts_to_embed = []
            metadata_list = []
            
            # ì²˜ë¦¬ëœ ì²­í¬ì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            for chunk in processed_chunks:
                if chunk.get("content"):
                    texts_to_embed.append(chunk["content"])
                    metadata_list.append(chunk.get("metadata", {}))
            
            # ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¶”ê°€
            image_results = state.get("image_analysis_results", [])
            for img_result in image_results:
                extracted_text = img_result.get("extracted_text", "")
                if extracted_text and len(extracted_text.strip()) > 5:
                    texts_to_embed.append(extracted_text)
                    metadata_list.append({
                        "source_type": "image_ocr",
                        "image_id": img_result.get("image_id", "unknown")
                    })
            
            if not texts_to_embed:
                return {"embeddings_generated": {"status": "no_texts"}}
            
            # ì„ë² ë”© ìƒì„± ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
            if self.embeddings_model and VECTOR_PROCESSING_AVAILABLE:
                try:
                    # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                    vector_store = FAISS.from_texts(
                        texts_to_embed,
                        self.embeddings_model,
                        metadatas=metadata_list
                    )
                    
                    embeddings_generated = {
                        "status": "success",
                        "embeddings_count": len(texts_to_embed),
                        "vector_store_created": True,
                        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2"
                    }
                    
                    # ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒíƒœë¡œ ì €ì¥ (ì‹¤ì œë¡œëŠ” ë©”ëª¨ë¦¬ì— ë³´ê´€)
                    self.vector_store = vector_store
                    
                except Exception as e:
                    logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
                    embeddings_generated = {
                        "status": "failed",
                        "error": str(e),
                        "texts_count": len(texts_to_embed)
                    }
            else:
                # ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°
                embeddings_generated = {
                    "status": "no_embedding_model",
                    "texts_count": len(texts_to_embed),
                    "fallback_mode": True
                }
            
            return {
                "embeddings_generated": embeddings_generated,
                "vector_store_built": {"vector_store_available": self.vector_store is not None},
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "embeddings_generation_completed_at": time.time(),
                    "texts_embedded": len(texts_to_embed)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}"],
                "embeddings_generated": {"status": "error", "error": str(e)}
            }

    async def _perform_retrieval_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ê²€ìƒ‰ ìˆ˜í–‰ ë…¸ë“œ - ì˜ë¯¸ì  ê²€ìƒ‰ ë° ê´€ë ¨ ì½˜í…ì¸  ê²€ìƒ‰"""
        try:
            logger.info("ğŸ” LangGraph MultimodalRAG: ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            
            query = state["original_query"]
            embeddings_generated = state.get("embeddings_generated", {})
            
            if embeddings_generated.get("status") != "success" or not self.vector_store:
                # ë²¡í„° ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
                return await self._perform_keyword_search(state)
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ìˆ˜í–‰
            try:
                # ìœ ì‚¬ë„ ê²€ìƒ‰ (top_k=5)
                search_results = self.vector_store.similarity_search_with_score(
                    query=query,
                    k=min(5, embeddings_generated.get("embeddings_count", 1))
                )
                
                similarity_search_results = []
                for doc, score in search_results:
                    similarity_search_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "similarity_score": float(score),
                        "relevance_score": max(0, 1 - score)  # ê±°ë¦¬ë¥¼ ê´€ë ¨ì„±ìœ¼ë¡œ ë³€í™˜
                    })
                
                # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                relevant_context = {
                    "search_method": "semantic_similarity",
                    "query": query,
                    "results_count": len(similarity_search_results),
                    "top_results": similarity_search_results,
                    "context_length": sum(len(r["content"]) for r in similarity_search_results)
                }
                
                return {
                    "similarity_search_results": similarity_search_results,
                    "relevant_context": relevant_context,
                    "execution_metadata": {
                        **state.get("execution_metadata", {}),
                        "retrieval_completed_at": time.time(),
                        "retrieved_results_count": len(similarity_search_results)
                    }
                }
                
            except Exception as e:
                logger.error(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                return await self._perform_keyword_search(state)
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ê²€ìƒ‰ ìˆ˜í–‰ ì‹¤íŒ¨: {str(e)}"],
                "similarity_search_results": []
            }

    async def _analyze_multimodal_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ë…¸ë“œ - í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            logger.info("ğŸ”— LangGraph MultimodalRAG: ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì¤‘...")
            
            text_results = state.get("text_analysis_results", [])
            image_results = state.get("image_analysis_results", [])
            similarity_results = state.get("similarity_search_results", [])
            
            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ëª¨ë‘ ìˆëŠ” ê²½ìš°ì—ë§Œ ìƒê´€ê´€ê³„ ë¶„ì„
            if not text_results or not image_results:
                return {
                    "cross_modal_correlations": {
                        "analysis_performed": False,
                        "reason": "insufficient_multimodal_content"
                    }
                }
            
            model = self._get_llm_model(state["model"])
            
            # ë©€í‹°ëª¨ë‹¬ ìƒê´€ê´€ê³„ ë¶„ì„
            correlation_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì½˜í…ì¸  ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

ë¶„ì„ í•­ëª©:
1. ë‚´ìš©ì  ì¼ì¹˜ì„±: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ê°™ì€ ì£¼ì œ/ê°œë…ì„ ë‹¤ë£¨ëŠ”ê°€
2. ë³´ì™„ì„±: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì„œë¡œë¥¼ ë³´ì™„í•˜ëŠ”ê°€
3. ì¤‘ë³µì„±: ë™ì¼í•œ ì •ë³´ê°€ ì¤‘ë³µë˜ëŠ”ê°€
4. ë§¥ë½ì  ì—°ê´€ì„±: ì „ì²´ì ì¸ ë¬¸ì„œ ë§¥ë½ì—ì„œì˜ ê´€ë ¨ì„±
5. ì •ë³´ í’ˆì§ˆ: ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ì •ë³´ í’ˆì§ˆ

JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”."""),
                ("human", """ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:
{text_results}

ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:
{image_results}

ê²€ìƒ‰ëœ ê´€ë ¨ ì½˜í…ì¸ :
{search_results}

ë©€í‹°ëª¨ë‹¬ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.""")
            ])
            
            response = await model.ainvoke(correlation_prompt.format_messages(
                query=state["original_query"],
                text_results=json.dumps(text_results[:3], ensure_ascii=False),  # ìƒìœ„ 3ê°œë§Œ
                image_results=json.dumps([{
                    "image_id": r.get("image_id"),
                    "extracted_text": r.get("extracted_text", "")[:200],  # 200ì ì œí•œ
                    "success": r.get("success")
                } for r in image_results[:3]], ensure_ascii=False),
                search_results=json.dumps([{
                    "content": r.get("content", "")[:300],  # 300ì ì œí•œ
                    "relevance_score": r.get("relevance_score", 0)
                } for r in similarity_results[:3]], ensure_ascii=False)
            ))
            
            try:
                cross_modal_correlations = json.loads(response.content)
            except json.JSONDecodeError:
                cross_modal_correlations = {
                    "analysis_performed": True,
                    "content_alignment": 0.7,
                    "complementarity": 0.6,
                    "redundancy": 0.3,
                    "contextual_relevance": 0.8,
                    "overall_correlation": 0.65,
                    "analysis_method": "llm_based"
                }
            
            cross_modal_correlations["analysis_performed"] = True
            cross_modal_correlations["text_sources"] = len(text_results)
            cross_modal_correlations["image_sources"] = len(image_results)
            
            return {
                "cross_modal_correlations": cross_modal_correlations,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "multimodal_analysis_completed_at": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"],
                "cross_modal_correlations": {"analysis_performed": False, "error": str(e)}
            }

    async def _augment_generation_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ì¦ê°• ìƒì„± ë…¸ë“œ - RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        try:
            logger.info("âœ¨ LangGraph MultimodalRAG: ì¦ê°• ìƒì„± ì¤‘...")
            
            relevant_context = state.get("relevant_context", {})
            cross_modal_correlations = state.get("cross_modal_correlations", {})
            query = state["original_query"]
            
            model = self._get_llm_model(state["model"])
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            
            # ê²€ìƒ‰ëœ ê´€ë ¨ ì½˜í…ì¸ 
            if relevant_context.get("top_results"):
                context_parts.append("**ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´:**")
                for i, result in enumerate(relevant_context["top_results"][:3], 1):
                    context_parts.append(f"{i}. {result.get('content', '')[:400]}...")
            
            # ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
            image_results = state.get("image_analysis_results", [])
            if image_results:
                extracted_texts = [r.get("extracted_text", "") for r in image_results if r.get("success")]
                if extracted_texts:
                    context_parts.append("\n**ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì •ë³´:**")
                    for i, text in enumerate(extracted_texts[:2], 1):
                        if text.strip():
                            context_parts.append(f"{i}. {text[:300]}...")
            
            # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼
            if cross_modal_correlations.get("analysis_performed"):
                context_parts.append(f"\n**ë©€í‹°ëª¨ë‹¬ ë¶„ì„:** í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒê´€ë„ {cross_modal_correlations.get('overall_correlation', 0.5):.2f}")
            
            augmented_context = "\n".join(context_parts) if context_parts else "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
            rag_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ë©€í‹°ëª¨ë‹¬ RAG ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€)ì—ì„œ ì¶”ì¶œëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  í¬ê´„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€ ì›ì¹™:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìš°ì„  í™œìš©
2. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ í†µí•©ì ìœ¼ë¡œ ê³ ë ¤
3. ì •í™•í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…ì‹œ
4. ì¶œì²˜ê°€ ëª…í™•í•œ ì •ë³´ì™€ ì¶”ë¡ ëœ ì •ë³´ êµ¬ë¶„
5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±

ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ëŠ” "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”."""),
                ("human", """ì§ˆë¬¸: {query}

ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.""")
            ])
            
            response = await model.ainvoke(rag_prompt.format_messages(
                query=query,
                context=augmented_context
            ))
            
            generated_response = response.content
            
            # ì¦ê°• í”„ë¡¬í”„íŠ¸ ì •ë³´ ì €ì¥
            augmented_prompt_info = {
                "original_query": query,
                "context_sources": len(relevant_context.get("top_results", [])),
                "image_sources": len([r for r in image_results if r.get("success")]),
                "context_length": len(augmented_context),
                "multimodal_analysis": cross_modal_correlations.get("analysis_performed", False)
            }
            
            return {
                "augmented_prompt": json.dumps(augmented_prompt_info, ensure_ascii=False),
                "generated_response": generated_response,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "augmented_generation_completed_at": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì¦ê°• ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì¦ê°• ìƒì„± ì‹¤íŒ¨: {str(e)}"],
                "generated_response": "ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }

    async def _optimize_response_node(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """ì‘ë‹µ ìµœì í™” ë…¸ë“œ - ìµœì¢… ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ"""
        try:
            logger.info("ğŸ¯ LangGraph MultimodalRAG: ì‘ë‹µ ìµœì í™” ì¤‘...")
            
            generated_response = state.get("generated_response", "")
            document_analysis = state.get("document_analysis", {})
            relevant_context = state.get("relevant_context", {})
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_scores = {
                "response_length_score": min(100, len(generated_response) / 10),  # ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
                "context_utilization_score": min(100, len(relevant_context.get("top_results", [])) * 25),
                "multimodal_integration_score": 0,
                "overall_quality_score": 0
            }
            
            # ë©€í‹°ëª¨ë‹¬ í†µí•© ì ìˆ˜
            cross_modal = state.get("cross_modal_correlations", {})
            if cross_modal.get("analysis_performed"):
                quality_scores["multimodal_integration_score"] = cross_modal.get("overall_correlation", 0.5) * 100
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_scores["overall_quality_score"] = (
                quality_scores["response_length_score"] * 0.3 +
                quality_scores["context_utilization_score"] * 0.4 +
                quality_scores["multimodal_integration_score"] * 0.3
            )
            
            # ì²˜ë¦¬ í†µê³„
            start_time = state.get("execution_metadata", {}).get("document_analysis_completed_at", time.time())
            processing_stats = {
                "total_processing_time_ms": int((time.time() - start_time) * 1000),
                "documents_processed": document_analysis.get("document_count", 0),
                "chunks_created": len(state.get("processed_chunks", [])),
                "embeddings_generated": state.get("embeddings_generated", {}).get("embeddings_count", 0),
                "search_results": len(state.get("similarity_search_results", [])),
                "processing_mode": state.get("processing_mode", "unknown")
            }
            
            return {
                "quality_scores": quality_scores,
                "processing_stats": processing_stats,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "response_optimization_completed_at": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {
                "errors": [f"ì‘ë‹µ ìµœì í™” ì‹¤íŒ¨: {str(e)}"],
                "quality_scores": {"overall_quality_score": 50}
            }

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤

    def _should_continue(self, state: MultimodalRAGState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ê³„ì† ì§„í–‰ ì—¬ë¶€"""
        if state.get("should_fallback", False) or len(state.get("errors", [])) > 3:
            return "fallback"
        return "continue"

    def _determine_processing_path(self, state: MultimodalRAGState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ…: ì²˜ë¦¬ ê²½ë¡œ ê²°ì •"""
        document_analysis = state.get("document_analysis", {})
        
        if document_analysis.get("has_images") and document_analysis.get("has_text"):
            return "multimodal"
        elif document_analysis.get("has_images"):
            return "image_only"
        else:
            return "text_only"

    def _get_llm_model(self, model_name: str):
        """LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if "claude" in model_name.lower():
            return ChatAnthropic(
                model_name=model_name,
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.1
            )
        elif "gemini" in model_name.lower():
            return ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=settings.GOOGLE_API_KEY,
                temperature=0.1
            )
        else:
            return ChatAnthropic(
                model_name="claude-3-sonnet-20240229",
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.1
            )

    def _detect_document_type(self, document: Dict[str, Any]) -> DocumentType:
        """ë¬¸ì„œ ìœ í˜• ê°ì§€"""
        filename = document.get("filename", "").lower()
        content_type = document.get("content_type", "").lower()
        
        if filename.endswith(('.pdf',)) or 'pdf' in content_type:
            return DocumentType.PDF
        elif filename.endswith(('.docx', '.doc')) or 'word' in content_type:
            return DocumentType.DOCX
        elif filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')) or 'image' in content_type:
            return DocumentType.IMAGE
        elif filename.endswith(('.html', '.htm')):
            return DocumentType.HTML
        elif filename.endswith(('.md', '.markdown')):
            return DocumentType.MARKDOWN
        elif filename.endswith(('.json',)):
            return DocumentType.JSON
        else:
            return DocumentType.TEXT

    def _calculate_complexity_score(self, documents: List[Dict[str, Any]]) -> int:
        """ë¬¸ì„œ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        score += len(documents) * 10  # ë¬¸ì„œ ìˆ˜
        
        for doc in documents:
            content = doc.get("content", "")
            if isinstance(content, str):
                score += len(content) // 1000  # í…ìŠ¤íŠ¸ ê¸¸ì´
            elif isinstance(content, bytes):
                score += len(content) // 10000  # ë°”ì´ë„ˆë¦¬ í¬ê¸°
        
        return min(100, score)

    async def _extract_single_document_content(self, document: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        doc_type = self._detect_document_type(document)
        doc_id = document.get("id", f"doc_{uuid.uuid4().hex[:8]}")
        
        try:
            if doc_type == DocumentType.TEXT:
                content = document.get("content", "")
                return {
                    "doc_id": doc_id,
                    "success": True,
                    "content": content,
                    "type": "text",
                    "metadata": {"original_length": len(content)}
                }
            
            elif doc_type == DocumentType.IMAGE:
                # ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” ë³„ë„ ë…¸ë“œì—ì„œ ìˆ˜í–‰
                return {
                    "doc_id": doc_id,
                    "success": True,
                    "content": "",  # ì´ë¯¸ì§€ ìì²´ëŠ” ì—¬ê¸°ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì§€ ì•ŠìŒ
                    "type": "image",
                    "metadata": {"requires_ocr": True},
                    "image_data": document.get("content")
                }
            
            elif doc_type == DocumentType.PDF:
                if not DOCUMENT_PROCESSING_AVAILABLE:
                    return {
                        "doc_id": doc_id,
                        "success": False,
                        "error": "PDF processing not available",
                        "content": "",
                        "type": "pdf"
                    }
                
                # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ êµ¬í˜„)
                content = document.get("content", "")
                return {
                    "doc_id": doc_id,
                    "success": True,
                    "content": content,  # ì‹¤ì œë¡œëŠ” pypdf ì‚¬ìš©
                    "type": "pdf",
                    "metadata": {"extraction_method": "text_only"}
                }
            
            else:
                # ê¸°íƒ€ ë¬¸ì„œ ìœ í˜•ì€ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                content = str(document.get("content", ""))
                return {
                    "doc_id": doc_id,
                    "success": True,
                    "content": content,
                    "type": doc_type.value,
                    "metadata": {"converted_to_text": True}
                }
            
        except Exception as e:
            return {
                "doc_id": doc_id,
                "success": False,
                "error": str(e),
                "content": "",
                "type": doc_type.value
            }

    async def _process_single_image(self, image_content: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (OCR ë“±)"""
        image_id = image_content.get("doc_id", "unknown")
        
        try:
            # OCRì´ ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            if DOCUMENT_PROCESSING_AVAILABLE:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” pytesseract ì‚¬ìš©
                extracted_text = "ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (OCR ë¯¸êµ¬í˜„)"
            else:
                extracted_text = ""
            
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë¶„ì„
            image_analysis = {
                "has_text": len(extracted_text) > 10,
                "estimated_content_type": "mixed",
                "processing_method": "ocr_extraction" if DOCUMENT_PROCESSING_AVAILABLE else "not_available"
            }
            
            return {
                "image_id": image_id,
                "success": True,
                "extracted_text": extracted_text,
                "analysis": image_analysis,
                "metadata": image_content.get("metadata", {})
            }
            
        except Exception as e:
            return {
                "image_id": image_id,
                "success": False,
                "error": str(e),
                "extracted_text": "",
                "analysis": {}
            }

    async def _perform_keyword_search(self, state: MultimodalRAGState) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (fallback)"""
        query = state["original_query"].lower()
        query_words = set(query.split())
        
        # ì²˜ë¦¬ëœ ì²­í¬ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        processed_chunks = state.get("processed_chunks", [])
        keyword_results = []
        
        for chunk in processed_chunks:
            content = chunk.get("content", "").lower()
            content_words = set(content.split())
            
            # ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            overlap = query_words.intersection(content_words)
            if overlap:
                score = len(overlap) / len(query_words)
                keyword_results.append({
                    "content": chunk.get("content", ""),
                    "metadata": chunk.get("metadata", {}),
                    "similarity_score": score,
                    "relevance_score": score,
                    "matching_keywords": list(overlap)
                })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        keyword_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        relevant_context = {
            "search_method": "keyword_matching",
            "query": state["original_query"],
            "results_count": len(keyword_results),
            "top_results": keyword_results[:5],
            "context_length": sum(len(r["content"]) for r in keyword_results[:5])
        }
        
        return {
            "similarity_search_results": keyword_results[:5],
            "relevant_context": relevant_context,
            "execution_metadata": {
                **state.get("execution_metadata", {}),
                "retrieval_completed_at": time.time(),
                "retrieval_method": "keyword_fallback",
                "retrieved_results_count": len(keyword_results[:5])
            }
        }

    async def execute_multimodal_rag(
        self, 
        input_data: AgentInput, 
        documents: Optional[List[Dict[str, Any]]] = None,
        model: str = "claude-sonnet", 
        progress_callback=None
    ) -> AgentOutput:
        """
        ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œ ì‹¤í–‰
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ LangGraph Multimodal RAG ì‹¤í–‰ ì‹œì‘ (ì‚¬ìš©ì: {input_data.user_id})")
        
        try:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (optional)
            try:
                await langgraph_monitor.start_execution("langgraph_multimodal_rag")
            except Exception as monitoring_error:
                logger.warning(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {monitoring_error}")
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì • (Reducer ê¸°ë³¸ê°’)
            initial_state = MultimodalRAGState(
                original_query=input_data.query,
                user_id=input_data.user_id,
                session_id=input_data.session_id,
                model=model,
                documents=documents or [],
                processing_mode=None,
                document_analysis=None,
                extracted_content=[],
                processed_chunks=[],
                embeddings_generated=None,
                vector_store_built=None,
                similarity_search_results=None,
                image_analysis_results=[],
                text_analysis_results=[],
                cross_modal_correlations=None,
                relevant_context=None,
                augmented_prompt=None,
                generated_response=None,
                execution_metadata={"start_time": start_time},
                processing_stats={},
                quality_scores={},
                errors=[],
                processing_failures=[],
                should_fallback=False
            )
            
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ì—ëŸ¬ ì•ˆì „ ì²˜ë¦¬)
            try:
                if self.checkpointer:
                    app = self.workflow.compile(checkpointer=self.checkpointer)
                    config = {"configurable": {"thread_id": f"multimodal_rag_{input_data.user_id}_{input_data.session_id}"}}
                    final_state = await app.ainvoke(initial_state, config=config)
                else:
                    app = self.workflow.compile()
                    final_state = await app.ainvoke(initial_state)
            except Exception as workflow_error:
                logger.error(f"âŒ LangGraph Multimodal RAG ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {workflow_error}")
                raise workflow_error  # ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ fallback ì²˜ë¦¬
            
            # ê²°ê³¼ ì²˜ë¦¬
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # ì—ëŸ¬ê°€ ìˆê±°ë‚˜ fallbackì´ í•„ìš”í•œ ê²½ìš°
            if final_state.get("should_fallback", False) or len(final_state.get("errors", [])) > 0:
                logger.warning("ğŸ”„ LangGraph Multimodal RAG ì‹¤í–‰ ì‹¤íŒ¨")
                return AgentOutput(
                    result="ë©€í‹°ëª¨ë‹¬ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ëª¨ë“œë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.",
                    metadata={
                        "agent_version": "langgraph_multimodal_rag",
                        "processing_failed": True,
                        "errors": final_state.get("errors", [])
                    },
                    execution_time_ms=execution_time_ms,
                    agent_id=self.agent_id,
                    model_used=model,
                    timestamp=datetime.utcnow().isoformat()
                )
            
            # ì„±ê³µì ì¸ ê²°ê³¼ ë°˜í™˜
            generated_response = final_state.get("generated_response", "ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            quality_scores = final_state.get("quality_scores", {})
            processing_stats = final_state.get("processing_stats", {})
            
            try:
                await langgraph_monitor.track_execution(
                    agent_name="langgraph_multimodal_rag",
                    execution_time=execution_time_ms / 1000,
                    status="success",
                    query=input_data.query,
                    response_length=len(generated_response) if generated_response else 0,
                    user_id=input_data.user_id
                )
            except Exception as monitoring_error:
                logger.warning(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {monitoring_error}")
            
            result = AgentOutput(
                result=generated_response,
                metadata={
                    "agent_version": "langgraph_multimodal_rag_v2",
                    "multimodal_processing": True,
                    "quality_score": quality_scores.get("overall_quality_score", 0),
                    "processing_stats": processing_stats,
                    "documents_processed": len(documents or []),
                    "langgraph_execution": True,
                    "multimodal_features": {
                        "text_processing": len(final_state.get("processed_chunks", [])) > 0,
                        "image_processing": len(final_state.get("image_analysis_results", [])) > 0,
                        "semantic_search": final_state.get("embeddings_generated", {}).get("status") == "success",
                        "cross_modal_analysis": final_state.get("cross_modal_correlations", {}).get("analysis_performed", False)
                    },
                    **final_state.get("execution_metadata", {})
                },
                execution_time_ms=execution_time_ms,
                agent_id=self.agent_id,
                model_used=model,
                timestamp=datetime.utcnow().isoformat()
            )
            
            quality_score = quality_scores.get("overall_quality_score", 0)
            logger.info(f"âœ… LangGraph Multimodal RAG ì™„ë£Œ ({execution_time_ms}ms, í’ˆì§ˆ: {quality_score:.1f}/100)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ LangGraph Multimodal RAG ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            try:
                await langgraph_monitor.track_execution(
                    agent_name="langgraph_multimodal_rag",
                    execution_time=(time.time() - start_time),
                    status="error",
                    query=input_data.query,
                    response_length=0,
                    user_id=input_data.user_id,
                    error_message=str(e)
                )
            except Exception as monitoring_error:
                logger.warning(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {monitoring_error}")
            
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            return AgentOutput(
                result="ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì—ì„œ ì¼ì‹œì  ì²˜ë¦¬ ì§€ì—°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                metadata={
                    "agent_version": "langgraph_multimodal_rag_v2",
                    "error_occurred": True,
                    "error_handled": True
                },
                execution_time_ms=execution_time_ms,
                agent_id=self.agent_id,
                model_used=model,
                timestamp=datetime.utcnow().isoformat(),
                error=f"LangGraph Multimodal RAG error: {str(e)}"
            )

    def get_capabilities(self) -> List[str]:
        """ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "ë‹¤ì¤‘ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬ (PDF, DOCX, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸)",
            "OCR ê¸°ë°˜ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ",
            "ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰ (FAISS ë²¡í„° ìŠ¤í† ì–´)",
            "ë©€í‹°ëª¨ë‹¬ ìƒê´€ê´€ê³„ ë¶„ì„",
            "RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±",
            "ì‹¤ì‹œê°„ í’ˆì§ˆ í‰ê°€",
            "ì ì‘í˜• ì²˜ë¦¬ ëª¨ë“œ",
            "í‚¤ì›Œë“œ ê¸°ë°˜ fallback ê²€ìƒ‰"
        ]

    def get_supported_models(self) -> List[str]:
        """ì§€ì› ëª¨ë¸ ëª©ë¡"""
        return [
            "claude-sonnet",
            "claude-haiku", 
            "claude-opus",
            "gemini-pro",
            "gemini-flash"
        ]


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
langgraph_multimodal_rag_agent = LangGraphMultimodalRAGAgent()