"""
LangGraph ê¸°ë°˜ WebSearchAgent - ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ êµ¬í˜„

ì´ ëª¨ë“ˆì€ ê¸°ì¡´ WebSearchAgentì˜ LangGraph ë²„ì „ì…ë‹ˆë‹¤.
Feature Flagë¥¼ í†µí•´ ì ì§„ì ìœ¼ë¡œ í™œì„±í™”ë˜ë©°, ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ë ˆê±°ì‹œ ë²„ì „ìœ¼ë¡œ fallbackë©ë‹ˆë‹¤.
"""

import time
import asyncio
import json
from typing import Dict, Any, List, Optional, TypedDict, Union
from datetime import datetime
import logging

# LangGraph í•µì‹¬ imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# ê¸°ì¡´ ì‹œìŠ¤í…œ imports
from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.agents.workers.web_search import WebSearchAgent, SearchQuery, EnhancedSearchResult
from app.services.search_service import search_service
from app.services.web_crawler import web_crawler
from app.core.config import settings
from app.core.feature_flags import is_langgraph_enabled, LangGraphFeatureFlags
from app.services.langgraph_monitor import langgraph_monitor

logger = logging.getLogger(__name__)


class WebSearchState(TypedDict):
    """LangGraph ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ìƒíƒœ ì •ì˜"""
    # ì…ë ¥ ë°ì´í„°
    original_query: str
    user_id: str
    session_id: Optional[str]
    model: str
    
    # ê²€ìƒ‰ ê³„íš ë‹¨ê³„
    search_plan: Optional[Dict[str, Any]]
    search_queries: List[SearchQuery]
    
    # ê²€ìƒ‰ ì‹¤í–‰ ë‹¨ê³„
    search_results: List[EnhancedSearchResult]
    raw_content: List[Dict[str, Any]]
    
    # ë¶„ì„ ë° ì¢…í•© ë‹¨ê³„
    relevance_analysis: Optional[Dict[str, Any]]
    synthesized_answer: Optional[str]
    final_response: Optional[str]
    
    # ë©”íƒ€ë°ì´í„°
    execution_metadata: Dict[str, Any]
    citations: List[Dict[str, Any]]
    sources: List[Dict[str, Any]]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: List[str]
    should_fallback: bool


class LangGraphWebSearchAgent(BaseAgent):
    """LangGraph ê¸°ë°˜ ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_id="langgraph_web_search",
            name="LangGraph ì›¹ ê²€ìƒ‰ ì—ì´ê°„íŠ¸",
            description="LangGraph StateGraphë¡œ êµ¬í˜„ëœ ê³ ê¸‰ ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸"
        )
        
        # ë ˆê±°ì‹œ ì—ì´ì „íŠ¸ (fallbackìš©)
        self.legacy_agent = WebSearchAgent()
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()
        
        # PostgreSQL ì²´í¬í¬ì¸í„° ì„¤ì • (ìƒíƒœ ì˜ì†ì„±)
        if settings.DATABASE_URL:
            self.checkpointer = PostgresSaver.from_conn_string(
                settings.DATABASE_URL,
                
            )
        else:
            self.checkpointer = None
            logger.warning("DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì²´í¬í¬ì¸í„° ë¹„í™œì„±í™”")

    def _build_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        
        # StateGraph ìƒì„±
        workflow = StateGraph(WebSearchState)
        
        # ë…¸ë“œ ì •ì˜
        workflow.add_node("plan_search", self._plan_search_node)
        workflow.add_node("execute_search", self._execute_search_node)
        workflow.add_node("analyze_relevance", self._analyze_relevance_node)
        workflow.add_node("synthesize_answer", self._synthesize_answer_node)
        workflow.add_node("generate_response", self._generate_response_node)
        
        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("plan_search")
        workflow.add_edge("plan_search", "execute_search")
        workflow.add_edge("execute_search", "analyze_relevance")
        workflow.add_edge("analyze_relevance", "synthesize_answer")
        workflow.add_edge("synthesize_answer", "generate_response")
        workflow.add_edge("generate_response", END)
        
        # ì¡°ê±´ë¶€ ì—£ì§€ (ì—ëŸ¬ ì²˜ë¦¬)
        workflow.add_conditional_edges(
            "plan_search",
            self._should_continue,
            {
                "continue": "execute_search",
                "fallback": END
            }
        )
        
        return workflow

    async def _plan_search_node(self, state: WebSearchState) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê³„íš ìƒì„± ë…¸ë“œ"""
        try:
            logger.info(f"ğŸ” LangGraph: ê²€ìƒ‰ ê³„íš ìƒì„± ì¤‘... (query: {state['original_query'][:50]})")
            
            # LLMì„ ì‚¬ìš©í•œ ê²€ìƒ‰ ê³„íš ìƒì„±
            model = self._get_llm_model(state["model"])
            
            planning_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ì›¹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

ê²€ìƒ‰ ê³„íšì— í¬í•¨í•´ì•¼ í•  ìš”ì†Œ:
1. í•µì‹¬ ê²€ìƒ‰ì–´ 3-5ê°œ (ìš°ì„ ìˆœìœ„ë³„ë¡œ)
2. ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜ (ì •ë³´í˜•/ì¶”ì²œí˜•/ë¹„êµí˜•/ë°©ë²•í˜•)
3. ì–¸ì–´ ì„¤ì • (í•œêµ­ì–´/ì˜ì–´)
4. íŠ¹í™” ê²€ìƒ‰ ì „ëµ (ì¼ë°˜/ì‚¬ì´íŠ¸íŠ¹í™”/URLí¬ë¡¤ë§)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""),
                ("human", "ì§ˆë¬¸: {query}")
            ])
            
            response = await model.ainvoke(planning_prompt.format_messages(query=state["original_query"]))
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                search_plan = json.loads(response.content)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê³„íš ìƒì„±
                search_plan = {
                    "primary_queries": [state["original_query"]],
                    "intent_type": "ì •ë³´í˜•",
                    "language": "ko",
                    "search_strategy": "ì¼ë°˜"
                }
            
            # SearchQuery ê°ì²´ë“¤ ìƒì„±
            search_queries = []
            for i, query in enumerate(search_plan.get("primary_queries", [])):
                search_queries.append(SearchQuery(
                    query=query,
                    priority=1 if i < 2 else 2,
                    intent_type=search_plan.get("intent_type", "ì •ë³´í˜•"),
                    language=search_plan.get("language", "ko"),
                    max_results=5,
                    search_type="general"
                ))
            
            return {
                "search_plan": search_plan,
                "search_queries": search_queries,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "plan_generation_time": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ê³„íš ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "errors": state.get("errors", []) + [f"ê²€ìƒ‰ ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}"],
                "should_fallback": True
            }

    async def _execute_search_node(self, state: WebSearchState) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì‹¤í–‰ ë…¸ë“œ"""
        try:
            logger.info(f"ğŸ” LangGraph: ê²€ìƒ‰ ì‹¤í–‰ ì¤‘... ({len(state['search_queries'])}ê°œ ì¿¼ë¦¬)")
            
            search_results = []
            raw_content = []
            
            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
            search_tasks = []
            for query in state["search_queries"]:
                task = self._execute_single_search(query)
                search_tasks.append(task)
            
            results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨ (ì¿¼ë¦¬ {i}): {result}")
                    continue
                
                search_results.append(result)
                raw_content.extend(result.results)
            
            return {
                "search_results": search_results,
                "raw_content": raw_content,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "search_execution_time": time.time(),
                    "total_results": len(raw_content)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "errors": state.get("errors", []) + [f"ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"],
                "should_fallback": True
            }

    async def _execute_single_search(self, query: SearchQuery) -> EnhancedSearchResult:
        """ë‹¨ì¼ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            # ê¸°ì¡´ search_service í™œìš©
            search_results = await search_service.search(
                query=query.query,
                max_results=query.max_results,
                language=query.language
            )
            
            return EnhancedSearchResult(
                search_query=query,
                results=search_results,
                relevance_score=0.8,  # ì„ì‹œê°’, ì¶”í›„ LLM ê¸°ë°˜ ìŠ¤ì½”ì–´ë§
                success=True
            )
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return EnhancedSearchResult(
                search_query=query,
                results=[],
                relevance_score=0.0,
                success=False
            )

    async def _analyze_relevance_node(self, state: WebSearchState) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ë¶„ì„ ë…¸ë“œ"""
        try:
            logger.info("ğŸ” LangGraph: ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ë¶„ì„ ì¤‘...")
            
            model = self._get_llm_model(state["model"])
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
            results_summary = []
            for result in state["raw_content"][:10]:  # ìƒìœ„ 10ê°œë§Œ ë¶„ì„
                results_summary.append({
                    "title": result.get("title", ""),
                    "snippet": result.get("snippet", "")[:200],  # 200ì ì œí•œ
                    "url": result.get("url", "")
                })
            
            analysis_prompt = ChatPromptTemplate.from_messages([
                ("system", """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì„¸ìš”.
ê° ê²°ê³¼ì— ëŒ€í•´ ê´€ë ¨ì„± ì ìˆ˜(0-10)ì™€ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."""),
                ("human", """ì§ˆë¬¸: {query}

ê²€ìƒ‰ ê²°ê³¼:
{results}

JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.""")
            ])
            
            response = await model.ainvoke(analysis_prompt.format_messages(
                query=state["original_query"],
                results=json.dumps(results_summary, ensure_ascii=False, indent=2)
            ))
            
            try:
                relevance_analysis = json.loads(response.content)
            except json.JSONDecodeError:
                relevance_analysis = {"analysis": "ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨"}
            
            return {
                "relevance_analysis": relevance_analysis,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "relevance_analysis_time": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ê´€ë ¨ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "errors": state.get("errors", []) + [f"ê´€ë ¨ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}"]
            }

    async def _synthesize_answer_node(self, state: WebSearchState) -> Dict[str, Any]:
        """ë‹µë³€ ì¢…í•© ë…¸ë“œ"""
        try:
            logger.info("ğŸ” LangGraph: ë‹µë³€ ì¢…í•© ì¤‘...")
            
            model = self._get_llm_model(state["model"])
            
            # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            key_info = []
            citations = []
            sources = []
            
            for i, result in enumerate(state["raw_content"][:8]):  # ìƒìœ„ 8ê°œ ê²°ê³¼
                key_info.append({
                    "content": result.get("snippet", ""),
                    "source": result.get("title", ""),
                    "url": result.get("url", ""),
                    "index": i + 1
                })
                
                citations.append({
                    "id": i + 1,
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "snippet": result.get("snippet", "")[:150]
                })
                
                sources.append({
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "domain": result.get("domain", ""),
                    "published_date": result.get("published_date")
                })
            
            synthesis_prompt = ChatPromptTemplate.from_messages([
                ("system", """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹µë³€ ì‘ì„± ì›ì¹™:
1. ì •í™•í•œ ì •ë³´ë§Œ í¬í•¨
2. ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì—¬ ì‹ ë¢°ì„± í™•ë³´  
3. êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
4. ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ëª…ì‹œ

ë‹µë³€ ëì—ëŠ” [ì¶œì²˜: 1, 2, 3] í˜•ì‹ìœ¼ë¡œ ì¸ìš© ë²ˆí˜¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”."""),
                ("human", """ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ì •ë³´:
{key_info}

ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.""")
            ])
            
            response = await model.ainvoke(synthesis_prompt.format_messages(
                query=state["original_query"],
                key_info=json.dumps(key_info, ensure_ascii=False, indent=2)
            ))
            
            synthesized_answer = response.content
            
            return {
                "synthesized_answer": synthesized_answer,
                "citations": citations,
                "sources": sources,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "synthesis_time": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ì¢…í•© ì‹¤íŒ¨: {e}")
            return {
                "errors": state.get("errors", []) + [f"ë‹µë³€ ì¢…í•© ì‹¤íŒ¨: {str(e)}"]
            }

    async def _generate_response_node(self, state: WebSearchState) -> Dict[str, Any]:
        """ìµœì¢… ì‘ë‹µ ìƒì„± ë…¸ë“œ"""
        try:
            logger.info("ğŸ” LangGraph: ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...")
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            start_time = state.get("execution_metadata", {}).get("plan_generation_time", time.time())
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            final_response = state.get("synthesized_answer", "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return {
                "final_response": final_response,
                "execution_metadata": {
                    **state.get("execution_metadata", {}),
                    "total_execution_time_ms": execution_time_ms,
                    "completion_time": time.time()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ìµœì¢… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "errors": state.get("errors", []) + [f"ìµœì¢… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"],
                "final_response": "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }

    def _should_continue(self, state: WebSearchState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜"""
        if state.get("should_fallback", False) or len(state.get("errors", [])) > 2:
            return "fallback"
        return "continue"

    def _get_llm_model(self, model_name: str):
        """LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if "claude" in model_name.lower():
            return ChatAnthropic(
                model_name=model_name,
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.3
            )
        elif "gemini" in model_name.lower():
            return ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=settings.GOOGLE_API_KEY,
                temperature=0.3
            )
        else:
            # ê¸°ë³¸ê°’: Claude
            return ChatAnthropic(
                model_name="claude-3-sonnet-20240229",
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.3
            )

    async def execute(self, input_data: AgentInput, model: str = "claude-sonnet", progress_callback=None) -> AgentOutput:
        """
        LangGraph ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì‹¤í–‰
        Feature Flagì— ë”°ë¼ LangGraph ë˜ëŠ” Legacy ëª¨ë“œë¡œ ì‹¤í–‰
        """
        start_time = time.time()
        
        # Feature Flag í™•ì¸
        if not is_langgraph_enabled(
            LangGraphFeatureFlags.LANGGRAPH_WEB_SEARCH, 
            input_data.user_id
        ):
            logger.info("ğŸ”„ Feature Flag: Legacy WebSearchAgent ì‚¬ìš©")
            return await self.legacy_agent.execute(input_data, model, progress_callback)
        
        logger.info(f"ğŸš€ LangGraph WebSearchAgent ì‹¤í–‰ ì‹œì‘ (ì‚¬ìš©ì: {input_data.user_id})")
        
        try:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            await langgraph_monitor.start_execution("langgraph_web_search")
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = WebSearchState(
                original_query=input_data.query,
                user_id=input_data.user_id,
                session_id=input_data.session_id,
                model=model,
                search_plan=None,
                search_queries=[],
                search_results=[],
                raw_content=[],
                relevance_analysis=None,
                synthesized_answer=None,
                final_response=None,
                execution_metadata={"start_time": start_time},
                citations=[],
                sources=[],
                errors=[],
                should_fallback=False
            )
            
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            if self.checkpointer:
                # ì²´í¬í¬ì¸í„° ì‚¬ìš© (ìƒíƒœ ì˜ì†ì„±)
                app = self.workflow.compile(checkpointer=self.checkpointer)
                config = {"configurable": {"thread_id": f"{input_data.user_id}_{input_data.session_id}"}}
                final_state = await app.ainvoke(initial_state, config=config)
            else:
                # ì²´í¬í¬ì¸í„° ì—†ì´ ì‹¤í–‰
                app = self.workflow.compile()
                final_state = await app.ainvoke(initial_state)
            
            # ê²°ê³¼ ì²˜ë¦¬
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # ì—ëŸ¬ê°€ ìˆê±°ë‚˜ fallbackì´ í•„ìš”í•œ ê²½ìš°
            if final_state.get("should_fallback", False) or len(final_state.get("errors", [])) > 0:
                logger.warning("ğŸ”„ LangGraph ì‹¤í–‰ ì‹¤íŒ¨ - Legacy ëª¨ë“œë¡œ fallback")
                langgraph_monitor.record_fallback("langgraph_web_search", f"Errors: {final_state.get('errors', [])}")
                return await self.legacy_agent.execute(input_data, model, progress_callback)
            
            # ì„±ê³µì ì¸ LangGraph ê²°ê³¼ ë°˜í™˜
            await langgraph_monitor.track_execution(
                agent_type="langgraph_web_search",
                execution_time=execution_time_ms / 1000,
                success=True,
                user_id=input_data.user_id
            )
            
            result = AgentOutput(
                result=final_state.get("final_response", "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
                metadata={
                    "agent_version": "langgraph",
                    "search_queries_count": len(final_state.get("search_queries", [])),
                    "results_count": len(final_state.get("raw_content", [])),
                    "langgraph_execution": True,
                    **final_state.get("execution_metadata", {})
                },
                execution_time_ms=execution_time_ms,
                agent_id=self.agent_id,
                model_used=model,
                timestamp=datetime.utcnow().isoformat(),
                citations=final_state.get("citations", []),
                sources=final_state.get("sources", [])
            )
            
            logger.info(f"âœ… LangGraph WebSearchAgent ì‹¤í–‰ ì™„ë£Œ ({execution_time_ms}ms)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ LangGraph WebSearchAgent ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # ì—ëŸ¬ ì‹œ ìë™ fallback
            await langgraph_monitor.track_execution(
                agent_type="langgraph_web_search",
                execution_time=(time.time() - start_time),
                success=False,
                error_message=str(e),
                user_id=input_data.user_id
            )
            
            langgraph_monitor.record_fallback("langgraph_web_search", f"Exception: {str(e)}")
            
            logger.info("ğŸ”„ ì˜ˆì™¸ ë°œìƒ - Legacy WebSearchAgentë¡œ fallback")
            return await self.legacy_agent.execute(input_data, model, progress_callback)

    def get_capabilities(self) -> List[str]:
        """ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½",
            "ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰",
            "LLM ê¸°ë°˜ ê´€ë ¨ì„± ë¶„ì„",
            "ì§€ëŠ¥í˜• ë‹µë³€ ì¢…í•©",
            "ìë™ fallback ì§€ì›",
            "ìƒíƒœ ì˜ì†ì„± (PostgreSQL)",
            "ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"
        ]

    def get_supported_models(self) -> List[str]:
        """ì§€ì› ëª¨ë¸ ëª©ë¡"""
        return [
            "claude-sonnet",
            "claude-haiku", 
            "claude-opus",
            "gemini-pro",
            "gemini-flash"
        ]


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
langgraph_web_search_agent = LangGraphWebSearchAgent()