"""
ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸
"""

import time
import asyncio
import httpx
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import quote_plus, urlparse
import json
import re
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass

from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.agents.llm_router import llm_router
from app.services.search_service import search_service
from app.services.web_crawler import web_crawler
from app.db.session import AsyncSessionLocal
from app.utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class SearchQuery:
    """ê²€ìƒ‰ì–´ ì •ë³´"""
    query: str
    priority: int  # 1: í•µì‹¬, 2: ë³´ì¡°, 3: ê´€ë ¨
    intent_type: str  # "ì •ë³´í˜•", "ì¶”ì²œí˜•", "ë¹„êµí˜•", "ë°©ë²•í˜•"
    language: str  # "ko", "en"
    max_results: int = 5
    search_type: str = "general"  # "general", "site_specific", "url_crawl"
    target_url: Optional[str] = None  # íŠ¹ì • ì‚¬ì´íŠ¸/URL ê²€ìƒ‰ìš©
    search_operators: List[str] = None  # Google search operators


@dataclass
class EnhancedSearchResult:
    """í–¥ìƒëœ ê²€ìƒ‰ ê²°ê³¼"""
    search_query: SearchQuery
    results: List[Dict[str, Any]]
    relevance_score: float
    success: bool


class WebSearchAgent(BaseAgent):
    """ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_id="web_search",
            name="ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸",
            description="ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤"
        )
    
    def _extract_url_info(self, user_query: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ URL ì •ë³´ ì¶”ì¶œ ë° ë¶„ì„"""
        url_info = {
            "has_url": False,
            "urls": [],
            "domains": [],
            "search_type": "general",
            "site_hints": []
        }
        
        # URL íŒ¨í„´ ë§¤ì¹­ (http, https, www í¬í•¨)
        url_patterns = [
            r'https?://[^\s]+',  # http://... ë˜ëŠ” https://...
            r'www\.[^\s]+',      # www.example.com
            r'[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}(?:/[^\s]*)?'  # domain.com í˜•íƒœ
        ]
        
        for pattern in url_patterns:
            matches = re.findall(pattern, user_query, re.IGNORECASE)
            for match in matches:
                # ê¸°ë³¸ ì²˜ë¦¬
                clean_url = match.strip('.,!?')
                if not clean_url.startswith(('http://', 'https://')):
                    if clean_url.startswith('www.'):
                        clean_url = 'https://' + clean_url
                    else:
                        clean_url = 'https://' + clean_url
                
                try:
                    parsed = urlparse(clean_url)
                    if parsed.netloc:
                        url_info["urls"].append(clean_url)
                        url_info["domains"].append(parsed.netloc)
                        url_info["has_url"] = True
                except:
                    continue
        
        # ì‚¬ì´íŠ¸ ì´ë¦„ íŒíŠ¸ ê°ì§€ (í•œêµ­ì–´ + ì˜ì–´)
        site_hints = {
            # ì£¼ìš” í•œêµ­ ì‚¬ì´íŠ¸
            "ë„¤ì´ë²„": "naver.com",
            "ë‹¤ìŒ": "daum.net", 
            "êµ¬ê¸€": "google.com",
            "ìœ íŠœë¸Œ": "youtube.com",
            "ê¹ƒí—ˆë¸Œ": "github.com",
            "ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°": "stackoverflow.com",
            "ìœ„í‚¤í”¼ë””ì•„": "wikipedia.org",
            "ì•„ë§ˆì¡´": "amazon.com",
            "í˜ì´ìŠ¤ë¶": "facebook.com",
            "íŠ¸ìœ„í„°": "twitter.com",
            "ë§í¬ë“œì¸": "linkedin.com",
            "ë ˆë”§": "reddit.com",
            "ì¸ìŠ¤íƒ€ê·¸ë¨": "instagram.com",
            
            # ì˜ì–´ ì‚¬ì´íŠ¸ëª…
            "naver": "naver.com",
            "google": "google.com", 
            "youtube": "youtube.com",
            "github": "github.com",
            "stackoverflow": "stackoverflow.com",
            "wikipedia": "wikipedia.org",
            "reddit": "reddit.com",
            "medium": "medium.com",
            "aws": "aws.amazon.com",
            "microsoft": "microsoft.com",
            "openai": "openai.com",
            "anthropic": "anthropic.com"
        }
        
        for hint, domain in site_hints.items():
            if hint in user_query.lower():
                url_info["site_hints"].append(domain)
                url_info["has_url"] = True
        
        # ê²€ìƒ‰ íƒ€ì… ê²°ì •
        if url_info["urls"] or url_info["site_hints"]:
            # íŠ¹ì • URLì´ ìˆìœ¼ë©´ ì‚¬ì´íŠ¸ë³„ ê²€ìƒ‰
            url_info["search_type"] = "site_specific"
            
            # ì™„ì „í•œ URL(ê²½ë¡œ í¬í•¨)ì´ ìˆìœ¼ë©´ í¬ë¡¤ë§ ê²€ìƒ‰ë„ ê³ ë ¤
            for url in url_info["urls"]:
                parsed = urlparse(url)
                if parsed.path and parsed.path != '/':
                    url_info["search_type"] = "url_crawl"
                    break
        
        # ê²€ìƒ‰ ëª…ë ¹ì–´ ê°ì§€
        search_commands = [
            "ì—ì„œ ê²€ìƒ‰", "ì—ì„œ ì°¾ì•„", "ì‚¬ì´íŠ¸ì—ì„œ", "í™ˆí˜ì´ì§€ì—ì„œ", 
            "ì—ì„œ ì°¾ì•„ì¤˜", "ì—ì„œ ê²€ìƒ‰í•´ì¤˜", "ì—ì„œ ì•Œì•„ë´",
            "site:", "inurl:", "intitle:"
        ]
        
        for command in search_commands:
            if command in user_query.lower():
                if url_info["search_type"] == "general":
                    url_info["search_type"] = "site_specific"
                break
        
        return url_info
    
    async def execute(self, input_data: AgentInput, model: str = "gemini", progress_callback=None) -> AgentOutput:
        """ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ê¸°ë°˜ ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹¤í–‰"""
        start_time = time.time()
        
        # ì›ë³¸ ì¿¼ë¦¬ ë° ëŒ€í™” ë§¥ë½ ì •ë³´ ì €ì¥
        original_query = input_data.query
        conversation_context = input_data.conversation_context
        
        if not self.validate_input(input_data):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ë°ì´í„°")
        
        async with AsyncSessionLocal() as session:
            try:
                # 0ë‹¨ê³„: URL ì •ë³´ ë¶„ì„ (5%)
                if progress_callback:
                    progress_callback("ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ì¤‘...", 5, {
                        "step_id": "query_analysis",
                        "step_name": "ê²€ìƒ‰ì–´ ë¶„ì„",
                        "description": "ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"
                    })
                url_info = self._extract_url_info(input_data.query)
                
                # 1ë‹¨ê³„: ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ìƒì„± (15%)
                if progress_callback:
                    search_type_msg = {
                        "general": "ì¼ë°˜ ê²€ìƒ‰ì–´ ë¶„ì„ ë° ìƒì„± ì¤‘...",
                        "site_specific": "ì‚¬ì´íŠ¸ë³„ ê²€ìƒ‰ì–´ ë¶„ì„ ë° ìƒì„± ì¤‘...",
                        "url_crawl": "URL í¬ë¡¤ë§ ê²€ìƒ‰ì–´ ë¶„ì„ ë° ìƒì„± ì¤‘..."
                    }
                    progress_callback(search_type_msg.get(url_info["search_type"], "ê²€ìƒ‰ì–´ ë¶„ì„ ë° ìƒì„± ì¤‘..."), 15, {
                        "step_id": "query_generation",
                        "step_name": "ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±",
                        "description": "ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"
                    })
                search_queries = await self._generate_multiple_search_queries(input_data.query, model, url_info, input_data.conversation_context)
                
                # 2ë‹¨ê³„: ë³‘ë ¬ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ (60%)
                if progress_callback:
                    progress_callback(f"ë‹¤ì¤‘ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘... ({len(search_queries)}ê°œ ê²€ìƒ‰ì–´)", 60, {
                        "step_id": "parallel_search",
                        "step_name": "ë³‘ë ¬ ì›¹ ê²€ìƒ‰",
                        "description": "ì—¬ëŸ¬ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ë™ì‹œì— ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤",
                        "search_queries": [q.query for q in search_queries]
                    })
                all_search_results = await self._execute_parallel_searches(search_queries, session, progress_callback, conversation_context, original_query)
                
                # 3ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±° (75%)
                if progress_callback:
                    progress_callback("ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° í•„í„°ë§ ì¤‘...", 75, {
                        "step_id": "result_filtering",
                        "step_name": "ê²°ê³¼ í•„í„°ë§",
                        "description": "ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤"
                    })
                integrated_results = await self._integrate_and_deduplicate_results(all_search_results, input_data.query)
                
                # 4ë‹¨ê³„: ì§€ëŠ¥í˜• ë­í‚¹ ì ìš© (85%)
                if progress_callback:
                    progress_callback("ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° ë­í‚¹ ì¤‘...", 85, {
                        "step_id": "result_ranking",
                        "step_name": "ê²°ê³¼ ìˆœìœ„í™”",
                        "description": "ê´€ë ¨ì„±ê³¼ ì‹ ë¢°ë„ì— ë”°ë¼ ê²°ê³¼ë¥¼ ìˆœìœ„í™”í•©ë‹ˆë‹¤"
                    })
                ranked_results = await self._apply_intelligent_ranking(integrated_results, input_data.query, model)
                
                # 5ë‹¨ê³„: LLM ê¸°ë°˜ í†µí•© ë‹µë³€ ìƒì„± (95%)
                if progress_callback:
                    progress_callback("AI ë¶„ì„ ë° í†µí•© ë‹µë³€ ìƒì„± ì¤‘...", 95, {
                        "step_id": "response_generation",
                        "step_name": "AI ë‹µë³€ ìƒì„±",
                        "description": "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤"
                    })
                enhanced_summary = await self._generate_enhanced_response(
                    original_query=input_data.query,
                    search_queries=search_queries,
                    search_results=ranked_results,
                    model=model
                )
                
                execution_time = int((time.time() - start_time) * 1000)
                
                # ìµœì¢… ê²°ê³¼ë¥¼ citationsì™€ sourcesë¡œ ë³€í™˜
                citations, sources = self._convert_to_citations_and_sources(ranked_results[:8])
                
                metadata = {
                    "search_queries": [q.query for q in search_queries],
                    "search_queries_count": len(search_queries),
                    "total_results_found": sum(len(r.results) for r in all_search_results if r.success),
                    "final_results_count": len(ranked_results),
                    "search_method": "multi_query_intelligent_search",
                    "query_types": list(set(q.intent_type for q in search_queries)),
                    "languages_used": list(set(q.language for q in search_queries)),
                    "search_types": list(set(q.search_type for q in search_queries)),
                    "target_sites": list(set(filter(None, [q.target_url for q in search_queries]))),
                    "used_operators": list(set(filter(None, [op for q in search_queries if q.search_operators for op in q.search_operators]))),
                    "url_analysis": url_info,
                    "top_sources": [r.get('title', '')[:50] for r in ranked_results[:3]],
                    # ë§¥ë½ í†µí•© ê²€ìƒ‰ì–´ ì •ë³´ ì¶”ê°€
                    "original_query": original_query,
                    "context_integrated_queries": conversation_context.optimal_search_queries if conversation_context else [],
                    "has_conversation_context": bool(conversation_context and conversation_context.optimal_search_queries)
                }
                
                return AgentOutput(
                    result=enhanced_summary,
                    metadata=metadata,
                    execution_time_ms=execution_time,
                    agent_id=self.agent_id,
                    model_used=model,
                    timestamp=datetime.now().isoformat(),
                    citations=citations,
                    sources=sources
                )
                
            except Exception as e:
                self.logger.error(f"ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                execution_time = int((time.time() - start_time) * 1000)
                
                return AgentOutput(
                    result=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    metadata={"error": True, "error_message": str(e)},
                    execution_time_ms=execution_time,
                    agent_id=self.agent_id,
                    model_used=model,
                    timestamp=datetime.now().isoformat(),
                    error=str(e)
                )
    
    async def _generate_multiple_search_queries(self, user_query: str, model: str, url_info: Dict[str, Any], conversation_context=None) -> List[SearchQuery]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ìƒì„± (URL ì •ë³´ í¬í•¨)"""
        try:
            # URL ì •ë³´ë¥¼ í™œìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            url_context = ""
            search_type = url_info.get("search_type", "general")
            
            if search_type == "site_specific":
                # íŠ¹ì • ì‚¬ì´íŠ¸ ê²€ìƒ‰
                target_sites = url_info.get("domains", []) + url_info.get("site_hints", [])
                if target_sites:
                    url_context = f"""
**íŠ¹ë³„ ì§€ì‹œì‚¬í•­**: ì‚¬ìš©ìê°€ íŠ¹ì • ì‚¬ì´íŠ¸ ê²€ìƒ‰ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
- ëŒ€ìƒ ì‚¬ì´íŠ¸: {', '.join(target_sites)}
- ê²€ìƒ‰ì–´ì— "site:{target_sites[0]}" í˜•íƒœì˜ Google ê²€ìƒ‰ ì—°ì‚°ìë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
- í•´ë‹¹ ì‚¬ì´íŠ¸ì— íŠ¹í™”ëœ ê²€ìƒ‰ì–´ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
            elif search_type == "url_crawl":
                # íŠ¹ì • URL í¬ë¡¤ë§
                target_urls = url_info.get("urls", [])
                if target_urls:
                    url_context = f"""
**íŠ¹ë³„ ì§€ì‹œì‚¬í•­**: ì‚¬ìš©ìê°€ íŠ¹ì • URLì—ì„œ ì •ë³´ë¥¼ ì°¾ê³ ì í•©ë‹ˆë‹¤.
- ëŒ€ìƒ URL: {', '.join(target_urls)}
- í•´ë‹¹ URLì˜ ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ì˜ˆì •ì…ë‹ˆë‹¤.
- URL í¬ë¡¤ë§ê³¼ ë³‘í–‰í•  ë³´ì¡° ê²€ìƒ‰ì–´ë„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
            
            # ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ê°ì§€
            location_context = ""
            weather_keywords = ["ë‚ ì”¨", "ê¸°ì˜¨", "ê°•ìˆ˜", "ìŠµë„", "ë¯¸ì„¸ë¨¼ì§€", "weather", "temperature", "rain"]
            location_keywords = ["ì˜¤ëŠ˜", "í˜„ì¬", "ì§€ê¸ˆ", "ì—¬ê¸°", "ë‚´ ìœ„ì¹˜", "ê·¼ì²˜"]
            
            has_weather_query = any(keyword in user_query.lower() for keyword in weather_keywords)
            has_location_query = any(keyword in user_query.lower() for keyword in location_keywords)
            
            if has_weather_query and has_location_query:
                from datetime import datetime
                today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
                location_context = f"""
**ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ íŠ¹ë³„ ì§€ì‹œì‚¬í•­**:
- ì‚¬ìš©ìê°€ í˜„ì¬ ìœ„ì¹˜ë‚˜ "ì˜¤ëŠ˜" ê´€ë ¨ ë‚ ì”¨ ì •ë³´ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
- ì˜¤ëŠ˜ ë‚ ì§œ: {today_date}
- í•œêµ­ ì£¼ìš” ë„ì‹œ(ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬, ì¸ì²œ, ê´‘ì£¼, ëŒ€ì „, ìš¸ì‚°) ê¸°ì¤€ ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
- "site:weather.go.kr" ë˜ëŠ” "site:kma.go.kr" ê¸°ìƒì²­ ì‚¬ì´íŠ¸ ê²€ìƒ‰ì„ ìš°ì„  í¬í•¨í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ ë‚ ì§œì™€ ì§€ì—­ëª…ì„ í¬í•¨í•œ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
- ì˜ˆ: "{today_date} ì„œìš¸ ë‚ ì”¨", "site:weather.go.kr ì˜¤ëŠ˜ ë‚ ì”¨"
"""
            
            # ë²”ìš© ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            conversation_context_prompt = ""
            logger.debug("ëŒ€í™” ë§¥ë½ ìˆ˜ì‹  ì—¬ë¶€", {"has_context": conversation_context is not None})
            if conversation_context:
                logger.info(f"ğŸ¯ ëŒ€í™” ë§¥ë½ ì •ë³´ - ë„ë©”ì¸: {conversation_context.domain}, ìµœì  ê²€ìƒ‰ì–´: {conversation_context.optimal_search_queries}")
                
                # LLMì´ ì´ë¯¸ ìƒì„±í•œ ìµœì  ê²€ìƒ‰ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
                if conversation_context.optimal_search_queries:
                    # LLMì´ ìƒì„±í•œ ìµœì  ê²€ìƒ‰ì–´ ìš°ì„  ì‚¬ìš©
                    logger.info(f"ğŸ¯ LLM ìµœì  ê²€ìƒ‰ì–´ ì‚¬ìš©: {conversation_context.optimal_search_queries}")
                    conversation_context_prompt = f"""
**ğŸ¯ LLM ë¶„ì„ ê¸°ë°˜ ìµœì  ê²€ìƒ‰ì–´ í™œìš©**:
LLMì´ ì „ì²´ ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ìƒì„±í•œ ìµœì  ê²€ìƒ‰ì–´ë“¤:
{chr(10).join([f'- {query}' for query in conversation_context.optimal_search_queries])}

**ëŒ€í™” ë§¥ë½ ì •ë³´**:
- ë„ë©”ì¸: {conversation_context.domain}
- ì£¼ì œ ì§„í™”: {' â†’ '.join(conversation_context.topic_evolution)}
- ì‚¬ìš©ì ì˜ë„: {conversation_context.user_intent}
- ë§¥ë½ ì—°ê²°: {conversation_context.context_connection}
- ê²€ìƒ‰ í¬ì»¤ìŠ¤: {conversation_context.search_focus}

**ìµœì í™”ëœ ê²€ìƒ‰ ì „ëµ**:
1. LLMì´ ì œì•ˆí•œ ìµœì  ê²€ìƒ‰ì–´ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ì–´ ìƒì„±
2. ì „ì²´ ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•œ êµ¬ì²´ì ì´ê³  ì˜ë¯¸ìˆëŠ” ê²€ìƒ‰ì–´ êµ¬ì„±
3. ì‚¬ìš©ìì˜ ì§„ì§œ ê²€ìƒ‰ ì˜ë„ì— ë¶€í•©í•˜ëŠ” ë‹¤ê°ë„ ê²€ìƒ‰ì–´ ìƒì„±
"""
                else:
                    # LLM ê²€ìƒ‰ì–´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë§¥ë½ ì •ë³´ í™œìš©
                    conversation_context_prompt = f"""
**ë²”ìš© ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ ì§€ì‹œì‚¬í•­**:
- ë„ë©”ì¸ ë¶„ë¥˜: {conversation_context.domain}
- ëŒ€í™” ì£¼ì œ: {', '.join(conversation_context.conversation_topics)}
- ì£¼ì œ ì§„í™”: {' â†’ '.join(conversation_context.topic_evolution) if conversation_context.topic_evolution else 'ë‹¨ì¼ ì£¼ì œ'}
- í•µì‹¬ ì—”í‹°í‹°: {', '.join(conversation_context.mentioned_entities)}
- ì‚¬ìš©ì ì˜ë„: {conversation_context.user_intent}
- ë§¥ë½ ì—°ê²°: {conversation_context.context_connection}
- ê²€ìƒ‰ í¬ì»¤ìŠ¤: {conversation_context.search_focus}
- ì´ì „ ê²€ìƒ‰ì–´: {', '.join(conversation_context.previous_search_queries[-3:])}

**ë²”ìš© ë§¥ë½ í™œìš© ê·œì¹™**:
1. ë„ë©”ì¸ì— ë§ëŠ” ì „ë¬¸ ìš©ì–´ì™€ í‚¤ì›Œë“œ í™œìš© ({conversation_context.domain} ë¶„ì•¼)
2. ì£¼ì œ ì§„í™” ê³¼ì •ì„ ë°˜ì˜í•œ í†µí•©ì  ê²€ìƒ‰ì–´ ìƒì„±
3. ì‚¬ìš©ì ì˜ë„({conversation_context.user_intent})ì— ë§ëŠ” ê²€ìƒ‰ ë°©í–¥ì„± ì„¤ì •
4. ë§¥ë½ ì—°ê²°ì„±ì„ ê³ ë ¤í•œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ê²€ìƒ‰ì–´ êµ¬ì„±
5. ì¤‘ë³µ ë°©ì§€ ë° ìƒˆë¡œìš´ ê´€ì ì˜ ì •ë³´ íƒìƒ‰
"""

            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ ë‹¤ì¤‘ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}"
ê²€ìƒ‰ íƒ€ì…: {search_type}
{url_context}
{location_context}
{conversation_context_prompt}

ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ 3-5ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”:

1. **í•µì‹¬ ê²€ìƒ‰ì–´** (ìš°ì„ ìˆœìœ„ 1): ì •í™•í•œ ë§¤ì¹­ì„ ìœ„í•œ ê°€ì¥ ì¤‘ìš”í•œ ê²€ìƒ‰ì–´
2. **ì§€ì—­ íŠ¹í™” ê²€ìƒ‰ì–´** (ìš°ì„ ìˆœìœ„ 1): ìœ„ì¹˜/ë‚ ì”¨ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ì§€ì—­ëª… í¬í•¨
3. **ë³´ì¡° ê²€ìƒ‰ì–´** (ìš°ì„ ìˆœìœ„ 2): ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ í™•ì¥ ê²€ìƒ‰ì–´
4. **ì˜ì–´ ê²€ìƒ‰ì–´** (ìš°ì„ ìˆœìœ„ 2): ì˜ì–´ë¡œ ë²ˆì—­í•œ ê²€ìƒ‰ì–´ (í•„ìš”ì‹œ)
5. **êµ¬ì²´ì  ê²€ìƒ‰ì–´** (ìš°ì„ ìˆœìœ„ 1): ë” êµ¬ì²´ì ì´ê³  ì„¸ë¶€ì ì¸ ê²€ìƒ‰ì–´
6. **ê´€ë ¨ ê²€ìƒ‰ì–´** (ìš°ì„ ìˆœìœ„ 3): ì—°ê´€ëœ ì£¼ì œì˜ ê²€ìƒ‰ì–´

ê° ê²€ìƒ‰ì–´ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•œ JSON í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "search_queries": [
    {{
      "query": "ê²€ìƒ‰ì–´",
      "priority": 1,  // 1: í•µì‹¬, 2: ë³´ì¡°, 3: ê´€ë ¨
      "intent_type": "ì •ë³´í˜•",  // "ì •ë³´í˜•", "ì¶”ì²œí˜•", "ë¹„êµí˜•", "ë°©ë²•í˜•"
      "language": "ko",  // "ko" ë˜ëŠ” "en"
      "max_results": 5,
      "search_type": "{search_type}",  // "general", "site_specific", "url_crawl"
      "target_url": null  // íŠ¹ì • URLì´ ìˆëŠ” ê²½ìš°
    }}
  ]
}}

JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            
            # JSON íŒŒì‹± ì‹œë„ (```json ì œê±°)
            try:
                import json
                # ```jsonìœ¼ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì œê±°
                clean_response = response.strip()
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]  # ```json ì œê±°
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]  # ``` ì œê±°
                clean_response = clean_response.strip()
                
                data = json.loads(clean_response)
                search_queries = []
                
                for query_data in data.get("search_queries", []):
                    # URL ì •ë³´ ì²˜ë¦¬
                    target_url = None
                    search_operators = []
                    query_search_type = query_data.get("search_type", search_type)
                    
                    # site: ì—°ì‚°ìê°€ ìˆëŠ” ê²€ìƒ‰ì–´ ì²˜ë¦¬
                    query_text = query_data["query"]
                    if "site:" in query_text:
                        search_operators.append("site")
                        query_search_type = "site_specific"
                    
                    # íŠ¹ì • URLì´ ì§€ì •ëœ ê²½ìš°
                    if url_info.get("urls"):
                        target_url = url_info["urls"][0]
                    elif url_info.get("site_hints"):
                        target_url = f"https://{url_info['site_hints'][0]}"
                    
                    search_query = SearchQuery(
                        query=query_text,
                        priority=query_data.get("priority", 2),
                        intent_type=query_data.get("intent_type", "ì •ë³´í˜•"),
                        language=query_data.get("language", "ko"),
                        max_results=query_data.get("max_results", 5),
                        search_type=query_search_type,
                        target_url=target_url,
                        search_operators=search_operators if search_operators else None
                    )
                    search_queries.append(search_query)
                
                # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬
                search_queries.sort(key=lambda x: x.priority)
                
                self.logger.info(f"ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ìƒì„± ì™„ë£Œ: {len(search_queries)}ê°œ")
                for sq in search_queries:
                    self.logger.info(f"  - {sq.query} (ìš°ì„ ìˆœìœ„: {sq.priority}, íƒ€ì…: {sq.intent_type})")
                
                return search_queries[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
                
            except json.JSONDecodeError as je:
                self.logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {je}, ì›ë³¸ ì‘ë‹µ: {response[:100]}")
                raise
                
        except Exception as e:
            self.logger.warning(f"ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ì–´ ìƒì„±: {e}")
            # fallback: ê¸°ë³¸ ê²€ìƒ‰ì–´ ìƒì„± (URL ì •ë³´ í¬í•¨)
            fallback_queries = []
            
            search_type = url_info.get("search_type", "general")
            target_url = None
            
            if url_info.get("urls"):
                target_url = url_info["urls"][0]
            elif url_info.get("site_hints"):
                target_url = f"https://{url_info['site_hints'][0]}"
            
            # ê¸°ë³¸ ê²€ìƒ‰ì–´
            base_query = user_query.strip()
            if search_type == "site_specific" and url_info.get("site_hints"):
                base_query = f"site:{url_info['site_hints'][0]} {base_query}"
            
            fallback_queries.append(SearchQuery(
                query=base_query,
                priority=1,
                intent_type="ì •ë³´í˜•",
                language="ko",
                max_results=5,
                search_type=search_type,
                target_url=target_url
            ))
            
            # ë³´ì¡° ê²€ìƒ‰ì–´ (ìµœì‹  ì •ë³´)
            aux_query = f"{user_query.strip()} 2024"
            if search_type == "site_specific" and url_info.get("site_hints"):
                aux_query = f"site:{url_info['site_hints'][0]} {aux_query}"
                
            fallback_queries.append(SearchQuery(
                query=aux_query,
                priority=2,
                intent_type="ì •ë³´í˜•",
                language="ko",
                max_results=3,
                search_type=search_type,
                target_url=target_url
            ))
            
            return fallback_queries
    
    async def _execute_parallel_searches(
        self, 
        search_queries: List[SearchQuery], 
        session: AsyncSession,
        progress_callback=None,
        conversation_context=None,
        original_query: str = None
    ) -> List[EnhancedSearchResult]:
        """ë³‘ë ¬ë¡œ ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ì‹¤í–‰"""
        search_tasks = []
        
        # ê° ê²€ìƒ‰ì–´ì— ëŒ€í•´ ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        for i, query in enumerate(search_queries):
            task = self._execute_single_search(query, session, i, len(search_queries), progress_callback, conversation_context, original_query)
            search_tasks.append(task)
        
        # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
        enhanced_results = []
        for result in results:
            if isinstance(result, EnhancedSearchResult):
                enhanced_results.append(result)
            elif isinstance(result, Exception):
                self.logger.warning(f"ê²€ìƒ‰ íƒœìŠ¤í¬ ì‹¤íŒ¨: {result}")
        
        self.logger.info(f"ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {len(enhanced_results)}/{len(search_queries)} ì„±ê³µ")
        return enhanced_results
    
    async def _execute_single_search(
        self,
        search_query: SearchQuery,
        session: AsyncSession,
        task_index: int,
        total_tasks: int,
        progress_callback=None,
        conversation_context=None,
        original_query: str = None
    ) -> EnhancedSearchResult:
        """ë‹¨ì¼ ê²€ìƒ‰ì–´ ì‹¤í–‰ (ì¼ë°˜ ê²€ìƒ‰ + URL í¬ë¡¤ë§ ì§€ì›)"""
        # ê° ê²€ìƒ‰ íƒœìŠ¤í¬ë§ˆë‹¤ ë…ë¦½ì ì¸ ì„¸ì…˜ ì‚¬ìš© (ë™ì‹œì„± ë¬¸ì œ í•´ê²°)
        async with AsyncSessionLocal() as independent_session:
            try:
                # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                if progress_callback:
                    base_progress = 40 + (task_index / total_tasks) * 20  # 40-60% ë²”ìœ„
                    if search_query.search_type == "url_crawl":
                        progress_callback(f"'{search_query.target_url}' í¬ë¡¤ë§ ì¤‘...", base_progress)
                    else:
                        # ë§¥ë½ í†µí•© ê²€ìƒ‰ì–´ í‘œì‹œ
                        display_query = search_query.query
                        has_context = False
                        matching_optimal = None
                        
                        if conversation_context and conversation_context.optimal_search_queries:
                            # ë§¥ë½ í†µí•© ê²€ìƒ‰ì–´ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ í‘œì‹œ
                            matching_optimal = next((q for q in conversation_context.optimal_search_queries if q in search_query.query), None)
                            if matching_optimal:
                                display_query = matching_optimal
                                has_context = True
                        
                        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                        metadata = {
                            'step_id': 'parallel_search',
                            'step_name': 'ë³‘ë ¬ ì›¹ ê²€ìƒ‰',
                            'description': f"'{display_query}' ê²€ìƒ‰ì„ ìˆ˜í–‰ì¤‘ì…ë‹ˆë‹¤",
                            'has_context': has_context,
                            'original_query': original_query if has_context else None,
                            'context_integrated_query': matching_optimal if has_context else None,
                            'current_search_query': display_query
                        }
                        
                        progress_callback(f"'{display_query}' ê²€ìƒ‰ ì¤‘...", base_progress, metadata)
                
                results_dict = []
                
                # URL í¬ë¡¤ë§ ì‹¤í–‰ (url_crawl íƒ€ì…ì¸ ê²½ìš°)
                if search_query.search_type == "url_crawl" and search_query.target_url:
                    crawl_result = await web_crawler.crawl_url(search_query.target_url)
                    
                    if not crawl_result.error:
                        # í¬ë¡¤ë§ëœ ì½˜í…ì¸ ì—ì„œ ê²€ìƒ‰
                        search_result = await web_crawler.search_in_content(crawl_result, search_query.query)
                        
                        if search_result["found"]:
                            # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ê²€ìƒ‰ ê²°ê³¼ í˜•íƒœë¡œ ë³€í™˜
                            snippet = ""
                            if search_result["matches"]:
                                snippet = search_result["matches"][0]["context"]
                            elif crawl_result.summary:
                                snippet = crawl_result.summary
                            else:
                                snippet = crawl_result.content[:300] + "..."
                            
                            result_dict = {
                                "title": crawl_result.title or "í¬ë¡¤ë§ëœ í˜ì´ì§€",
                                "url": crawl_result.url,
                                "snippet": snippet,
                                "source": f"crawled_{urlparse(crawl_result.url).netloc}",
                                "score": 0.95,  # ì§ì ‘ í¬ë¡¤ë§ëœ ê²°ê³¼ëŠ” ë†’ì€ ì ìˆ˜
                                "timestamp": crawl_result.timestamp,
                                "crawl_data": {
                                    "headings": crawl_result.headings,
                                    "matches_count": search_result["matches_count"],
                                    "title_match": search_result["title_match"],
                                    "heading_matches": search_result["heading_matches"]
                                }
                            }
                            results_dict.append(result_dict)
                        else:
                            # ê²€ìƒ‰ì–´ê°€ ì—†ì–´ë„ í˜ì´ì§€ ì •ë³´ëŠ” ì œê³µ
                            result_dict = {
                                "title": crawl_result.title or "í¬ë¡¤ë§ëœ í˜ì´ì§€",
                                "url": crawl_result.url,
                                "snippet": crawl_result.summary or crawl_result.content[:300] + "...",
                                "source": f"crawled_{urlparse(crawl_result.url).netloc}",
                                "score": 0.7,  # ê´€ë ¨ì„±ì€ ë‚®ì§€ë§Œ ìœ ìš©í•œ ì •ë³´
                                "timestamp": crawl_result.timestamp,
                                "crawl_data": {
                                    "headings": crawl_result.headings,
                                    "no_matches": True
                                }
                            }
                            results_dict.append(result_dict)
                    else:
                        # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì •ë³´ í¬í•¨
                        self.logger.warning(f"URL í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_result.error}")
                
                # ì¼ë°˜ ì›¹ ê²€ìƒ‰ë„ í•¨ê»˜ ì‹¤í–‰ (ë³´ì™„ì  ì •ë³´ ì œê³µ)
                # SearchQueryì˜ search_typeì„ SearchServiceì˜ SearchTypeìœ¼ë¡œ ë§¤í•‘
                from app.services.search_service import SearchType
                
                service_search_type = SearchType.WEB  # ê¸°ë³¸ê°’
                if search_query.search_type == "general":
                    service_search_type = SearchType.WEB
                elif search_query.search_type == "site_specific":
                    service_search_type = SearchType.WEB  # ì‚¬ì´íŠ¸ë³„ ê²€ìƒ‰ë„ ì¼ë°˜ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬
                elif search_query.search_type == "url_crawl":
                    service_search_type = SearchType.WEB
                
                # ê²€ìƒ‰ì–´ì—ì„œ ë„ë©”ì¸ì´ë‚˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ SearchType ì„¤ì •
                query_lower = search_query.query.lower()
                if any(word in query_lower for word in ["ë‰´ìŠ¤", "news", "ìµœì‹  ì†Œì‹", "ìµœê·¼ ì†Œì‹"]):
                    service_search_type = SearchType.NEWS
                elif any(word in query_lower for word in ["ë…¼ë¬¸", "ì—°êµ¬", "í•™ìˆ ", "academic", "scholar"]):
                    service_search_type = SearchType.ACADEMIC
                elif any(word in query_lower for word in ["github", "stackoverflow", "ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "ì½”ë”©"]):
                    service_search_type = SearchType.TECHNICAL
                elif any(word in query_lower for word in ["ì •ë¶€", "ê³µì‹", "gov.kr", "go.kr", "government"]):
                    service_search_type = SearchType.GOVERNMENT
                elif any(word in query_lower for word in ["ì‡¼í•‘", "êµ¬ë§¤", "ê°€ê²©", "shopping", "buy"]):
                    service_search_type = SearchType.SHOPPING
                
                search_results = await search_service.search_web(
                    query=search_query.query,
                    max_results=search_query.max_results,
                    use_cache=True,
                    session=independent_session,
                    search_type=service_search_type
                )
                
                # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
                for result in search_results:
                    result_dict = {
                        "title": result.title,
                        "url": result.url,
                        "snippet": result.snippet,
                        "source": result.source,
                        "score": result.score,
                        "timestamp": getattr(result, 'timestamp', None)
                    }
                    results_dict.append(result_dict)
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = self._calculate_relevance_score(search_query, results_dict)
                
                return EnhancedSearchResult(
                    search_query=search_query,
                    results=results_dict,
                    relevance_score=relevance_score,
                    success=len(results_dict) > 0
                )
                
            except Exception as e:
                self.logger.error(f"ë‹¨ì¼ ê²€ìƒ‰ ì‹¤íŒ¨ [{search_query.query}]: {e}")
                return EnhancedSearchResult(
                    search_query=search_query,
                    results=[],
                    relevance_score=0.0,
                    success=False
                )
    
    def _calculate_relevance_score(self, search_query: SearchQuery, results: List[Dict]) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not results:
            return 0.0
        
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 0.5
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        priority_weight = {1: 1.0, 2: 0.8, 3: 0.6}.get(search_query.priority, 0.5)
        
        # ê²°ê³¼ ê°œìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        result_count_bonus = min(len(results) / 5.0, 1.0) * 0.3
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = sum(r.get("score", 0.5) for r in results) / len(results)
        
        final_score = (base_score + result_count_bonus + avg_score) * priority_weight
        return min(final_score, 1.0)
    
    async def _integrate_and_deduplicate_results(
        self,
        all_results: List[EnhancedSearchResult],
        original_query: str
    ) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°"""
        all_unique_results = {}
        
        for enhanced_result in all_results:
            if not enhanced_result.success:
                continue
                
            for result in enhanced_result.results:
                url = result.get("url", "")
                title = result.get("title", "")
                
                # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
                if url and url not in all_unique_results:
                    # ê²€ìƒ‰ì–´ ì •ë³´ ì¶”ê°€
                    result_with_context = result.copy()
                    result_with_context["search_query"] = enhanced_result.search_query.query
                    result_with_context["query_priority"] = enhanced_result.search_query.priority
                    result_with_context["query_type"] = enhanced_result.search_query.intent_type
                    result_with_context["relevance_score"] = enhanced_result.relevance_score
                    
                    all_unique_results[url] = result_with_context
                elif url in all_unique_results:
                    # ì´ë¯¸ ìˆëŠ” ê²°ê³¼ì˜ ì ìˆ˜ í–¥ìƒ (ë‹¤ì¤‘ ê²€ìƒ‰ì–´ì—ì„œ ë°œê²¬ëœ ê²½ìš°)
                    existing = all_unique_results[url]
                    existing["score"] = max(existing["score"], result.get("score", 0.5))
                    existing["relevance_score"] = max(existing["relevance_score"], enhanced_result.relevance_score)
        
        # ë„ë©”ì¸ë³„ ê²°ê³¼ ìˆ˜ ì œí•œ (ë‹¤ì–‘ì„± ë³´ì¥)
        domain_counts = {}
        filtered_results = []
        
        for result in all_unique_results.values():
            url = result.get("url", "")
            if url:
                from urllib.parse import urlparse
                domain = urlparse(url).netloc
                
                if domain_counts.get(domain, 0) < 3:  # ë„ë©”ì¸ë‹¹ ìµœëŒ€ 3ê°œ
                    filtered_results.append(result)
                    domain_counts[domain] = domain_counts.get(domain, 0) + 1
        
        self.logger.info(f"ê²°ê³¼ í†µí•© ì™„ë£Œ: {len(all_unique_results)} â†’ {len(filtered_results)} (ì¤‘ë³µ ì œê±° ë° ë‹¤ì–‘ì„± í•„í„° ì ìš©)")
        return filtered_results
    
    async def _apply_intelligent_ranking(
        self,
        results: List[Dict[str, Any]],
        original_query: str,
        model: str
    ) -> List[Dict[str, Any]]:
        """ì§€ëŠ¥í˜• ë­í‚¹ ì ìš©"""
        if not results:
            return results
        
        # ë‹¤ì°¨ì› ìŠ¤ì½”ì–´ë§
        for result in results:
            score_components = {
                "relevance": result.get("relevance_score", 0.5),
                "authority": self._calculate_authority_score(result.get("url", "")),
                "freshness": self._calculate_freshness_score(result.get("timestamp")),
                "priority": self._get_priority_weight(result.get("query_priority", 2)),
                "diversity": 0.1  # ê¸°ë³¸ ë‹¤ì–‘ì„± ì ìˆ˜
            }
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weights = {
                "relevance": 0.4,
                "authority": 0.25, 
                "freshness": 0.15,
                "priority": 0.15,
                "diversity": 0.05
            }
            
            final_score = sum(score_components[key] * weights[key] for key in weights)
            result["final_ranking_score"] = final_score
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        ranked_results = sorted(results, key=lambda x: x.get("final_ranking_score", 0), reverse=True)
        
        top_scores = [round(r.get('final_ranking_score', 0), 3) for r in ranked_results[:3]]
        self.logger.info(f"ì§€ëŠ¥í˜• ë­í‚¹ ì ìš© ì™„ë£Œ: ìƒìœ„ 3ê°œ ì ìˆ˜ {top_scores}")
        return ranked_results
    
    def _calculate_authority_score(self, url: str) -> float:
        """ë„ë©”ì¸ ê¶Œìœ„ë„ ì ìˆ˜ ê³„ì‚°"""
        if not url:
            return 0.5
        
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.lower()
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ë“¤
        high_authority_domains = {
            "wikipedia.org": 0.95,
            "github.com": 0.9,
            "stackoverflow.com": 0.9,
            "medium.com": 0.8,
            "google.com": 0.85,
            "microsoft.com": 0.85,
            "openai.com": 0.85,
            "arxiv.org": 0.95,
            "nature.com": 0.95,
            "sciencedirect.com": 0.9
        }
        
        for trusted_domain, score in high_authority_domains.items():
            if trusted_domain in domain:
                return score
        
        # í•œêµ­ ì£¼ìš” ë„ë©”ì¸ë“¤
        korean_domains = {
            "naver.com": 0.8,
            "daum.net": 0.75,
            "tistory.com": 0.7,
            "blog.naver.com": 0.65,
            "brunch.co.kr": 0.7
        }
        
        for korean_domain, score in korean_domains.items():
            if korean_domain in domain:
                return score
        
        # ê¸°íƒ€ ë„ë©”ì¸
        return 0.6
    
    def _calculate_freshness_score(self, timestamp) -> float:
        """ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚°"""
        if not timestamp:
            return 0.5
        
        try:
            from datetime import datetime, timedelta
            if isinstance(timestamp, str):
                # ISO í˜•ì‹ íŒŒì‹± ì‹œë„
                timestamp = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            
            now = datetime.now()
            age_days = (now - timestamp).days
            
            if age_days <= 30:
                return 0.9
            elif age_days <= 90:
                return 0.8
            elif age_days <= 365:
                return 0.6
            else:
                return 0.4
                
        except:
            return 0.5
    
    def _get_priority_weight(self, priority: int) -> float:
        """ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        return {1: 1.0, 2: 0.8, 3: 0.6}.get(priority, 0.5)
    
    def _convert_to_citations_and_sources(self, results: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """ê²°ê³¼ë¥¼ citationsì™€ sources í˜•íƒœë¡œ ë³€í™˜"""
        citations = []
        sources = []
        
        for i, result in enumerate(results):
            citation = {
                "id": f"search_{i+1}",
                "title": result.get("title", "ì œëª© ì—†ìŒ"),
                "url": result.get("url", ""),
                "snippet": result.get("snippet", "")[:200] + ("..." if len(result.get("snippet", "")) > 200 else ""),
                "source": result.get("source", "unknown"),
                "score": result.get("final_ranking_score", result.get("score", 0.5))
            }
            citations.append(citation)
            
            source = {
                "title": result.get("title", "ì œëª© ì—†ìŒ"),
                "url": result.get("url", ""),
                "type": "web_search",
                "provider": result.get("source", "unknown").split('_')[0] if '_' in result.get("source", "") else result.get("source", "unknown"),
                "search_query": result.get("search_query", ""),
                "ranking_score": result.get("final_ranking_score", 0.5)
            }
            sources.append(source)
        
        return citations, sources
    
    async def _generate_enhanced_response(
        self,
        original_query: str,
        search_queries: List[SearchQuery],
        search_results: List[Dict[str, Any]],
        model: str
    ) -> str:
        """ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µí•©ëœ ë‹µë³€ ìƒì„±"""
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            results_text = ""
            for i, result in enumerate(search_results[:8], 1):
                results_text += f"""
{i}. {result.get('title', 'ì œëª© ì—†ìŒ')}
   URL: {result.get('url', '')}
   ë‚´ìš©: {result.get('snippet', 'ì„¤ëª… ì—†ìŒ')[:300]}
   ê²€ìƒ‰ì–´: "{result.get('search_query', '')}"
   í’ˆì§ˆì ìˆ˜: {result.get('final_ranking_score', 0):.2f}
"""
            
            # ì‚¬ìš©ëœ ê²€ìƒ‰ì–´ë“¤
            search_queries_text = ", ".join([f'"{q.query}"' for q in search_queries])
            
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{original_query}"

ë‹¤ì¤‘ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•œ í¬ê´„ì ì¸ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
ì‚¬ìš©ëœ ê²€ìƒ‰ì–´: {search_queries_text}

ë‹¤ìŒì€ í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë­í‚¹ëœ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:
{results_text}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€
2. ë‹¤ì–‘í•œ ê²€ìƒ‰ ê²°ê³¼ì˜ í•µì‹¬ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê· í˜•ì¡íŒ ì‹œê° ì œê³µ
3. ì‹¤ìš©ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ ìš°ì„ 
4. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì˜ ì •ë³´ ê°•ì¡°
5. í•„ìš”ì‹œ ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ í¬í•¨
6. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰½ê²Œ ì‘ì„±
7. ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ì„ í™œìš©í•˜ì—¬ í¬ê´„ì ì¸ ë‹µë³€ ì œê³µ

ë‹µë³€:
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            return response
            
        except Exception as e:
            self.logger.error(f"í†µí•© ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # Fallback: ê°„ë‹¨í•œ ê²°ê³¼ ìš”ì•½
            summary = f"'{original_query}'ì— ëŒ€í•œ ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n"
            for i, result in enumerate(search_results[:5], 1):
                summary += f"{i}. {result.get('title', 'ì œëª© ì—†ìŒ')}\n"
                summary += f"   {result.get('snippet', 'ì„¤ëª… ì—†ìŒ')[:150]}...\n\n"
            return summary
    
    async def _enhance_summary(
        self,
        original_query: str,
        search_summary: str,
        model: str
    ) -> str:
        """ê²€ìƒ‰ ìš”ì•½ì„ LLMìœ¼ë¡œ ë”ìš± í–¥ìƒì‹œí‚´"""
        try:
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{original_query}"

ì›¹ ê²€ìƒ‰ ìš”ì•½:
{search_summary}

ìœ„ì˜ ê²€ìƒ‰ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë”ìš± ì§ì ‘ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ê°œì„  ì‚¬í•­:
1. ì‚¬ìš©ì ì˜ë„ì— ë§ëŠ” í•µì‹¬ ì •ë³´ ê°•ì¡°
2. ì‹¤ìš©ì ì¸ ì¡°ì–¸ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
3. ê´€ë ¨ëœ ì¶”ê°€ ì •ë³´ë‚˜ ì£¼ì˜ì‚¬í•­
4. ë” ì½ê¸° ì‰¬ìš´ êµ¬ì¡°ë¡œ ì¬êµ¬ì„±

í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            return response
            
        except Exception as e:
            self.logger.warning(f"ìš”ì•½ í–¥ìƒ ì‹¤íŒ¨, ì›ë³¸ ìš”ì•½ ì‚¬ìš©: {e}")
            return search_summary
    
    
    
    def get_capabilities(self) -> List[str]:
        """ì—ì´ì „íŠ¸ ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "ë‹¤ì¤‘ ê²€ìƒ‰ì–´ ìƒì„±",
            "ë³‘ë ¬ ì›¹ ê²€ìƒ‰",
            "ì§€ëŠ¥í˜• ê²°ê³¼ ë­í‚¹",
            "ì¤‘ë³µ ì œê±° ë° ê²°ê³¼ í†µí•©", 
            "ì‹¤ì‹œê°„ ì •ë³´ ì¡°íšŒ",
            "ë„ë©”ì¸ ê¶Œìœ„ë„ í‰ê°€",
            "ìµœì‹ ì„± ê¸°ë°˜ í•„í„°ë§",
            "ë‹¤ì¤‘ ì†ŒìŠ¤ ì¢…í•© ë¶„ì„"
        ]
    
    def get_supported_models(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡"""
        return ["gemini", "claude", "openai"]


# ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
web_search_agent = WebSearchAgent()