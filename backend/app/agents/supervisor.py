"""
Supervisor ì—ì´ì „íŠ¸ - ì§€ëŠ¥í˜• ë¼ìš°íŒ… ì‹œìŠ¤í…œì„ í†µí•´ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ Worker ì—ì´ì „íŠ¸ì—ê²Œ ë¶„ë°°
"""

import time
import json
from typing import Dict, Any, Optional, List
from enum import Enum
import logging

from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.agents.llm_router import llm_router
from app.agents.workers.web_search import web_search_agent
from app.agents.workers.information_gap_analyzer import information_gap_analyzer
from app.agents.workers.simple_canvas import SimpleCanvasAgent
from app.agents.routing.intent_classifier import dynamic_intent_classifier, IntentType

logger = logging.getLogger(__name__)


# TaskTypeì€ IntentTypeìœ¼ë¡œ ëŒ€ì²´ë¨
TaskType = IntentType  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­


class SupervisorAgent(BaseAgent):
    """Supervisor ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_id="supervisor",
            name="Supervisor ì—ì´ì „íŠ¸",
            description="ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ Worker ì—ì´ì „íŠ¸ì—ê²Œ ë¶„ë°°í•©ë‹ˆë‹¤"
        )
        
        # Worker ì—ì´ì „íŠ¸ ë“±ë¡ (information_gap_analyzerëŠ” ë‚´ë¶€ ë¡œì§ìœ¼ë¡œ ì‚¬ìš©)
        self.simple_canvas_agent = SimpleCanvasAgent()  # ë‹¨ìˆœí™”ëœ Canvas ì—ì´ì „íŠ¸
        self.workers = {
            TaskType.WEB_SEARCH: web_search_agent,
            TaskType.CANVAS: self.simple_canvas_agent,
            # TaskType.DEEP_RESEARCH: deep_search_agent,  # ì¶”í›„ êµ¬í˜„
            # TaskType.MULTIMODAL_RAG: multimodal_rag_agent,  # ì¶”í›„ êµ¬í˜„
        }
        
        # ì •ë³´ ë¶„ì„ê¸°ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©
        self.information_analyzer = information_gap_analyzer
    
    async def execute(self, input_data: AgentInput, model: str = "claude-sonnet", progress_callback=None) -> AgentOutput:
        """Supervisor ì—ì´ì „íŠ¸ ì‹¤í–‰ - ì§€ëŠ¥í˜• ë¼ìš°íŒ… ì‹œìŠ¤í…œ ì‚¬ìš©"""
        start_time = time.time()
        
        if not self.validate_input(input_data):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ë°ì´í„°")
        
        try:
            if progress_callback:
                await progress_callback({
                    "step": "intent_analysis",
                    "message": "ì‚¬ìš©ì ì˜ë„ ë¶„ì„ ì¤‘...",
                    "progress": 10
                })
            
            # 1ë‹¨ê³„: ëŒ€í™” ë§¥ë½ ì¤€ë¹„
            if not input_data.conversation_context and input_data.context:
                # ê¸°ì¡´ contextì—ì„œ conversation_context ì¶”ì¶œí•˜ì—¬ ì „ë‹¬
                conversation_context_data = input_data.context.get('conversation_context', {})
                if conversation_context_data:
                    # ConversationContext ê°ì²´ ìƒì„±
                    from app.agents.base import ConversationContext
                    input_data.conversation_context = ConversationContext(**conversation_context_data)
                    self.logger.info(f"ğŸ” ëŒ€í™” ë§¥ë½ ë¡œë“œ: ì£¼ì œ={input_data.conversation_context.current_focus_topic}")
            
            # 2ë‹¨ê³„: ì§€ëŠ¥í˜• ì˜ë„ ë¶„ë¥˜ ì‹¤í–‰
            self.logger.info(f"ğŸ§  ì§€ëŠ¥í˜• ì˜ë„ ë¶„ë¥˜ ì‹œì‘ - ì¿¼ë¦¬: {input_data.query[:100]}...")
            
            classification_result = await dynamic_intent_classifier.execute(input_data, model)
            classification_data = json.loads(classification_result.result)
            
            primary_intent = IntentType(classification_data["primary_intent"])
            confidence = classification_data["confidence"]
            reasoning = classification_data["reasoning"]
            
            self.logger.info(f"ğŸ¯ ë¶„ë¥˜ ê²°ê³¼: {primary_intent.value} (ì‹ ë¢°ë„: {confidence:.2f})")
            self.logger.info(f"ğŸ“ ë¶„ë¥˜ ê·¼ê±°: {reasoning}")
            
            # 2ë‹¨ê³„: ì‹ ë¢°ë„ ê¸°ë°˜ ì²˜ë¦¬ ê²°ì •
            if confidence < 0.6:
                self.logger.warning(f"âš ï¸  ë¶„ë¥˜ ì‹ ë¢°ë„ ë‚®ìŒ ({confidence:.2f}) - ì¶”ê°€ ë¶„ì„ ë˜ëŠ” ì‚¬ìš©ì í™•ì¸ í•„ìš”")
                
                if classification_data.get("requires_clarification", False):
                    # ëª…í™•í™”ê°€ í•„ìš”í•œ ê²½ìš°
                    return self.create_output(
                        result=f"ì§ˆë¬¸ì´ ë‹¤ì†Œ ì• ë§¤í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?\n\nì¶”ì²œ ì§ˆë¬¸:\n" +
                               "\n".join([f"â€¢ {q}" for q in classification_data.get("suggested_follow_ups", [])]),
                        metadata={
                            "supervisor_decision": "clarification_needed",
                            "classification_confidence": confidence,
                            "original_intent": primary_intent.value,
                            "reasoning": reasoning
                        },
                        execution_time_ms=int((time.time() - start_time) * 1000),
                        model_used=model
                    )
            
            # 3ë‹¨ê³„: ì •ë³´ ë¶€ì¡± ë¶„ì„ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            if primary_intent in [IntentType.WEB_SEARCH, IntentType.DEEP_RESEARCH] and confidence >= 0.7:
                self.logger.info("ğŸ” ì •ë³´ ë¶€ì¡± ë¶„ì„ ì‹¤í–‰")
                
                info_analysis_result = await self.information_analyzer.execute(input_data, model)
                
                if info_analysis_result.metadata.get("needs_more_info", False):
                    self.logger.info("â“ ì •ë³´ ë¶€ì¡± ê°ì§€ - ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ ìš”ì²­")
                    info_analysis_result.metadata["supervisor_decision"] = "information_request"
                    info_analysis_result.metadata["original_intent"] = primary_intent.value
                    info_analysis_result.metadata["classification_confidence"] = confidence
                    return info_analysis_result
            
            # 4ë‹¨ê³„: ë³µí•© ì˜ë„ ì²˜ë¦¬ (Multi-step)
            if primary_intent == IntentType.MULTI_STEP:
                self.logger.info("ğŸ”— ë³µí•© ì‘ì—… ê°ì§€ - ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œì‘")
                
                if progress_callback:
                    await progress_callback({
                        "step": "multi_step_analysis",
                        "message": "ë³µí•© ì‘ì—… ë¶„ì„ ì¤‘...",
                        "progress": 25
                    })
                
                # ë³µí•© ì‘ì—… ì²˜ë¦¬
                return await self._handle_multi_step_task(input_data, model, start_time, reasoning, progress_callback)
            
            # 5ë‹¨ê³„: ë‹¨ì¼ Worker ì—ì´ì „íŠ¸ ì„ íƒ ë° ì‹¤í–‰
            worker_agent = self._select_worker(primary_intent)
            
            if worker_agent:
                self.logger.info(f"ğŸš€ ì‘ì—… ìœ„ì„: {primary_intent.value} â†’ {worker_agent.agent_id}")
                
                if progress_callback:
                    await progress_callback({
                        "step": "delegating_to_worker",
                        "message": f"{worker_agent.name}ì—ê²Œ ì‘ì—… ìœ„ì„ ì¤‘...",
                        "progress": 30
                    })
                
                # Worker ì—ì´ì „íŠ¸ ì‹¤í–‰
                result = await worker_agent.execute(input_data, model, progress_callback)
                
                # ë©”íƒ€ë°ì´í„° ê°•í™”
                result.metadata.update({
                    "supervisor_decision": primary_intent.value,
                    "delegated_to": worker_agent.agent_id,
                    "classification_confidence": confidence,
                    "classification_reasoning": reasoning,
                    "routing_version": "v2_intelligent"
                })
                
                return result
            else:
                # Workerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì²˜ë¦¬
                self.logger.info(f"ğŸ¤– ì§ì ‘ ì²˜ë¦¬: {primary_intent.value} (í•´ë‹¹ Worker ì—†ìŒ)")
                return await self._handle_directly(input_data, model, start_time, primary_intent.value)
                
        except Exception as e:
            self.logger.error(f"âŒ Supervisor ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ fallback ë¶„ë¥˜ ì‹œë„
            try:
                fallback_intent = self._emergency_fallback_classification(input_data.query)
                worker_agent = self._select_worker(fallback_intent)
                
                if worker_agent:
                    self.logger.info(f"ğŸ†˜ ê¸´ê¸‰ fallback ì‹¤í–‰: {fallback_intent.value}")
                    result = await worker_agent.execute(input_data, model, progress_callback)
                    result.metadata["supervisor_decision"] = "emergency_fallback"
                    result.metadata["original_error"] = str(e)
                    return result
            except:
                pass
            
            return self.create_output(
                result=f"ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                metadata={
                    "error": str(e),
                    "supervisor_decision": "error_fallback"
                },
                execution_time_ms=execution_time,
                model_used=model
            )
    
    # ë ˆê±°ì‹œ ë©”ì„œë“œ - ìƒˆë¡œìš´ ì‹œìŠ¤í…œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    async def _analyze_task_type_direct(self, query: str, model: str) -> TaskType:
        """ë ˆê±°ì‹œ ë©”ì„œë“œ - dynamic_intent_classifierë¡œ ëŒ€ì²´ë¨"""
        self.logger.warning("ë ˆê±°ì‹œ ë©”ì„œë“œ _analyze_task_type_direct í˜¸ì¶œë¨ - dynamic_intent_classifier ì‚¬ìš© ê¶Œì¥")
        return self._emergency_fallback_classification(query)
    
    # ë ˆê±°ì‹œ ë©”ì„œë“œë“¤ - ìƒˆë¡œìš´ ì‹œìŠ¤í…œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    async def _smart_information_analysis(self, query: str, model: str) -> Dict[str, Any]:
        """ë ˆê±°ì‹œ ë©”ì„œë“œ - information_gap_analyzerë¡œ ëŒ€ì²´ë¨"""
        self.logger.warning("ë ˆê±°ì‹œ ë©”ì„œë“œ _smart_information_analysis í˜¸ì¶œë¨")
        return {"needs_analysis": False, "confidence": 0.5, "method": "legacy_fallback"}
    
    def _smart_fallback_analysis(self, query: str) -> TaskType:
        """ë ˆê±°ì‹œ ë©”ì„œë“œ - _emergency_fallback_classificationìœ¼ë¡œ ëŒ€ì²´ë¨"""
        self.logger.warning("ë ˆê±°ì‹œ ë©”ì„œë“œ _smart_fallback_analysis í˜¸ì¶œë¨")
        return self._emergency_fallback_classification(query)
    
    def _select_worker(self, intent_type: IntentType) -> Optional[BaseAgent]:
        """ì˜ë„ ìœ í˜•ì— ë”°ë¥¸ Worker ì—ì´ì „íŠ¸ ì„ íƒ"""
        # IntentTypeì„ TaskTypeìœ¼ë¡œ ë§¤í•‘ (í•˜ìœ„ í˜¸í™˜ì„±)
        task_type_mapping = {
            IntentType.WEB_SEARCH: TaskType.WEB_SEARCH,
            IntentType.DEEP_RESEARCH: TaskType.DEEP_RESEARCH,
            IntentType.CANVAS: TaskType.CANVAS,
            IntentType.GENERAL_CHAT: TaskType.GENERAL_CHAT,
            IntentType.MULTI_STEP: TaskType.WEB_SEARCH,  # ì„ì‹œë¡œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë§¤í•‘
            IntentType.CLARIFICATION: TaskType.GENERAL_CHAT  # ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ë§¤í•‘
        }
        
        mapped_task_type = task_type_mapping.get(intent_type, TaskType.GENERAL_CHAT)
        worker = self.workers.get(mapped_task_type)
        
        if worker:
            return worker
        
        # í•´ë‹¹ Workerê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ Worker ì„ íƒ
        if intent_type in [IntentType.DEEP_RESEARCH, IntentType.MULTI_STEP]:
            # Deep Researchë‚˜ Multi-stepì´ ì—†ìœ¼ë©´ Web Searchë¡œ ëŒ€ì²´
            return self.workers.get(TaskType.WEB_SEARCH)
        elif intent_type == IntentType.CLARIFICATION:
            # ëª…í™•í™” ìš”ì²­ì€ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ì²˜ë¦¬
            return None  # ì§ì ‘ ì²˜ë¦¬
        
        return None
    
    def _emergency_fallback_classification(self, query: str) -> IntentType:
        """ê¸´ê¸‰ ìƒí™©ìš© ë‹¨ìˆœ ë¶„ë¥˜"""
        # Canvas í‚¤ì›Œë“œ (ê°€ì¥ ëª…í™•í•œ íŒ¨í„´)
        canvas_keywords = ["ê·¸ë ¤", "ë§Œë“¤ì–´", "ìƒì„±í•´", "ë””ìì¸", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”", "ì´ë¯¸ì§€"]
        if any(keyword in query for keyword in canvas_keywords) and "ì„¤ëª…" not in query:
            return IntentType.CANVAS
        
        # ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_keywords = ["ê²€ìƒ‰", "ì°¾ì•„", "ìµœì‹ ", "í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ê°€ê²©", "ì–´ë””ì„œ", "ì–¸ì œ"]
        if any(keyword in query for keyword in search_keywords):
            return IntentType.WEB_SEARCH
        
        # ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”
        return IntentType.GENERAL_CHAT
    
    async def _handle_directly(self, input_data: AgentInput, model: str, start_time: float, intent_type: str = "general_chat") -> AgentOutput:
        """Supervisorê°€ ì§ì ‘ ì²˜ë¦¬"""
        try:
            # ì˜ë„ ìœ í˜•ì— ë”°ë¥¸ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸
            if intent_type == "clarification":
                prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‹¤ì†Œ ëª¨í˜¸í•©ë‹ˆë‹¤: "{input_data.query}"

ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›Œ ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê²Œ ë‹¤ìŒê³¼ ê°™ì€ ë„ì›€ì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. ì§ˆë¬¸ì„ ë” ëª…í™•íˆ í•˜ëŠ” ë°©ë²• ì œì•ˆ
2. êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ìƒí™© ìš”ì²­  
3. ê´€ë ¨ëœ ëª‡ ê°€ì§€ ê°€ëŠ¥í•œ í•´ì„ ì œì‹œ

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            else:
                prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{input_data.query}"

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
í˜„ì¬ íŠ¹ë³„í•œ ë„êµ¬ë‚˜ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ, 
ê°€ëŠ¥í•œ í•œ ìœ ìš©í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result=response,
                metadata={
                    "handled_by": "supervisor_direct",
                    "intent_type": intent_type,
                    "reason": "no_suitable_worker_available"
                },
                execution_time_ms=execution_time,
                model_used=model
            )
            
        except Exception as e:
            self.logger.error(f"ì§ì ‘ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                metadata={"error": str(e)},
                execution_time_ms=execution_time,
                model_used=model
            )
    
    def get_capabilities(self) -> list[str]:
        """Supervisor ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "ì§€ëŠ¥í˜• ì˜ë„ ë¶„ë¥˜",
            "ë§¥ë½ ì¸ì‹ ë¼ìš°íŒ…",
            "ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ê¸°",
            "Worker ì—ì´ì „íŠ¸ ê´€ë¦¬",
            "ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™”",
            "ê¸´ê¸‰ ìƒí™© ì²˜ë¦¬"
        ]
    
    def get_supported_models(self) -> list[str]:
        """ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡"""
        return ["gemini", "claude", "openai"]
    
    def get_available_workers(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Worker ì—ì´ì „íŠ¸ ëª©ë¡"""
        return {
            task_type.value: worker.name 
            for task_type, worker in self.workers.items()
        }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ë¼ìš°íŒ… ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
        try:
            classifier_report = dynamic_intent_classifier.get_performance_report()
            return {
                "routing_version": "v2_intelligent",
                "status": "active",
                "classifier_performance": classifier_report,
                "available_workers": len(self.workers),
                "supported_intents": [intent.value for intent in IntentType],
                "capabilities": self.get_capabilities()
            }
        except Exception as e:
            return {
                "routing_version": "v2_intelligent",
                "status": "error",
                "error": str(e),
                "fallback_available": True
            }
    
    async def record_user_correction(self, user_id: str, original_intent: str, correct_intent: str, query: str):
        """ì‚¬ìš©ì ìˆ˜ì • ì‚¬í•­ ê¸°ë¡ (í•™ìŠµ í–¥ìƒìš©)"""
        try:
            await dynamic_intent_classifier.record_correction(user_id, original_intent, correct_intent, query)
            self.logger.info(f"âœ… ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡ ì™„ë£Œ: {original_intent} â†’ {correct_intent}")
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def _handle_multi_step_task(
        self, 
        input_data: AgentInput, 
        model: str, 
        start_time: float, 
        reasoning: str,
        progress_callback=None
    ) -> AgentOutput:
        """ë³µí•© ì‘ì—… ì²˜ë¦¬ - ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì‘ì—…ì„ ìˆœì°¨ ì‹¤í–‰"""
        try:
            self.logger.info(f"ğŸ”— ë³µí•© ì‘ì—… ë¶„í•´ ì‹œì‘ - ì¿¼ë¦¬: {input_data.query}")
            
            # 1ë‹¨ê³„: ë³µí•© ì‘ì—…ì„ ê°œë³„ ë‹¨ê³„ë¡œ ë¶„í•´
            task_breakdown = await self._decompose_multi_step_task(input_data.query, model)
            
            if not task_breakdown or len(task_breakdown) < 2:
                # ë¶„í•´ ì‹¤íŒ¨ ì‹œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ fallback
                self.logger.warning("ë³µí•© ì‘ì—… ë¶„í•´ ì‹¤íŒ¨ - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬")
                worker_agent = self.workers.get(TaskType.WEB_SEARCH)
                if worker_agent:
                    return await worker_agent.execute(input_data, model, progress_callback)
                else:
                    return await self._handle_directly(input_data, model, start_time, "multi_step_fallback")
            
            self.logger.info(f"ğŸ“‹ ì‘ì—… ë¶„í•´ ì™„ë£Œ - {len(task_breakdown)}ê°œ ë‹¨ê³„: {[step['action'] for step in task_breakdown]}")
            
            # 2ë‹¨ê³„: ìˆœì°¨ì ìœ¼ë¡œ ê° ë‹¨ê³„ ì‹¤í–‰
            accumulated_results = []
            current_context = input_data.context or {}
            
            for i, step in enumerate(task_breakdown):
                step_number = i + 1
                total_steps = len(task_breakdown)
                
                if progress_callback:
                    await progress_callback({
                        "step": f"multi_step_{step_number}",
                        "message": f"ë‹¨ê³„ {step_number}/{total_steps}: {step['description']}",
                        "progress": 30 + (50 * step_number // total_steps)
                    })
                
                self.logger.info(f"ğŸ”„ ë‹¨ê³„ {step_number}/{total_steps} ì‹¤í–‰: {step['action']} - {step['description']}")
                
                # ê° ë‹¨ê³„ì— ë§ëŠ” Worker ì„ íƒ
                step_intent = IntentType(step['action'])
                worker_agent = self._select_worker(step_intent)
                
                if worker_agent:
                    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                    if accumulated_results:
                        current_context['previous_step_results'] = accumulated_results
                    
                    # ë‹¨ê³„ë³„ ì…ë ¥ ë°ì´í„° êµ¬ì„±
                    step_input = AgentInput(
                        query=step['query'],
                        user_id=input_data.user_id,
                        session_id=input_data.session_id,
                        context=current_context,
                        conversation_context=input_data.conversation_context
                    )
                    
                    # ë‹¨ê³„ ì‹¤í–‰
                    step_result = await worker_agent.execute(step_input, model)
                    
                    accumulated_results.append({
                        "step": step_number,
                        "action": step['action'],
                        "description": step['description'],
                        "query": step['query'],
                        "result": step_result.result,
                        "metadata": step_result.metadata,
                        "execution_time_ms": step_result.execution_time_ms
                    })
                    
                    self.logger.info(f"âœ… ë‹¨ê³„ {step_number} ì™„ë£Œ - {step['action']}")
                else:
                    # Workerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì²˜ë¦¬
                    self.logger.warning(f"âš ï¸ ë‹¨ê³„ {step_number} Worker ì—†ìŒ - ì§ì ‘ ì²˜ë¦¬")
                    accumulated_results.append({
                        "step": step_number,
                        "action": step['action'],
                        "description": step['description'],
                        "query": step['query'],
                        "result": f"ë‹¨ê³„ {step_number} ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ìš© ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "metadata": {"error": "no_worker_available"},
                        "execution_time_ms": 0
                    })
            
            # 3ë‹¨ê³„: ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = await self._synthesize_multi_step_results(
                input_data.query, accumulated_results, model
            )
            
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result=final_response,
                metadata={
                    "supervisor_decision": "multi_step",
                    "routing_version": "v2_intelligent",
                    "multi_step_breakdown": task_breakdown,
                    "steps_completed": len(accumulated_results),
                    "step_results": accumulated_results
                },
                execution_time_ms=execution_time,
                model_used=model
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ë³µí•© ì‘ì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result="ì£„ì†¡í•©ë‹ˆë‹¤. ë³µí•© ì‘ì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                metadata={
                    "supervisor_decision": "multi_step_error",
                    "error": str(e)
                },
                execution_time_ms=execution_time,
                model_used=model
            )
    
    async def _decompose_multi_step_task(self, query: str, model: str) -> List[Dict[str, str]]:
        """ë³µí•© ì‘ì—…ì„ ê°œë³„ ë‹¨ê³„ë¡œ ë¶„í•´"""
        try:
            prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•  ë‹¨ê³„ë“¤ë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ê° ë‹¨ê³„ëŠ” ë‹¤ìŒ ì‘ì—… ìœ í˜• ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:
- web_search: ì¸í„°ë„· ê²€ìƒ‰
- deep_research: ì‹¬ì¸µ ë¶„ì„  
- canvas: ì‹œê°ì  ì°½ì‘
- general_chat: ì¼ë°˜ ëŒ€í™”

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
[
  {{
    "action": "web_search|deep_research|canvas|general_chat",
    "description": "ì´ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰í•  ì‘ì—… ì„¤ëª…",
    "query": "ì‹¤ì œë¡œ ì‹¤í–‰í•  êµ¬ì²´ì  ì§ˆë¬¸"
  }}
]

ì˜ˆì‹œ:
ì§ˆë¬¸: "ìµœì‹  ìŠ¤ë§ˆíŠ¸í°ì„ ì°¾ì•„ì„œ ì¥ë‹¨ì  ë¹„êµí•´ì¤˜"
ì‘ë‹µ:
[
  {{
    "action": "web_search",
    "description": "ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ ê²€ìƒ‰",
    "query": "2025ë…„ ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ ë¦¬ìŠ¤íŠ¸"
  }},
  {{
    "action": "deep_research", 
    "description": "ì°¾ì€ ìŠ¤ë§ˆíŠ¸í°ë“¤ì˜ ì¥ë‹¨ì  ë¶„ì„",
    "query": "ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ë“¤ì˜ ìƒì„¸ ë¹„êµ ë¶„ì„"
  }}
]

JSONë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
            
            response, _ = await llm_router.generate_response(model, prompt, include_datetime=False)
            
            # JSON íŒŒì‹±
            clean_response = response.strip()
            if clean_response.startswith('```json'):
                clean_response = clean_response[7:]
            if clean_response.endswith('```'):
                clean_response = clean_response[:-3]
            
            task_breakdown = json.loads(clean_response.strip())
            
            # ìœ íš¨ì„± ê²€ì¦
            valid_actions = {"web_search", "deep_research", "canvas", "general_chat"}
            filtered_breakdown = []
            
            for step in task_breakdown:
                if (isinstance(step, dict) and 
                    "action" in step and 
                    "description" in step and 
                    "query" in step and
                    step["action"] in valid_actions):
                    filtered_breakdown.append(step)
            
            return filtered_breakdown if len(filtered_breakdown) >= 2 else []
            
        except Exception as e:
            self.logger.error(f"ë³µí•© ì‘ì—… ë¶„í•´ ì‹¤íŒ¨: {e}")
            return []
    
    async def _synthesize_multi_step_results(self, original_query: str, step_results: List[Dict], model: str) -> str:
        """ì—¬ëŸ¬ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±"""
        try:
            # ê° ë‹¨ê³„ ê²°ê³¼ ìš”ì•½
            results_summary = []
            for result in step_results:
                summary = f"**ë‹¨ê³„ {result['step']}: {result['description']}**\n"
                summary += f"ê²°ê³¼: {result['result'][:200]}{'...' if len(result['result']) > 200 else ''}\n"
                results_summary.append(summary)
            
            prompt = f"""
ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ì–»ì€ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•˜ê³  ìœ ìš©í•œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{original_query}"

ë‹¨ê³„ë³„ ê²°ê³¼:
{chr(10).join(results_summary)}

ë‹¤ìŒ ì›ì¹™ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ ì œê³µ
2. ë…¼ë¦¬ì  íë¦„ìœ¼ë¡œ ì •ë³´ë¥¼ êµ¬ì„±
3. ì‚¬ìš©ìê°€ ì›í–ˆë˜ ì •ë³´ë¥¼ ëª…í™•íˆ ì „ë‹¬
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
5. í•„ìš”ì‹œ ìš”ì•½, ê²°ë¡ , ì¶”ì²œ ì‚¬í•­ í¬í•¨

ìµœì¢… ë‹µë³€:
"""
            
            response, _ = await llm_router.generate_response(model, prompt, include_datetime=False)
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì¢…í•© ì‹¤íŒ¨: {e}")
            # Fallback: ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ë‹¨ìˆœ ë‚˜ì—´
            fallback_response = f"ì›ë³¸ ì§ˆë¬¸: {original_query}\n\n"
            for result in step_results:
                fallback_response += f"**{result['description']}:**\n{result['result']}\n\n"
            return fallback_response
    
    async def analyze_and_suggest_agent(self, query: str, current_agent: str, model: str = "gemini") -> Dict[str, Any]:
        """í˜„ì¬ ì—ì´ì „íŠ¸ì™€ ë‹¤ë¥¸ ë” ì í•©í•œ ì—ì´ì „íŠ¸ë¥¼ ì œì•ˆ"""
        try:
            # ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„ (ì •ë³´ ë¶„ì„ ì—†ì´ ë°”ë¡œ ì‘ì—… ìœ í˜• ë¶„ì„)
            suggested_task_type = await self._analyze_task_type_direct(query, model)
            suggested_agent = suggested_task_type.value
            
            # í˜„ì¬ ì—ì´ì „íŠ¸ì™€ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì œì•ˆ
            if suggested_agent != current_agent and suggested_agent != "general_chat":
                # ìƒì„¸ ë¶„ì„ìœ¼ë¡œ ì‹ ë¢°ë„ ë° ì´ìœ  ìƒì„±
                confidence, reason = await self._analyze_suggestion_details(
                    query, current_agent, suggested_agent, model
                )
                
                return {
                    "needs_switch": True,
                    "suggested_agent": suggested_agent,
                    "confidence": confidence,
                    "reason": reason,
                    "current_agent": current_agent
                }
            
            return {"needs_switch": False}
            
        except Exception as e:
            self.logger.error(f"ì—ì´ì „íŠ¸ ì œì•ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"needs_switch": False, "error": str(e)}
    
    async def _analyze_suggestion_details(self, query: str, current_agent: str, suggested_agent: str, model: str) -> tuple[float, str]:
        """ì œì•ˆ ìƒì„¸ ë¶„ì„ - ì‹ ë¢°ë„ì™€ ì´ìœ  ìƒì„±"""
        try:
            agent_descriptions = {
                "none": "ì¼ë°˜ ì±„íŒ…",
                "web_search": "ì›¹ ê²€ìƒ‰ì„ í†µí•œ ìµœì‹  ì •ë³´ ì¡°íšŒ",
                "deep_research": "ì‹¬ì¸µì ì¸ ë¶„ì„ê³¼ ì—°êµ¬",
                "canvas": "ì´ë¯¸ì§€ ìƒì„±, ë§ˆì¸ë“œë§µ, ì‹œê°ì  ì°½ì‘",
                "multimodal_rag": "ë¬¸ì„œ ë° ì´ë¯¸ì§€ ë¶„ì„"
            }
            
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì—ì´ì „íŠ¸ ì „í™˜ì´ í•„ìš”í•œ ì´ìœ ì™€ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"
í˜„ì¬ ì—ì´ì „íŠ¸: {agent_descriptions.get(current_agent, current_agent)}
ì œì•ˆ ì—ì´ì „íŠ¸: {agent_descriptions.get(suggested_agent, suggested_agent)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ì‹ ë¢°ë„: [0.1-1.0 ì‚¬ì´ì˜ ìˆ«ì]
ì´ìœ : [í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ]

ì˜ˆì‹œ:
ì‹ ë¢°ë„: 0.9
ì´ìœ : ìµœì‹  ì£¼ê°€ ì •ë³´ ì¡°íšŒëŠ” ì›¹ ê²€ìƒ‰ì´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            lines = response.strip().split('\n')
            
            confidence = 0.7  # ê¸°ë³¸ê°’
            reason = f"{agent_descriptions.get(suggested_agent, suggested_agent)}ê°€ ì´ ì‘ì—…ì— ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            
            for line in lines:
                if line.startswith('ì‹ ë¢°ë„:'):
                    try:
                        confidence = float(line.split(':')[1].strip())
                        confidence = max(0.1, min(1.0, confidence))  # ë²”ìœ„ ì œí•œ
                    except:
                        pass
                elif line.startswith('ì´ìœ :'):
                    reason = line.split(':', 1)[1].strip()
            
            return confidence, reason
            
        except Exception as e:
            self.logger.warning(f"ì œì•ˆ ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7, f"{suggested_agent.replace('_', ' ')}ì´ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"


# Supervisor ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
supervisor_agent = SupervisorAgent()