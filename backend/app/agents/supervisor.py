"""
Supervisor ì—ì´ì „íŠ¸ - ì§€ëŠ¥í˜• ë¼ìš°íŒ… ì‹œìŠ¤í…œì„ í†µí•´ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ Worker ì—ì´ì „íŠ¸ì—ê²Œ ë¶„ë°°
"""

import time
import json
import asyncio
from typing import Dict, Any, Optional, List
from enum import Enum
import logging

from app.agents.base import BaseAgent, AgentInput, AgentOutput
from app.agents.llm_router import llm_router
from app.agents.workers.web_search import web_search_agent
from app.agents.workers.information_gap_analyzer import information_gap_analyzer
from app.agents.workers.simple_canvas import SimpleCanvasAgent
from app.agents.routing.intent_classifier import dynamic_intent_classifier, IntentType

# ğŸš€ 2025 ì°¨ì„¸ëŒ€ Fast Path ìµœì í™” ì‹œìŠ¤í…œ
from app.agents.intent_classifier import intent_classifier, IntentType as NewIntentType, IntentClassificationResult
from app.agents.context_optimizer import context_optimizer, ContextOptimizationResult

# LangGraph ì—ì´ì „íŠ¸ imports (100% í™œì„±í™”)
from app.agents.langgraph.web_search_langgraph import langgraph_web_search_agent
from app.agents.langgraph.canvas_langgraph import langgraph_canvas_agent
from app.agents.langgraph.information_gap_langgraph import langgraph_information_gap_analyzer
from app.core.feature_flags import is_langgraph_enabled, LangGraphFeatureFlags

logger = logging.getLogger(__name__)


# TaskTypeì€ IntentTypeìœ¼ë¡œ ëŒ€ì²´ë¨
TaskType = IntentType  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­


class SupervisorAgent(BaseAgent):
    """Supervisor ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            agent_id="supervisor",
            name="Supervisor ì—ì´ì „íŠ¸",
            description="ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ Worker ì—ì´ì „íŠ¸ì—ê²Œ ë¶„ë°°í•©ë‹ˆë‹¤"
        )
        
        # Worker ì—ì´ì „íŠ¸ ë“±ë¡ (information_gap_analyzerëŠ” ë‚´ë¶€ ë¡œì§ìœ¼ë¡œ ì‚¬ìš©)
        self.simple_canvas_agent = SimpleCanvasAgent()  # ë‹¨ìˆœí™”ëœ Canvas ì—ì´ì „íŠ¸
        
        # ğŸš€ 100% LangGraph ì—ì´ì „íŠ¸ ë§µ (ìµœê³  ì„±ëŠ¥)
        self.workers = {
            TaskType.WEB_SEARCH: self._get_web_search_agent,          # LangGraph WebSearch
            TaskType.CANVAS: self._get_canvas_agent,                  # LangGraph Canvas
            TaskType.GENERAL_CHAT: None,                              # ì§ì ‘ ì²˜ë¦¬
            TaskType.DEEP_RESEARCH: self._get_web_search_agent,       # WebSearchë¡œ ëŒ€ì²´
            TaskType.MULTI_STEP: self._get_web_search_agent,          # WebSearchë¡œ ëŒ€ì²´
            TaskType.CLARIFICATION: self._get_information_gap_agent,  # Information Gap Analyzer
        }
        
        # ğŸš€ ì •ë³´ ë¶„ì„ê¸° LangGraph ë²„ì „ìœ¼ë¡œ 100% ì „í™˜
        self.information_analyzer = langgraph_information_gap_analyzer

    def _get_web_search_agent(self, user_id: str = None):
        """
        ğŸš€ 100% LangGraph WebSearch ì—ì´ì „íŠ¸ (ìš´ì˜ ì¤‘ë‹¨ ì œì•½ ì—†ìŒ)
        """
        self.logger.info(f"ğŸš€ LangGraph WebSearchAgent 100% í™œì„±í™” (ì‚¬ìš©ì: {user_id})")
        return langgraph_web_search_agent

    def _get_canvas_agent(self, user_id: str = None):
        """
        ğŸš€ 100% LangGraph Canvas ì—ì´ì „íŠ¸ (ìš´ì˜ ì¤‘ë‹¨ ì œì•½ ì—†ìŒ)
        """
        self.logger.info(f"ğŸš€ LangGraph CanvasAgent 100% í™œì„±í™” (ì‚¬ìš©ì: {user_id})")
        return langgraph_canvas_agent

    def _get_information_gap_agent(self, user_id: str = None):
        """
        ğŸš€ 100% LangGraph Information Gap Analyzer (ìš´ì˜ ì¤‘ë‹¨ ì œì•½ ì—†ìŒ)
        """
        self.logger.info(f"ğŸš€ LangGraph Information Gap Analyzer 100% í™œì„±í™” (ì‚¬ìš©ì: {user_id})")
        return langgraph_information_gap_analyzer
    
    async def execute(self, input_data: AgentInput, model: str = "claude-sonnet", progress_callback=None) -> AgentOutput:
        """ğŸš€ ì°¨ì„¸ëŒ€ Fast Path ìµœì í™” ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ Supervisor ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        
        if not self.validate_input(input_data):
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ë°ì´í„°")
        
        try:
            # ğŸ§  Stage 1: ëŒ€í™” ë§¥ë½ ìµœì í™” (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)
            context_task = None
            if hasattr(input_data, 'conversation_history') and input_data.conversation_history:
                self.logger.info(f"ğŸ”§ ë§¥ë½ ìµœì í™” ì‹œì‘: {len(input_data.conversation_history)}ê°œ í„´")
                context_task = asyncio.create_task(
                    context_optimizer.optimize_context(
                        input_data.conversation_history,
                        input_data.query,
                        max_tokens=300  # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì œí•œ
                    )
                )
            
            # ğŸ§  Stage 2: 3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ì˜ë„ ë¶„ë¥˜ (ìµœëŒ€ 2ì´ˆ)
            self.logger.info(f"ğŸ§  ì°¨ì„¸ëŒ€ ì˜ë„ ë¶„ë¥˜ ì‹œì‘: '{input_data.query[:50]}...'")
            
            # ë§¥ë½ ìµœì í™” ê²°ê³¼ ëŒ€ê¸° (ìˆëŠ” ê²½ìš°)
            optimized_context = ""
            if context_task:
                try:
                    context_result: ContextOptimizationResult = await asyncio.wait_for(context_task, timeout=1.5)
                    optimized_context = context_result.optimized_context
                    self.logger.info(
                        f"âœ… ë§¥ë½ ìµœì í™” ì™„ë£Œ: {context_result.original_token_count} â†’ "
                        f"{context_result.optimized_token_count} í† í° ({context_result.compression_ratio:.2f}x)"
                    )
                except asyncio.TimeoutError:
                    self.logger.warning("âš ï¸ ë§¥ë½ ìµœì í™” íƒ€ì„ì•„ì›ƒ - ì›ë³¸ ì‚¬ìš©")
                    if context_task:
                        context_task.cancel()
            
            # ğŸš€ ì˜ë„ ë¶„ë¥˜ ìˆ˜í–‰
            classification_result: IntentClassificationResult = await intent_classifier.classify_intent(
                input_data.query, 
                optimized_context if optimized_context else None
            )
            
            self.logger.info(
                f"ğŸ¯ ì˜ë„ ë¶„ë¥˜ ì™„ë£Œ: {classification_result.intent_type.value} "
                f"(ì‹ ë¢°ë„: {classification_result.confidence:.2f}, "
                f"Stage {classification_result.classification_stage}, {classification_result.processing_time_ms}ms)"
            )
            
            # ğŸƒâ€â™‚ï¸ Fast Path ì‹¤í–‰ (ê°„ë‹¨í•œ íŒ©íŠ¸ ì§ˆë¬¸)
            if classification_result.intent_type == NewIntentType.SIMPLE_FACT:
                self.logger.info(f"ğŸƒâ€â™‚ï¸ Fast Path í™œì„±í™” - ê°„ë‹¨í•œ ì§ˆë¬¸ ê°ì§€")
                
                if progress_callback:
                    await progress_callback({
                        "step": "fast_processing",
                        "message": "ë¹ ë¥¸ ì‘ë‹µ ìƒì„± ì¤‘...",
                        "progress": 50
                    })
                    
                return await self._handle_simple_question_fast(input_data, model, start_time)
            
            # ğŸ”„ ë³µì¡ ì²˜ë¦¬ ê²½ë¡œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            return await self._handle_complex_question(input_data, model, classification_result, start_time, progress_callback)
                
        except Exception as e:
            self.logger.error(f"âŒ Supervisor ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ fallback ë¶„ë¥˜ ì‹œë„
            try:
                fallback_intent = self._emergency_fallback_classification(input_data.query)
                worker_agent = self._select_worker(fallback_intent, input_data.user_id)
                
                if worker_agent:
                    self.logger.info(f"ğŸ†˜ ê¸´ê¸‰ fallback ì‹¤í–‰: {fallback_intent.value}")
                    result = await worker_agent.execute(input_data, model, progress_callback)
                    result.metadata["supervisor_decision"] = "emergency_fallback"
                    result.metadata["original_error"] = str(e)
                    return result
            except:
                pass
            
            return self.create_output(
                result=f"ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                metadata={
                    "error": str(e),
                    "supervisor_decision": "error_fallback"
                },
                execution_time_ms=execution_time,
                model_used=model
            )
    
    # ë ˆê±°ì‹œ ë©”ì„œë“œ - ìƒˆë¡œìš´ ì‹œìŠ¤í…œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    async def _analyze_task_type_direct(self, query: str, model: str) -> TaskType:
        """ë ˆê±°ì‹œ ë©”ì„œë“œ - dynamic_intent_classifierë¡œ ëŒ€ì²´ë¨"""
        self.logger.warning("ë ˆê±°ì‹œ ë©”ì„œë“œ _analyze_task_type_direct í˜¸ì¶œë¨ - dynamic_intent_classifier ì‚¬ìš© ê¶Œì¥")
        return self._emergency_fallback_classification(query)
    
    # ë ˆê±°ì‹œ ë©”ì„œë“œë“¤ - ìƒˆë¡œìš´ ì‹œìŠ¤í…œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    async def _smart_information_analysis(self, query: str, model: str) -> Dict[str, Any]:
        """ë ˆê±°ì‹œ ë©”ì„œë“œ - information_gap_analyzerë¡œ ëŒ€ì²´ë¨"""
        self.logger.warning("ë ˆê±°ì‹œ ë©”ì„œë“œ _smart_information_analysis í˜¸ì¶œë¨")
        return {"needs_analysis": False, "confidence": 0.5, "method": "legacy_fallback"}
    
    def _smart_fallback_analysis(self, query: str) -> TaskType:
        """ë ˆê±°ì‹œ ë©”ì„œë“œ - _emergency_fallback_classificationìœ¼ë¡œ ëŒ€ì²´ë¨"""
        self.logger.warning("ë ˆê±°ì‹œ ë©”ì„œë“œ _smart_fallback_analysis í˜¸ì¶œë¨")
        return self._emergency_fallback_classification(query)
    
    def _select_worker(self, intent_type: IntentType, user_id: str = None) -> Optional[BaseAgent]:
        """ì˜ë„ ìœ í˜•ì— ë”°ë¥¸ Worker ì—ì´ì „íŠ¸ ì„ íƒ (í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)"""
        # IntentTypeì„ TaskTypeìœ¼ë¡œ ë§¤í•‘ (í•˜ìœ„ í˜¸í™˜ì„±)
        task_type_mapping = {
            IntentType.WEB_SEARCH: TaskType.WEB_SEARCH,
            IntentType.DEEP_RESEARCH: TaskType.DEEP_RESEARCH,
            IntentType.CANVAS: TaskType.CANVAS,
            IntentType.GENERAL_CHAT: TaskType.GENERAL_CHAT,
            IntentType.MULTI_STEP: TaskType.WEB_SEARCH,  # ì„ì‹œë¡œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë§¤í•‘
            IntentType.CLARIFICATION: TaskType.GENERAL_CHAT  # ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ë§¤í•‘
        }
        
        mapped_task_type = task_type_mapping.get(intent_type, TaskType.GENERAL_CHAT)
        worker_or_selector = self.workers.get(mapped_task_type)
        
        if worker_or_selector:
            # í•¨ìˆ˜ì¸ ê²½ìš° (í•˜ì´ë¸Œë¦¬ë“œ ì„ íƒê¸°) ì‹¤í–‰
            if callable(worker_or_selector):
                return worker_or_selector(user_id)
            else:
                # ê¸°ì¡´ Worker ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ì¸ ê²½ìš°
                return worker_or_selector
        
        # í•´ë‹¹ Workerê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ Worker ì„ íƒ
        if intent_type in [IntentType.DEEP_RESEARCH, IntentType.MULTI_STEP]:
            # Deep Researchë‚˜ Multi-stepì´ ì—†ìœ¼ë©´ Web Searchë¡œ ëŒ€ì²´
            web_search_selector = self.workers.get(TaskType.WEB_SEARCH)
            if callable(web_search_selector):
                return web_search_selector(user_id)
            return web_search_selector
        elif intent_type == IntentType.CLARIFICATION:
            # ëª…í™•í™” ìš”ì²­ì€ ì¼ë°˜ ì±„íŒ…ìœ¼ë¡œ ì²˜ë¦¬
            return None  # ì§ì ‘ ì²˜ë¦¬
        
        return None
    
    def _emergency_fallback_classification(self, query: str) -> IntentType:
        """ê¸´ê¸‰ ìƒí™©ìš© ë‹¨ìˆœ ë¶„ë¥˜"""
        # Canvas í‚¤ì›Œë“œ (ê°€ì¥ ëª…í™•í•œ íŒ¨í„´)
        canvas_keywords = ["ê·¸ë ¤", "ë§Œë“¤ì–´", "ìƒì„±í•´", "ë””ìì¸", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”", "ì´ë¯¸ì§€"]
        if any(keyword in query for keyword in canvas_keywords) and "ì„¤ëª…" not in query:
            return IntentType.CANVAS
        
        # ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_keywords = ["ê²€ìƒ‰", "ì°¾ì•„", "ìµœì‹ ", "í˜„ì¬", "ì§€ê¸ˆ", "ì˜¤ëŠ˜", "ê°€ê²©", "ì–´ë””ì„œ", "ì–¸ì œ"]
        if any(keyword in query for keyword in search_keywords):
            return IntentType.WEB_SEARCH
        
        # ê¸°ë³¸ê°’: ì¼ë°˜ ëŒ€í™”
        return IntentType.GENERAL_CHAT
    
    async def _handle_directly(self, input_data: AgentInput, model: str, start_time: float, intent_type: str = "general_chat") -> AgentOutput:
        """Supervisorê°€ ì§ì ‘ ì²˜ë¦¬"""
        try:
            # ì˜ë„ ìœ í˜•ì— ë”°ë¥¸ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸
            if intent_type == "clarification":
                prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‹¤ì†Œ ëª¨í˜¸í•©ë‹ˆë‹¤: "{input_data.query}"

ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›Œ ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê²Œ ë‹¤ìŒê³¼ ê°™ì€ ë„ì›€ì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. ì§ˆë¬¸ì„ ë” ëª…í™•íˆ í•˜ëŠ” ë°©ë²• ì œì•ˆ
2. êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ìƒí™© ìš”ì²­  
3. ê´€ë ¨ëœ ëª‡ ê°€ì§€ ê°€ëŠ¥í•œ í•´ì„ ì œì‹œ

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            else:
                prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{input_data.query}"

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
í˜„ì¬ íŠ¹ë³„í•œ ë„êµ¬ë‚˜ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ, 
ê°€ëŠ¥í•œ í•œ ìœ ìš©í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result=response,
                metadata={
                    "handled_by": "supervisor_direct",
                    "intent_type": intent_type,
                    "reason": "no_suitable_worker_available"
                },
                execution_time_ms=execution_time,
                model_used=model
            )
            
        except Exception as e:
            self.logger.error(f"ì§ì ‘ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                metadata={"error": str(e)},
                execution_time_ms=execution_time,
                model_used=model
            )
    
    def _extract_original_query(self, query: str) -> str:
        """
        ëŒ€í™” ë§¥ë½ì´ ì¶”ê°€ëœ ì¿¼ë¦¬ì—ì„œ ì›ë³¸ ì§ˆë¬¸ë§Œ ì¶”ì¶œ
        ì˜ˆ: "ëŒ€í™” ê¸°ë¡:\nì–´ì‹œìŠ¤í„´íŠ¸: ê¹Œì¹˜ê°€ ë­ì•¼?\n\ní˜„ì¬ ì§ˆë¬¸: ê¹Œì¹˜ê°€ ë­ì•¼?" â†’ "ê¹Œì¹˜ê°€ ë­ì•¼?"
        """
        import re
        
        # "í˜„ì¬ ì§ˆë¬¸:" íŒ¨í„´ ì°¾ê¸°
        current_question_match = re.search(r'í˜„ì¬ ì§ˆë¬¸:\s*(.+?)(?:\n|$)', query, re.DOTALL)
        if current_question_match:
            extracted = current_question_match.group(1).strip()
            self.logger.info(f"ğŸ” ì›ë³¸ ì§ˆë¬¸ ì¶”ì¶œ ì„±ê³µ: '{extracted}'")
            return extracted
        
        # "USER:" íŒ¨í„´ ì°¾ê¸° (ë‹¤ë¥¸ í˜•ì‹ì˜ ê²½ìš°)
        user_message_match = re.search(r'USER:\s*(.+?)(?:\n|$)', query, re.MULTILINE)
        if user_message_match:
            extracted = user_message_match.group(1).strip()
            self.logger.info(f"ğŸ” USER íŒ¨í„´ ì¶”ì¶œ ì„±ê³µ: '{extracted}'")
            return extracted
        
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        self.logger.info(f"ğŸ” ì›ë³¸ ì§ˆë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - ì „ì²´ ì¿¼ë¦¬ ì‚¬ìš©")
        return query.strip()
    
    async def _is_simple_question(self, query: str) -> bool:
        """
        ìˆœìˆ˜ LLM ê¸°ë°˜ ê°„ë‹¨ ì§ˆë¬¸ ê°ì§€ ì‹œìŠ¤í…œ (íŒ¨í„´ ë§¤ì¹­ ì™„ì „ ì œê±°)
        Supervisorê°€ ì§ˆë¬¸ ì˜ë„ë¥¼ ì§ì ‘ íŒë‹¨í•˜ì—¬ ê°„ë‹¨í•œ ì§ˆë¬¸ ì—¬ë¶€ ê²°ì •
        """
        self.logger.info(f"ğŸ§  LLM ê¸°ë°˜ ì˜ë„ íŒë‹¨ ì‹œì‘: '{query}'")
        
        validation_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ "ê°„ë‹¨í•œ íŒ©íŠ¸ ì§ˆë¬¸"ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ê°„ë‹¨í•œ íŒ©íŠ¸ ì§ˆë¬¸ì˜ ì¡°ê±´:
- ë‹¨ìˆœí•œ ì •ì˜, ì„¤ëª…, ê°œë…ì— ëŒ€í•œ ì§ˆë¬¸ 
- "~ê°€ ë­ì•¼?", "~ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜", "~ë€ ë¬´ì—‡ì¸ê°€?" ë“±
- ì¼ë°˜ ìƒì‹ì´ë‚˜ ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ ë°”ë¡œ ë‹µë³€ ê°€ëŠ¥
- ì›¹ ê²€ìƒ‰, ë³µì¡í•œ ë¶„ì„, ê³„ì‚°, ìƒì„± ì‘ì—…ì´ ë¶ˆí•„ìš”

ë‹µë³€: ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë©´ "SIMPLE", ë³µì¡í•œ ì§ˆë¬¸ì´ë©´ "COMPLEX"ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

        try:
            # ë¹ ë¥¸ ëª¨ë¸ë¡œ ì˜ë„ íŒë‹¨
            response, _ = await llm_router.generate_response("gemini-flash", validation_prompt)
            is_simple = "SIMPLE" in response.upper()
            
            self.logger.info(f"ğŸ§  LLM ì˜ë„ íŒë‹¨ ê²°ê³¼: {'SIMPLE' if is_simple else 'COMPLEX'}")
            self.logger.info(f"ğŸ§  LLM ì‘ë‹µ ìƒì„¸: {response[:100]}")
            
            return is_simple
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ LLM ê²€ì¦ ì‹¤íŒ¨: {e} - ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬")
            return False  # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
    
    async def _handle_simple_question_fast(self, input_data: AgentInput, model: str, start_time: float) -> AgentOutput:
        """ê°„ë‹¨í•œ ì§ˆë¬¸ì„ ìœ„í•œ ê³ ì† ì²˜ë¦¬ ê²½ë¡œ (ì˜ë„ ë¶„ë¥˜ ìš°íšŒ)"""
        try:
            self.logger.info(f"ğŸƒâ€â™‚ï¸ Fast Path ì‹¤í–‰: {input_data.query}")
            
            # ê°„ë‹¨í•˜ê³  ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
            prompt = f"""ì§ˆë¬¸: "{input_data.query}"

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨ëª…ë£Œí•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ê¸°ë³¸ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            # LLM ì‘ë‹µ ìƒì„± (ë³µì¡í•œ ë¶„ì„ ë‹¨ê³„ ì™„ì „ ìš°íšŒ)
            response, _ = await llm_router.generate_response(model, prompt)
            execution_time = int((time.time() - start_time) * 1000)
            
            self.logger.info(f"âš¡ Fast Path ì™„ë£Œ: {execution_time}ms (ê¸°ì¡´ 25ì´ˆ â†’ {execution_time/1000:.1f}ì´ˆ)")
            
            return self.create_output(
                result=response,
                metadata={
                    "handled_by": "supervisor_fast_path",
                    "optimization": "intent_classification_bypassed",
                    "method": "pure_llm_simple_question_detection",
                    "performance_gain": f"~95% faster ({execution_time}ms vs ~25000ms)",
                    "routing_version": "fast_path_v2_pure_llm"
                },
                execution_time_ms=execution_time,
                model_used=model
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Fast Path ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            # Fast Path ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ê²½ë¡œë¡œ í´ë°±
            self.logger.info("ğŸ”„ Fast Path ì‹¤íŒ¨ - ì¼ë°˜ ì²˜ë¦¬ ê²½ë¡œë¡œ í´ë°±")
            return await self._handle_directly(input_data, model, start_time, "general_chat")
    
    async def _handle_complex_question(
        self, 
        input_data: AgentInput, 
        model: str, 
        classification_result: IntentClassificationResult, 
        start_time: float, 
        progress_callback=None
    ) -> AgentOutput:
        """ë³µì¡í•œ ì§ˆë¬¸ ì²˜ë¦¬ - ê¸°ì¡´ LangGraph ì‹œìŠ¤í…œ í™œìš©"""
        
        self.logger.info(f"ğŸ”„ ë³µì¡ ì²˜ë¦¬ ëª¨ë“œ: {classification_result.intent_type.value}")
        
        if progress_callback:
            await progress_callback({
                "step": "complex_analysis",
                "message": "ë³µì¡í•œ ë¶„ì„ ìˆ˜í–‰ ì¤‘...",
                "progress": 20
            })
        
        # NewIntentTypeì„ ê¸°ì¡´ IntentTypeìœ¼ë¡œ ë§¤í•‘
        intent_mapping = {
            NewIntentType.WEB_SEARCH: IntentType.WEB_SEARCH,
            NewIntentType.REASONING: IntentType.DEEP_RESEARCH,
            NewIntentType.CANVAS: IntentType.CANVAS,
            NewIntentType.COMPLEX: IntentType.MULTI_STEP
        }
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ì˜ë„ ìœ í˜•
        legacy_intent = intent_mapping.get(classification_result.intent_type, IntentType.GENERAL_CHAT)
        
        # ê¸°ì¡´ ëŒ€í™” ë§¥ë½ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        if not input_data.conversation_context and input_data.context:
            conversation_context_data = input_data.context.get('conversation_context', {})
            if conversation_context_data:
                from app.agents.base import ConversationContext
                input_data.conversation_context = ConversationContext(**conversation_context_data)
                self.logger.info(f"ğŸ” ëŒ€í™” ë§¥ë½ ë¡œë“œ: ì£¼ì œ={input_data.conversation_context.current_focus_topic}")
        
        # Worker ì—ì´ì „íŠ¸ ì„ íƒ ë° ì‹¤í–‰
        worker_agent = self._select_worker(legacy_intent, input_data.user_id)
        
        if worker_agent:
            self.logger.info(f"ğŸš€ ì‘ì—… ìœ„ì„: {legacy_intent.value} â†’ {worker_agent.agent_id}")
            
            if progress_callback:
                await progress_callback({
                    "step": "delegating_to_worker",
                    "message": f"{worker_agent.name}ì—ê²Œ ì‘ì—… ìœ„ì„ ì¤‘...",
                    "progress": 40
                })
            
            # Worker ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await worker_agent.execute(input_data, model, progress_callback)
            
            # ë©”íƒ€ë°ì´í„° ê°•í™”
            result.metadata.update({
                "supervisor_decision": legacy_intent.value,
                "delegated_to": worker_agent.agent_id,
                "classification_confidence": classification_result.confidence,
                "classification_stage": classification_result.classification_stage,
                "intent_classification_time_ms": classification_result.processing_time_ms,
                "routing_version": "v3_hybrid_fast_path",
                "needs_web_search": classification_result.needs_web_search,
                "needs_reasoning": classification_result.needs_reasoning,
                "needs_canvas": classification_result.needs_canvas
            })
            
            return result
        else:
            # Workerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì²˜ë¦¬
            self.logger.info(f"ğŸ¤– ì§ì ‘ ì²˜ë¦¬: {legacy_intent.value} (í•´ë‹¹ Worker ì—†ìŒ)")
            return await self._handle_directly(input_data, model, start_time, legacy_intent.value)
    
    def get_capabilities(self) -> list[str]:
        """Supervisor ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "âš¡ Fast Path ê°„ë‹¨ ì§ˆë¬¸ ì²˜ë¦¬ (25ì´ˆâ†’5ì´ˆ ìµœì í™”)",
            "ì§€ëŠ¥í˜• ì˜ë„ ë¶„ë¥˜",
            "ë§¥ë½ ì¸ì‹ ë¼ìš°íŒ…",
            "ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ê¸°",
            "Worker ì—ì´ì „íŠ¸ ê´€ë¦¬",
            "ì‹¤ì‹œê°„ ì„±ëŠ¥ ìµœì í™”",
            "ê¸´ê¸‰ ìƒí™© ì²˜ë¦¬"
        ]
    
    def get_supported_models(self) -> list[str]:
        """ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡"""
        return ["gemini", "claude", "openai"]
    
    def get_available_workers(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Worker ì—ì´ì „íŠ¸ ëª©ë¡"""
        return {
            task_type.value: worker.name 
            for task_type, worker in self.workers.items()
        }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ë¼ìš°íŒ… ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
        try:
            classifier_report = dynamic_intent_classifier.get_performance_report()
            return {
                "routing_version": "v2_intelligent",
                "status": "active",
                "classifier_performance": classifier_report,
                "available_workers": len(self.workers),
                "supported_intents": [intent.value for intent in IntentType],
                "capabilities": self.get_capabilities()
            }
        except Exception as e:
            return {
                "routing_version": "v2_intelligent",
                "status": "error",
                "error": str(e),
                "fallback_available": True
            }
    
    async def record_user_correction(self, user_id: str, original_intent: str, correct_intent: str, query: str):
        """ì‚¬ìš©ì ìˆ˜ì • ì‚¬í•­ ê¸°ë¡ (í•™ìŠµ í–¥ìƒìš©)"""
        try:
            await dynamic_intent_classifier.record_correction(user_id, original_intent, correct_intent, query)
            self.logger.info(f"âœ… ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡ ì™„ë£Œ: {original_intent} â†’ {correct_intent}")
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    async def _handle_multi_step_task(
        self, 
        input_data: AgentInput, 
        model: str, 
        start_time: float, 
        reasoning: str,
        progress_callback=None
    ) -> AgentOutput:
        """ë³µí•© ì‘ì—… ì²˜ë¦¬ - ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì‘ì—…ì„ ìˆœì°¨ ì‹¤í–‰"""
        try:
            self.logger.info(f"ğŸ”— ë³µí•© ì‘ì—… ë¶„í•´ ì‹œì‘ - ì¿¼ë¦¬: {input_data.query}")
            
            # 1ë‹¨ê³„: ë³µí•© ì‘ì—…ì„ ê°œë³„ ë‹¨ê³„ë¡œ ë¶„í•´
            task_breakdown = await self._decompose_multi_step_task(input_data.query, model)
            
            if not task_breakdown or len(task_breakdown) < 2:
                # ë¶„í•´ ì‹¤íŒ¨ ì‹œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ fallback
                self.logger.warning("ë³µí•© ì‘ì—… ë¶„í•´ ì‹¤íŒ¨ - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬")
                worker_agent = self.workers.get(TaskType.WEB_SEARCH)
                if worker_agent:
                    return await worker_agent.execute(input_data, model, progress_callback)
                else:
                    return await self._handle_directly(input_data, model, start_time, "multi_step_fallback")
            
            self.logger.info(f"ğŸ“‹ ì‘ì—… ë¶„í•´ ì™„ë£Œ - {len(task_breakdown)}ê°œ ë‹¨ê³„: {[step['action'] for step in task_breakdown]}")
            
            # 2ë‹¨ê³„: ìˆœì°¨ì ìœ¼ë¡œ ê° ë‹¨ê³„ ì‹¤í–‰
            accumulated_results = []
            current_context = input_data.context or {}
            
            for i, step in enumerate(task_breakdown):
                step_number = i + 1
                total_steps = len(task_breakdown)
                
                if progress_callback:
                    await progress_callback({
                        "step": f"multi_step_{step_number}",
                        "message": f"ë‹¨ê³„ {step_number}/{total_steps}: {step['description']}",
                        "progress": 30 + (50 * step_number // total_steps)
                    })
                
                self.logger.info(f"ğŸ”„ ë‹¨ê³„ {step_number}/{total_steps} ì‹¤í–‰: {step['action']} - {step['description']}")
                
                # ê° ë‹¨ê³„ì— ë§ëŠ” Worker ì„ íƒ
                step_intent = IntentType(step['action'])
                worker_agent = self._select_worker(step_intent)
                
                if worker_agent:
                    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                    if accumulated_results:
                        current_context['previous_step_results'] = accumulated_results
                    
                    # ë‹¨ê³„ë³„ ì…ë ¥ ë°ì´í„° êµ¬ì„±
                    step_input = AgentInput(
                        query=step['query'],
                        user_id=input_data.user_id,
                        session_id=input_data.session_id,
                        context=current_context,
                        conversation_context=input_data.conversation_context
                    )
                    
                    # ë‹¨ê³„ ì‹¤í–‰
                    step_result = await worker_agent.execute(step_input, model)
                    
                    accumulated_results.append({
                        "step": step_number,
                        "action": step['action'],
                        "description": step['description'],
                        "query": step['query'],
                        "result": step_result.result,
                        "metadata": step_result.metadata,
                        "execution_time_ms": step_result.execution_time_ms
                    })
                    
                    self.logger.info(f"âœ… ë‹¨ê³„ {step_number} ì™„ë£Œ - {step['action']}")
                else:
                    # Workerê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì²˜ë¦¬
                    self.logger.warning(f"âš ï¸ ë‹¨ê³„ {step_number} Worker ì—†ìŒ - ì§ì ‘ ì²˜ë¦¬")
                    accumulated_results.append({
                        "step": step_number,
                        "action": step['action'],
                        "description": step['description'],
                        "query": step['query'],
                        "result": f"ë‹¨ê³„ {step_number} ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ìš© ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "metadata": {"error": "no_worker_available"},
                        "execution_time_ms": 0
                    })
            
            # 3ë‹¨ê³„: ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = await self._synthesize_multi_step_results(
                input_data.query, accumulated_results, model
            )
            
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result=final_response,
                metadata={
                    "supervisor_decision": "multi_step",
                    "routing_version": "v2_intelligent",
                    "multi_step_breakdown": task_breakdown,
                    "steps_completed": len(accumulated_results),
                    "step_results": accumulated_results
                },
                execution_time_ms=execution_time,
                model_used=model
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ë³µí•© ì‘ì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            execution_time = int((time.time() - start_time) * 1000)
            
            return self.create_output(
                result="ì£„ì†¡í•©ë‹ˆë‹¤. ë³µí•© ì‘ì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                metadata={
                    "supervisor_decision": "multi_step_error",
                    "error": str(e)
                },
                execution_time_ms=execution_time,
                model_used=model
            )
    
    async def _decompose_multi_step_task(self, query: str, model: str) -> List[Dict[str, str]]:
        """ë³µí•© ì‘ì—…ì„ ê°œë³„ ë‹¨ê³„ë¡œ ë¶„í•´"""
        try:
            prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•  ë‹¨ê³„ë“¤ë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ê° ë‹¨ê³„ëŠ” ë‹¤ìŒ ì‘ì—… ìœ í˜• ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:
- web_search: ì¸í„°ë„· ê²€ìƒ‰
- deep_research: ì‹¬ì¸µ ë¶„ì„  
- canvas: ì‹œê°ì  ì°½ì‘
- general_chat: ì¼ë°˜ ëŒ€í™”

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
[
  {{
    "action": "web_search|deep_research|canvas|general_chat",
    "description": "ì´ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰í•  ì‘ì—… ì„¤ëª…",
    "query": "ì‹¤ì œë¡œ ì‹¤í–‰í•  êµ¬ì²´ì  ì§ˆë¬¸"
  }}
]

ì˜ˆì‹œ:
ì§ˆë¬¸: "ìµœì‹  ìŠ¤ë§ˆíŠ¸í°ì„ ì°¾ì•„ì„œ ì¥ë‹¨ì  ë¹„êµí•´ì¤˜"
ì‘ë‹µ:
[
  {{
    "action": "web_search",
    "description": "ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ ê²€ìƒ‰",
    "query": "2025ë…„ ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ ë¦¬ìŠ¤íŠ¸"
  }},
  {{
    "action": "deep_research", 
    "description": "ì°¾ì€ ìŠ¤ë§ˆíŠ¸í°ë“¤ì˜ ì¥ë‹¨ì  ë¶„ì„",
    "query": "ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ëª¨ë¸ë“¤ì˜ ìƒì„¸ ë¹„êµ ë¶„ì„"
  }}
]

JSONë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
            
            response, _ = await llm_router.generate_response(model, prompt, include_datetime=False)
            
            # JSON íŒŒì‹±
            clean_response = response.strip()
            if clean_response.startswith('```json'):
                clean_response = clean_response[7:]
            if clean_response.endswith('```'):
                clean_response = clean_response[:-3]
            
            task_breakdown = json.loads(clean_response.strip())
            
            # ìœ íš¨ì„± ê²€ì¦
            valid_actions = {"web_search", "deep_research", "canvas", "general_chat"}
            filtered_breakdown = []
            
            for step in task_breakdown:
                if (isinstance(step, dict) and 
                    "action" in step and 
                    "description" in step and 
                    "query" in step and
                    step["action"] in valid_actions):
                    filtered_breakdown.append(step)
            
            return filtered_breakdown if len(filtered_breakdown) >= 2 else []
            
        except Exception as e:
            self.logger.error(f"ë³µí•© ì‘ì—… ë¶„í•´ ì‹¤íŒ¨: {e}")
            return []
    
    async def _synthesize_multi_step_results(self, original_query: str, step_results: List[Dict], model: str) -> str:
        """ì—¬ëŸ¬ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±"""
        try:
            # ê° ë‹¨ê³„ ê²°ê³¼ ìš”ì•½
            results_summary = []
            for result in step_results:
                summary = f"**ë‹¨ê³„ {result['step']}: {result['description']}**\n"
                summary += f"ê²°ê³¼: {result['result'][:200]}{'...' if len(result['result']) > 200 else ''}\n"
                results_summary.append(summary)
            
            prompt = f"""
ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ì–»ì€ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•˜ê³  ìœ ìš©í•œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{original_query}"

ë‹¨ê³„ë³„ ê²°ê³¼:
{chr(10).join(results_summary)}

ë‹¤ìŒ ì›ì¹™ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ ì œê³µ
2. ë…¼ë¦¬ì  íë¦„ìœ¼ë¡œ ì •ë³´ë¥¼ êµ¬ì„±
3. ì‚¬ìš©ìê°€ ì›í–ˆë˜ ì •ë³´ë¥¼ ëª…í™•íˆ ì „ë‹¬
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
5. í•„ìš”ì‹œ ìš”ì•½, ê²°ë¡ , ì¶”ì²œ ì‚¬í•­ í¬í•¨

ìµœì¢… ë‹µë³€:
"""
            
            response, _ = await llm_router.generate_response(model, prompt, include_datetime=False)
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì¢…í•© ì‹¤íŒ¨: {e}")
            # Fallback: ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ë‹¨ìˆœ ë‚˜ì—´
            fallback_response = f"ì›ë³¸ ì§ˆë¬¸: {original_query}\n\n"
            for result in step_results:
                fallback_response += f"**{result['description']}:**\n{result['result']}\n\n"
            return fallback_response
    
    async def analyze_and_suggest_agent(self, query: str, current_agent: str, model: str = "gemini") -> Dict[str, Any]:
        """í˜„ì¬ ì—ì´ì „íŠ¸ì™€ ë‹¤ë¥¸ ë” ì í•©í•œ ì—ì´ì „íŠ¸ë¥¼ ì œì•ˆ"""
        try:
            # ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„ (ì •ë³´ ë¶„ì„ ì—†ì´ ë°”ë¡œ ì‘ì—… ìœ í˜• ë¶„ì„)
            suggested_task_type = await self._analyze_task_type_direct(query, model)
            suggested_agent = suggested_task_type.value
            
            # í˜„ì¬ ì—ì´ì „íŠ¸ì™€ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì œì•ˆ
            if suggested_agent != current_agent and suggested_agent != "general_chat":
                # ìƒì„¸ ë¶„ì„ìœ¼ë¡œ ì‹ ë¢°ë„ ë° ì´ìœ  ìƒì„±
                confidence, reason = await self._analyze_suggestion_details(
                    query, current_agent, suggested_agent, model
                )
                
                return {
                    "needs_switch": True,
                    "suggested_agent": suggested_agent,
                    "confidence": confidence,
                    "reason": reason,
                    "current_agent": current_agent
                }
            
            return {"needs_switch": False}
            
        except Exception as e:
            self.logger.error(f"ì—ì´ì „íŠ¸ ì œì•ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"needs_switch": False, "error": str(e)}
    
    async def _analyze_suggestion_details(self, query: str, current_agent: str, suggested_agent: str, model: str) -> tuple[float, str]:
        """ì œì•ˆ ìƒì„¸ ë¶„ì„ - ì‹ ë¢°ë„ì™€ ì´ìœ  ìƒì„±"""
        try:
            agent_descriptions = {
                "none": "ì¼ë°˜ ì±„íŒ…",
                "web_search": "ì›¹ ê²€ìƒ‰ì„ í†µí•œ ìµœì‹  ì •ë³´ ì¡°íšŒ",
                "deep_research": "ì‹¬ì¸µì ì¸ ë¶„ì„ê³¼ ì—°êµ¬",
                "canvas": "ì´ë¯¸ì§€ ìƒì„±, ë§ˆì¸ë“œë§µ, ì‹œê°ì  ì°½ì‘",
                "multimodal_rag": "ë¬¸ì„œ ë° ì´ë¯¸ì§€ ë¶„ì„"
            }
            
            prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì—ì´ì „íŠ¸ ì „í™˜ì´ í•„ìš”í•œ ì´ìœ ì™€ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"
í˜„ì¬ ì—ì´ì „íŠ¸: {agent_descriptions.get(current_agent, current_agent)}
ì œì•ˆ ì—ì´ì „íŠ¸: {agent_descriptions.get(suggested_agent, suggested_agent)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ì‹ ë¢°ë„: [0.1-1.0 ì‚¬ì´ì˜ ìˆ«ì]
ì´ìœ : [í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ]

ì˜ˆì‹œ:
ì‹ ë¢°ë„: 0.9
ì´ìœ : ìµœì‹  ì£¼ê°€ ì •ë³´ ì¡°íšŒëŠ” ì›¹ ê²€ìƒ‰ì´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
"""
            
            response, _ = await llm_router.generate_response(model, prompt)
            lines = response.strip().split('\n')
            
            confidence = 0.7  # ê¸°ë³¸ê°’
            reason = f"{agent_descriptions.get(suggested_agent, suggested_agent)}ê°€ ì´ ì‘ì—…ì— ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            
            for line in lines:
                if line.startswith('ì‹ ë¢°ë„:'):
                    try:
                        confidence = float(line.split(':')[1].strip())
                        confidence = max(0.1, min(1.0, confidence))  # ë²”ìœ„ ì œí•œ
                    except:
                        pass
                elif line.startswith('ì´ìœ :'):
                    reason = line.split(':', 1)[1].strip()
            
            return confidence, reason
            
        except Exception as e:
            self.logger.warning(f"ì œì•ˆ ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.7, f"{suggested_agent.replace('_', ' ')}ì´ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"


# Supervisor ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
supervisor_agent = SupervisorAgent()