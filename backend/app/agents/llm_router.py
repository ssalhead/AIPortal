"""
LLM ëª¨ë¸ ë¼ìš°í„°
"""

from typing import Optional, Dict, Any, AsyncGenerator, List
from langchain_aws import ChatBedrock
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models import BaseLanguageModel
import boto3
from datetime import datetime

from app.core.config import settings
from app.agents.mock_llm import mock_llm
from app.services.logging_service import logging_service, log_llm_usage
from app.utils.logger import get_logger

logger = get_logger(__name__)


class LLMRouter:
    """LLM ëª¨ë¸ ë¼ìš°í„° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._models: Dict[str, BaseLanguageModel] = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ì„ ì´ˆê¸°í™”"""
        try:
            # AWS Bedrock Claude ëª¨ë¸ ì´ˆê¸°í™”
            if settings.AWS_ACCESS_KEY_ID and settings.AWS_SECRET_ACCESS_KEY:
                try:
                    # Bedrock í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    bedrock_client = boto3.client(
                        service_name="bedrock-runtime",
                        region_name=settings.AWS_REGION,
                        aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
                        aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
                    )
                    
                    # Claude Sonnet 4.0 (ìµœì‹  ëª¨ë¸) - inference profile ì‚¬ìš©
                    self._models["claude-4"] = ChatBedrock(
                        client=bedrock_client,
                        model_id="us.anthropic.claude-sonnet-4-20250514-v1:0",
                        model_kwargs={
                            "temperature": 0.7,
                            "max_tokens": 4096,
                            "top_p": 0.9,
                        }
                    )
                    logger.debug("AWS Bedrock Claude 4.0 Sonnet ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Claude 3.7 Sonnet - inference profile ì‚¬ìš©
                    self._models["claude-3.7"] = ChatBedrock(
                        client=bedrock_client,
                        model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                        model_kwargs={
                            "temperature": 0.7,
                            "max_tokens": 4096,
                            "top_p": 0.9,
                        }
                    )
                    logger.debug("AWS Bedrock Claude 3.7 Sonnet ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Claude 3.5 Sonnet (ê¸°ë³¸ ëª¨ë¸)
                    self._models["claude"] = ChatBedrock(
                        client=bedrock_client,
                        model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
                        model_kwargs={
                            "temperature": 0.7,
                            "max_tokens": 4096,
                            "top_p": 0.9,
                        }
                    )
                    logger.debug("AWS Bedrock Claude 3.5 Sonnet ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Claude 3.5 Sonnet (ë³„ì¹­ - ëª…ì‹œì  ë²„ì „)
                    self._models["claude-3.5"] = ChatBedrock(
                        client=bedrock_client,
                        model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
                        model_kwargs={
                            "temperature": 0.7,
                            "max_tokens": 4096,
                            "top_p": 0.9,
                        }
                    )
                    logger.debug("AWS Bedrock Claude 3.5 Sonnet (ë³„ì¹­) ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Claude 3.5 Haiku (ë¹ ë¥¸ ì‘ë‹µìš©)
                    self._models["claude-haiku"] = ChatBedrock(
                        client=bedrock_client,
                        model_id="anthropic.claude-3-haiku-20240307-v1:0",
                        model_kwargs={
                            "temperature": 0.7,
                            "max_tokens": 4096,
                        }
                    )
                    logger.debug("AWS Bedrock Claude 3.5 Haiku ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"AWS Bedrock ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            else:
                logger.warning("AWS ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - Claude ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")
            
            # GCP Gemini ëª¨ë¸ ì´ˆê¸°í™”
            if settings.GOOGLE_API_KEY:
                try:
                    # Gemini Pro 1.5
                    self._models["gemini-pro"] = ChatGoogleGenerativeAI(
                        model="gemini-1.5-pro",
                        google_api_key=settings.GOOGLE_API_KEY,
                        temperature=0.7,
                        max_output_tokens=8192,
                        top_p=0.9,
                    )
                    logger.debug("Google Gemini 1.5 Pro ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Gemini Flash (ë” ë¹ ë¥¸ ì‘ë‹µìš©)
                    self._models["gemini-flash"] = ChatGoogleGenerativeAI(
                        model="gemini-1.5-flash",
                        google_api_key=settings.GOOGLE_API_KEY,
                        temperature=0.7,
                        max_output_tokens=8192,
                    )
                    logger.debug("Google Gemini 1.5 Flash ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Gemini 1.0 Pro (ê¸°ë³¸ ëª¨ë¸)
                    self._models["gemini-1.0"] = ChatGoogleGenerativeAI(
                        model="gemini-1.0-pro",
                        google_api_key=settings.GOOGLE_API_KEY,
                        temperature=0.7,
                        max_output_tokens=8192,
                    )
                    logger.debug("Google Gemini 1.0 Pro ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # Gemini ê¸°ë³¸ ë³„ì¹­ (gemini-proë¡œ ë§¤í•‘)
                    self._models["gemini"] = self._models["gemini-pro"]
                    logger.debug("Google Gemini ê¸°ë³¸ ë³„ì¹­ ì„¤ì • ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"Google Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            else:
                logger.warning("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - Gemini ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def get_model(self, model_name: str) -> Optional[BaseLanguageModel]:
        """
        ì§€ì •ëœ ëª¨ë¸ ë°˜í™˜
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„ (claude, claude-haiku, gemini, gemini-flash)
            
        Returns:
            ì–¸ì–´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        model = self._models.get(model_name.lower())
        if model is None:
            logger.warning(f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ëª¨ë¸ë¡œ fallback
            return self.get_fallback_model()
        return model
    
    def get_fallback_model(self) -> Optional[BaseLanguageModel]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ fallback ëª¨ë¸ ë°˜í™˜
        
        Returns:
            ì–¸ì–´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        # ìš°ì„ ìˆœìœ„: gemini-pro > gemini-flash > gemini-1.0 > claude-4 > claude-3.7 > claude-3.5 > claude > claude-haiku (Gemini ìš°ì„  - ì•ˆì •ì„±)
        for model_name in ["gemini-pro", "gemini-flash", "gemini-1.0", "claude-4", "claude-3.7", "claude-3.5", "claude", "claude-haiku"]:
            if model_name in self._models:
                logger.debug(f"Fallback ëª¨ë¸ë¡œ {model_name} ì‚¬ìš©")
                return self._models[model_name]
        
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŒ")
        return None
    
    def get_available_models(self) -> list[str]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜
        
        Returns:
            ëª¨ë¸ ì´ë¦„ ëª©ë¡
        """
        return list(self._models.keys())
    
    def is_model_available(self, model_name: str) -> bool:
        """
        ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            
        Returns:
            ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        """
        return model_name.lower() in self._models

    def get_optimal_model(self, task_type: str = "general", context_length: int = 0) -> str:
        """
        ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ
        
        Args:
            task_type: ì‘ì—… ìœ í˜• (general, reasoning, speed, creative, coding)
            context_length: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            
        Returns:
            ìµœì  ëª¨ë¸ ì´ë¦„
        """
        available_models = self.get_available_models()
        
        if not available_models:
            return "mock-general"
        
        # ì‘ì—… ìœ í˜•ë³„ ëª¨ë¸ ìš°ì„ ìˆœìœ„ (Claude 4.0 > 3.7 > 3.5 > Gemini Pro ìˆœì„œ)
        model_preferences = {
            "reasoning": ["claude-4", "claude-3.7", "claude-3.5", "claude", "gemini-pro", "claude-haiku", "gemini-flash", "gemini-1.0"],
            "creative": ["claude-4", "claude-3.7", "claude-3.5", "claude", "gemini-pro", "gemini-flash", "claude-haiku", "gemini-1.0"],
            "coding": ["claude-4", "claude-3.7", "claude-3.5", "claude", "gemini-pro", "claude-haiku", "gemini-flash", "gemini-1.0"],
            "speed": ["claude-haiku", "gemini-flash", "gemini-pro", "claude-3.5", "claude", "claude-3.7", "claude-4", "gemini-1.0"],
            "general": ["claude-4", "claude-3.7", "claude-3.5", "claude", "gemini-pro", "claude-haiku", "gemini-flash", "gemini-1.0"]
        }
        
        preferred_models = model_preferences.get(task_type, model_preferences["general"])
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ì—ì„œ ì²« ë²ˆì§¸ ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì„ íƒ
        for model in preferred_models:
            if model in available_models:
                logger.debug(f"ì‘ì—… ìœ í˜• '{task_type}'ì— ëŒ€í•´ '{model}' ëª¨ë¸ ì„ íƒ")
                return model
        
        # fallback
        return available_models[0] if available_models else "mock-general"

    def is_mock_mode(self) -> bool:
        """Mock ëª¨ë“œ ì—¬ë¶€ í™•ì¸"""
        return (
            not any([
                settings.GOOGLE_API_KEY, 
                (settings.AWS_ACCESS_KEY_ID and settings.AWS_SECRET_ACCESS_KEY)
            ]) or
            getattr(settings, 'MOCK_LLM_ENABLED', False)
        )

    def _add_datetime_context(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ì— í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ì •ë³´ ì¶”ê°€"""
        from app.utils.timezone import now_kst
        
        current_datetime = now_kst()
        date_context = f"""
[í˜„ì¬ ì‹œê°„ ì •ë³´]
- í˜„ì¬ ë‚ ì§œ/ì‹œê°„: {current_datetime.strftime('%Yë…„ %mì›” %dì¼ (%A) %Hì‹œ %Më¶„')} (í•œêµ­ ì‹œê°„, KST)
- ì˜¤ëŠ˜: {current_datetime.strftime('%Y-%m-%d')}
- í˜„ì¬ ì—°ë„: {current_datetime.year}ë…„
- í˜„ì¬ ì›”: {current_datetime.month}ì›”
- í˜„ì¬ ìš”ì¼: {current_datetime.strftime('%A')} (í•œêµ­ì–´: {['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼'][current_datetime.weekday()]})

ìœ„ ì‹œê°„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ "ì˜¤ëŠ˜", "í˜„ì¬", "ìµœê·¼" ë“±ì˜ ì‹œê°„ í‘œí˜„ì´ í¬í•¨ëœ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸:
{prompt}"""
        
        return date_context

    @log_llm_usage
    async def generate_response(
        self, 
        model_name: str, 
        prompt: str, 
        user_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        include_datetime: bool = True,
        **kwargs
    ) -> tuple[str, str]:
        """
        ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            prompt: í”„ë¡¬í”„íŠ¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            conversation_id: ëŒ€í™” ID (ë¡œê¹…ìš©)
            include_datetime: ë‚ ì§œ/ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            (ì‘ë‹µ í…ìŠ¤íŠ¸, ì‹¤ì œ ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„)
        """
        # ë‚ ì§œ/ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì œëª© ìƒì„± ë“± íŠ¹ë³„í•œ ê²½ìš° ì œì™¸)
        final_prompt = self._add_datetime_context(prompt) if include_datetime else prompt
        
        # Mock ëª¨ë“œ í™•ì¸
        if self.is_mock_mode():
            logger.debug_performance("Mock ëª¨ë“œ ì‘ë‹µ ìƒì„±", {"model": model_name})
            mock_model_name = f"mock-{model_name.lower()}"
            response = mock_llm.generate_response(final_prompt, mock_model_name)
            return response, mock_model_name
        
        model = self.get_model(model_name)
        if model is None:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        try:
            # ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ
            response = await model.ainvoke(final_prompt)
            return response.content, model_name
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ '{model_name}' ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            
            # Mock ëª¨ë“œë¡œ fallback
            logger.warning(f"ì‹¤ì œ API í˜¸ì¶œ ì‹¤íŒ¨, Mock ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´: {e}")
            mock_model_name = f"mock-{model_name.lower()}-fallback"
            response = mock_llm.generate_response(
                final_prompt, 
                f"{mock_model_name} (API ì˜¤ë¥˜ë¡œ ì¸í•œ Mock ì‘ë‹µ)"
            )
            return response, mock_model_name

    async def generate_response_with_context(
        self,
        message: str,
        model: str,
        agent_type: str,
        user_id: str,
        session_id: Optional[str] = None,
        stream: bool = False
    ):
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            model: ì‚¬ìš©í•  ëª¨ë¸
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì…
            user_id: ì‚¬ìš©ì ID
            session_id: ì„¸ì…˜ ID (ì»¨í…ìŠ¤íŠ¸ ë¡œë”©ìš©)
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            
        Returns:
            ì‘ë‹µ ê°ì²´
        """
        try:
            # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            final_prompt = await self._build_prompt_with_context(
                message=message,
                session_id=session_id,
                user_id=user_id,
                agent_type=agent_type
            )
            
            # ê¸°ì¡´ generate_response ë©”ì„œë“œ í˜¸ì¶œ
            response_content, used_model = await self.generate_response(
                model_name=model,
                prompt=final_prompt,
                user_id=user_id,
                conversation_id=session_id
            )
            
            # ì‘ë‹µ ê°ì²´ êµ¬ì„±
            from app.models.citation import CitedResponse
            
            return CitedResponse(
                response_text=response_content,
                sources=[],
                total_sources=0,
                citation_count=0,
                citations=[]
            )
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì¬ì‹œë„
            response_content, used_model = await self.generate_response(
                model_name=model,
                prompt=message,
                user_id=user_id,
                conversation_id=session_id
            )
            
            from app.models.citation import CitedResponse
            
            return CitedResponse(
                response_text=response_content,
                sources=[],
                total_sources=0,
                citation_count=0,
                citations=[]
            )
    
    async def _build_prompt_with_context(
        self,
        message: str,
        session_id: Optional[str],
        user_id: str,
        agent_type: str
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰ (ì œëª© ìƒì„± ì‹œì—ëŠ” session_idê°€ None)
        if not session_id:
            return message
        
        try:
            from app.services.conversation_memory_service import conversation_memory_service
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
            context_data = await conversation_memory_service.get_conversation_context(
                conversation_id=session_id,
                user_id=user_id
            )
            
            context_prompt = context_data.get('context_prompt', '')
            total_tokens = context_data.get('total_tokens', 0)
            
            # í† í° ì œí•œ í™•ì¸
            if total_tokens > 3000:  # í† í° ì œí•œ ì´ˆê³¼ì‹œ ì»¨í…ìŠ¤íŠ¸ ë‹¨ì¶•
                logger.warning(f"ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ ì´ˆê³¼: {total_tokens}, ë‹¨ì¶• ì²˜ë¦¬")
                # ë‹¨ê¸°ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš©
                short_term = context_data.get('short_term_memory', [])
                if short_term:
                    context_prompt = self._build_short_context(short_term[-2:])  # ìµœê·¼ 2ê°œë§Œ
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            if context_prompt:
                final_prompt = f"""{context_prompt}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {message}

ìœ„ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""
                
                logger.debug_performance("ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ", {"tokens": total_tokens})
                return final_prompt
            else:
                return message
                
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return message
    
    def _build_short_context(self, qa_pairs: List[Dict]) -> str:
        """ë‹¨ì¶•ëœ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not qa_pairs:
            return ""
        
        context_parts = ["[ìµœê·¼ ëŒ€í™”]"]
        for pair in qa_pairs:
            context_parts.append(f"ì‚¬ìš©ì: {pair['question']}")
            if pair['answer']:
                context_parts.append(f"AI: {pair['answer']}")
        
        return "\n".join(context_parts)

    async def stream_response(
        self,
        model_name: str,
        prompt: str,
        include_datetime: bool = True,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            prompt: í”„ë¡¬í”„íŠ¸
            include_datetime: ë‚ ì§œ/ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Yields:
            ì‘ë‹µ í…ìŠ¤íŠ¸ ì²­í¬
        """
        # ë‚ ì§œ/ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        final_prompt = self._add_datetime_context(prompt) if include_datetime else prompt
        
        # Mock ëª¨ë“œ í™•ì¸
        if self.is_mock_mode():
            logger.debug_streaming("Mock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±", {"model": model_name})
            mock_model_name = f"mock-{model_name.lower()}"
            async for chunk in mock_llm.stream_response(final_prompt, mock_model_name):
                yield chunk
            return
        
        model = self.get_model(model_name)
        if model is None:
            # Mockìœ¼ë¡œ fallback
            async for chunk in mock_llm.stream_response(final_prompt, f"mock-{model_name}-unavailable"):
                yield chunk
            return
        
        try:
            # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° (LangChain ëª¨ë¸ì´ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ëŠ” ê²½ìš°)
            if hasattr(model, 'astream'):
                async for chunk in model.astream(final_prompt):
                    if hasattr(chunk, 'content'):
                        yield chunk.content
                    else:
                        yield str(chunk)
            else:
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¼ë°˜ ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì „ì†¡
                response = await model.ainvoke(final_prompt)
                content = response.content
                
                # ë¬¸ì¥ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜ (ì¤„ë°”ê¿ˆ í¬í•¨)
                import asyncio
                import re
                
                logger.debug_streaming("ì‹¤ì œ LLM ì‘ë‹µ ì²­í¬ ë¶„í• ", {"length": len(content)})
                
                # ë¬¸ì¥ê³¼ ì¤„ë°”ê¿ˆì„ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ë¶„í• 
                chunks = []
                lines = content.split('\n')
                
                for line in lines:
                    if line.strip():
                        # ê¸´ ì¤„ì€ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• 
                        sentences = re.split(r'([.!?]\s+)', line)
                        current_chunk = ""
                        
                        for sentence in sentences:
                            current_chunk += sentence
                            if len(current_chunk) > 50 or sentence.endswith(('.', '!', '?')):
                                if current_chunk.strip():
                                    chunks.append(current_chunk)
                                    current_chunk = ""
                        
                        if current_chunk.strip():
                            chunks.append(current_chunk)
                    else:
                        # ë¹ˆ ì¤„ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì¶”ê°€
                        chunks.append('\n')
                
                # ì²­í¬ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                for i, chunk in enumerate(chunks):
                    if i < len(chunks) - 1 and not chunk.endswith('\n'):
                        chunk += '\n'  # ì¤„ë°”ê¿ˆ ì¶”ê°€
                    
                    logger.debug(f"ğŸ“¤ ì²­í¬ ì „ì†¡: {repr(chunk[:30])}")
                    yield chunk
                    await asyncio.sleep(0.05)  # ìŠ¤íŠ¸ë¦¬ë° ë”œë ˆì´
                    
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # Mock ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ fallback
            async for chunk in mock_llm.stream_response(
                final_prompt, 
                f"mock-{model_name}-error-fallback"
            ):
                yield chunk


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
llm_router = LLMRouter()