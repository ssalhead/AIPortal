"""
LangGraph ì—ì´ì „íŠ¸ ì¢…í•© ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë° ì•ˆì •ì„± ê²€ì¦ ì‹œìŠ¤í…œ
"""

import asyncio
import time
import random
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics
import traceback

from app.agents.base import AgentInput
from app.agents.langgraph.information_gap_langgraph import LangGraphInformationGapAnalyzer
from app.agents.langgraph.supervisor_langgraph import LangGraphSupervisorAgent
from app.agents.langgraph.multimodal_rag_langgraph import LangGraphMultimodalRAGAgent
from app.services.langgraph_monitor import langgraph_monitor
from app.services.performance_optimizer import performance_optimizer
from app.utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class StressTestResult:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    agent_name: str
    test_type: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    throughput: float  # requests per second
    error_rate: float
    memory_usage_peak: float
    errors: List[str]
    test_duration: float
    timestamp: datetime


@dataclass
class TestScenario:
    """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤"""
    name: str
    description: str
    agent_class: Any
    test_queries: List[str]
    concurrent_users: int
    total_requests: int
    request_interval: float  # seconds
    timeout: float


class LangGraphStressTester:
    """LangGraph ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.test_results: List[StressTestResult] = []
        self.agents = self._initialize_agents()
        self.test_scenarios = self._create_test_scenarios()
        
        logger.info("LangGraph ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_agents(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        return {
            "information_gap": LangGraphInformationGapAnalyzer(),
            "supervisor": LangGraphSupervisorAgent(),
            "multimodal_rag": LangGraphMultimodalRAGAgent()
        }
    
    def _create_test_scenarios(self) -> List[TestScenario]:
        """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        
        # ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        simple_queries = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
            "íŒŒì´ì¬ì´ ë­”ê°€ìš”?",
            "AIê°€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê°„ë‹¨í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤"
        ]
        
        complex_queries = [
            "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ê³ , ê°ê°ì˜ ì¥ë‹¨ì ê³¼ ì‹¤ì œ ì‘ìš© ì‚¬ë¡€ë¥¼ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì˜ ì‘ë™ ì›ë¦¬ì™€ ì•”í˜¸í™”í, DeFi, NFT ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œì˜ í™œìš© ë°©ì•ˆì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "ê¸°í›„ ë³€í™”ê°€ ê¸€ë¡œë²Œ ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ê³ , íƒ„ì†Œ ì¤‘ë¦½ ë‹¬ì„±ì„ ìœ„í•œ ì •ì±…ê³¼ ê¸°ìˆ ì  í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
            "ì–‘ì ì»´í“¨íŒ…ì˜ í˜„ì¬ ê¸°ìˆ  ìˆ˜ì¤€ê³¼ í–¥í›„ ë°œì „ ì „ë§, ê·¸ë¦¬ê³  ê¸°ì¡´ ì»´í“¨í„°ì™€ì˜ ì°¨ì´ì  ë° ë³´ì•ˆì— ë¯¸ì¹  ì˜í–¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ì¸ê³µì§€ëŠ¥ì˜ ìœ¤ë¦¬ì  ë¬¸ì œì ê³¼ í¸í–¥ì„± í•´ê²° ë°©ì•ˆ, ê·¸ë¦¬ê³  AI ê±°ë²„ë„ŒìŠ¤ ì²´ê³„ êµ¬ì¶•ì˜ í•„ìš”ì„±ì— ëŒ€í•´ ë…¼ì˜í•´ì£¼ì„¸ìš”."
        ]
        
        edge_case_queries = [
            "ğŸš€ğŸŒŸğŸ’¡" * 100,  # íŠ¹ìˆ˜ë¬¸ì ë°˜ë³µ
            "A" * 1000,       # ê¸´ ë‹¨ì¼ ë¬¸ì
            "",               # ë¹ˆ ë¬¸ìì—´
            "í•œê¸€ê³¼ Englishì™€ æ—¥æœ¬èªë¥¼ æ··åœ¨ì‹œí‚¨ multilingual query ã§ã™",
            "SELECT * FROM users WHERE password = ''; DROP TABLE users; --"  # SQL Injection ì‹œë„
        ]
        
        return [
            TestScenario(
                name="light_load",
                description="ê°€ë²¼ìš´ ë¶€í•˜ í…ŒìŠ¤íŠ¸ - ê°„ë‹¨í•œ ì¿¼ë¦¬",
                agent_class=LangGraphInformationGapAnalyzer,
                test_queries=simple_queries,
                concurrent_users=5,
                total_requests=50,
                request_interval=1.0,
                timeout=30.0
            ),
            TestScenario(
                name="moderate_load",
                description="ì¤‘ê°„ ë¶€í•˜ í…ŒìŠ¤íŠ¸ - ë³µí•© ì¿¼ë¦¬",
                agent_class=LangGraphSupervisorAgent,
                test_queries=complex_queries,
                concurrent_users=10,
                total_requests=100,
                request_interval=0.5,
                timeout=60.0
            ),
            TestScenario(
                name="heavy_load",
                description="ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸ - ëŒ€ëŸ‰ ë™ì‹œ ìš”ì²­",
                agent_class=LangGraphMultimodalRAGAgent,
                test_queries=simple_queries + complex_queries,
                concurrent_users=20,
                total_requests=200,
                request_interval=0.1,
                timeout=90.0
            ),
            TestScenario(
                name="edge_cases",
                description="ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ - ì˜ˆì™¸ ìƒí™©",
                agent_class=LangGraphInformationGapAnalyzer,
                test_queries=edge_case_queries,
                concurrent_users=5,
                total_requests=25,
                request_interval=2.0,
                timeout=45.0
            ),
            TestScenario(
                name="sustained_load",
                description="ì§€ì† ë¶€í•˜ í…ŒìŠ¤íŠ¸ - ì¥ì‹œê°„ ì‹¤í–‰",
                agent_class=LangGraphSupervisorAgent,
                test_queries=simple_queries + complex_queries,
                concurrent_users=8,
                total_requests=500,
                request_interval=0.2,
                timeout=120.0
            )
        ]
    
    async def run_comprehensive_stress_test(self) -> Dict[str, Any]:
        """ì¢…í•© ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ LangGraph ì¢…í•© ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        start_time = time.time()
        all_results = []
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        await performance_optimizer.start_monitoring()
        
        try:
            for scenario in self.test_scenarios:
                logger.info(f"ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰: {scenario.name} - {scenario.description}")
                
                result = await self._run_scenario(scenario)
                all_results.append(result)
                
                # ì‹œë‚˜ë¦¬ì˜¤ ê°„ ì¿¨ë‹¤ìš´
                await asyncio.sleep(5)
            
            # ì „ì²´ í…ŒìŠ¤íŠ¸ ë¶„ì„
            total_duration = time.time() - start_time
            analysis = self._analyze_test_results(all_results, total_duration)
            
            logger.info(f"âœ… ì¢…í•© ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ({total_duration:.2f}ì´ˆ)")
            
            return {
                "test_summary": {
                    "total_scenarios": len(self.test_scenarios),
                    "total_duration": total_duration,
                    "timestamp": datetime.now().isoformat()
                },
                "scenario_results": [asdict(result) for result in all_results],
                "analysis": analysis,
                "system_performance": await self._get_system_performance_summary()
            }
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "partial_results": [asdict(result) for result in all_results]
            }
        
        finally:
            await performance_optimizer.stop_monitoring()
    
    async def _run_scenario(self, scenario: TestScenario) -> StressTestResult:
        """ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
        start_time = time.time()
        
        # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        agent = scenario.agent_class()
        
        # ê²°ê³¼ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸
        response_times = []
        errors = []
        successful_requests = 0
        failed_requests = 0
        
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        semaphore = asyncio.Semaphore(scenario.concurrent_users)
        
        async def execute_request(query: str, request_id: int):
            async with semaphore:
                try:
                    request_start = time.time()
                    
                    # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                    test_input = AgentInput(
                        query=query,
                        user_id=f"stress_test_user_{request_id}",
                        session_id=f"stress_test_session_{request_id}",
                        context={"test_mode": True, "stress_test": True}
                    )
                    
                    # ì—ì´ì „íŠ¸ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì ìš©)
                    result = await asyncio.wait_for(
                        agent.execute(test_input),
                        timeout=scenario.timeout
                    )
                    
                    request_time = time.time() - request_start
                    response_times.append(request_time)
                    
                    return True, request_time, None
                    
                except asyncio.TimeoutError:
                    error_msg = f"Timeout after {scenario.timeout}s"
                    errors.append(error_msg)
                    return False, scenario.timeout, error_msg
                    
                except Exception as e:
                    error_msg = f"Request {request_id}: {str(e)}"
                    errors.append(error_msg)
                    return False, time.time() - request_start, error_msg
        
        # ëª¨ë“  ìš”ì²­ ì‹¤í–‰
        tasks = []
        for i in range(scenario.total_requests):
            query = random.choice(scenario.test_queries)
            task = execute_request(query, i)
            tasks.append(task)
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            if scenario.request_interval > 0:
                await asyncio.sleep(scenario.request_interval)
        
        # ëª¨ë“  ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì§‘ê³„
        for result in results:
            if isinstance(result, Exception):
                failed_requests += 1
                errors.append(str(result))
            else:
                success, response_time, error = result
                if success:
                    successful_requests += 1
                else:
                    failed_requests += 1
                    if error:
                        errors.append(error)
        
        test_duration = time.time() - start_time
        
        # í†µê³„ ê³„ì‚°
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = 0
        
        throughput = successful_requests / test_duration if test_duration > 0 else 0
        error_rate = failed_requests / scenario.total_requests if scenario.total_requests > 0 else 0
        
        return StressTestResult(
            agent_name=agent.__class__.__name__,
            test_type=scenario.name,
            total_requests=scenario.total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            throughput=throughput,
            error_rate=error_rate,
            memory_usage_peak=0,  # TODO: ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            errors=errors[:10],  # ìƒìœ„ 10ê°œ ì—ëŸ¬ë§Œ ì €ì¥
            test_duration=test_duration,
            timestamp=datetime.now()
        )
    
    def _analyze_test_results(self, results: List[StressTestResult], total_duration: float) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        
        if not results:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì „ì²´ í†µê³„
        total_requests = sum(r.total_requests for r in results)
        total_successful = sum(r.successful_requests for r in results)
        total_failed = sum(r.failed_requests for r in results)
        
        overall_success_rate = total_successful / total_requests * 100 if total_requests > 0 else 0
        overall_error_rate = total_failed / total_requests * 100 if total_requests > 0 else 0
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        avg_response_times = [r.avg_response_time for r in results if r.avg_response_time > 0]
        overall_avg_response_time = statistics.mean(avg_response_times) if avg_response_times else 0
        
        throughputs = [r.throughput for r in results if r.throughput > 0]
        overall_throughput = statistics.mean(throughputs) if throughputs else 0
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ìˆœìœ„
        scenario_rankings = sorted(results, key=lambda x: (x.error_rate, x.avg_response_time))
        
        # ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤ ì‹ë³„
        problematic_scenarios = [
            r for r in results 
            if r.error_rate > 0.1 or r.avg_response_time > 30
        ]
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        if overall_error_rate < 1 and overall_avg_response_time < 5:
            performance_grade = "excellent"
        elif overall_error_rate < 5 and overall_avg_response_time < 10:
            performance_grade = "good"
        elif overall_error_rate < 10 and overall_avg_response_time < 20:
            performance_grade = "moderate"
        elif overall_error_rate < 20 and overall_avg_response_time < 30:
            performance_grade = "poor"
        else:
            performance_grade = "critical"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_performance_recommendations(results, performance_grade)
        
        return {
            "overall_statistics": {
                "total_requests": total_requests,
                "successful_requests": total_successful,
                "failed_requests": total_failed,
                "success_rate": round(overall_success_rate, 2),
                "error_rate": round(overall_error_rate, 2),
                "avg_response_time": round(overall_avg_response_time, 3),
                "throughput": round(overall_throughput, 2),
                "test_duration": round(total_duration, 2)
            },
            "performance_grade": performance_grade,
            "best_performing_scenario": asdict(scenario_rankings[0]) if scenario_rankings else None,
            "worst_performing_scenario": asdict(scenario_rankings[-1]) if scenario_rankings else None,
            "problematic_scenarios": [asdict(s) for s in problematic_scenarios],
            "recommendations": recommendations,
            "detailed_analysis": {
                "response_time_distribution": {
                    "min": min([r.min_response_time for r in results]) if results else 0,
                    "max": max([r.max_response_time for r in results]) if results else 0,
                    "avg": overall_avg_response_time,
                    "median": statistics.median(avg_response_times) if avg_response_times else 0
                },
                "error_patterns": self._analyze_error_patterns(results),
                "throughput_analysis": {
                    "peak_throughput": max(throughputs) if throughputs else 0,
                    "avg_throughput": overall_throughput,
                    "min_throughput": min(throughputs) if throughputs else 0
                }
            }
        }
    
    def _analyze_error_patterns(self, results: List[StressTestResult]) -> Dict[str, Any]:
        """ì—ëŸ¬ íŒ¨í„´ ë¶„ì„"""
        all_errors = []
        for result in results:
            all_errors.extend(result.errors)
        
        if not all_errors:
            return {"message": "ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        # ì—ëŸ¬ ìœ í˜•ë³„ ì§‘ê³„
        error_types = {}
        for error in all_errors:
            error_type = error.split(':')[0] if ':' in error else error
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬ ì‹ë³„
        most_common_error = max(error_types.items(), key=lambda x: x[1])
        
        return {
            "total_unique_errors": len(error_types),
            "most_common_error": {
                "type": most_common_error[0],
                "count": most_common_error[1]
            },
            "error_frequency": error_types,
            "sample_errors": all_errors[:5]  # ìƒ˜í”Œ ì—ëŸ¬ 5ê°œ
        }
    
    def _generate_performance_recommendations(self, results: List[StressTestResult], grade: str) -> List[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if grade == "critical":
            recommendations.extend([
                "ì‹œìŠ¤í…œ ì„±ëŠ¥ì´ ì‹¬ê°í•œ ìƒíƒœì…ë‹ˆë‹¤. ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "ì—ëŸ¬ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.",
                "ì‘ë‹µ ì‹œê°„ì´ ê³¼ë„í•˜ê²Œ ê¹ë‹ˆë‹¤. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            ])
        elif grade == "poor":
            recommendations.extend([
                "ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê°œì„ í•˜ì„¸ìš”.",
                "ì—ëŸ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ ê°•í™”í•˜ì„¸ìš”."
            ])
        elif grade == "moderate":
            recommendations.extend([
                "ì¼ë¶€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì„±ëŠ¥ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.",
                "ì‘ë‹µ ì‹œê°„ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            ])
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¸ë¶€ ê¶Œì¥ì‚¬í•­
        high_error_scenarios = [r for r in results if r.error_rate > 0.05]
        if high_error_scenarios:
            recommendations.append(f"ì—ëŸ¬ìœ¨ì´ ë†’ì€ ì‹œë‚˜ë¦¬ì˜¤: {', '.join([s.test_type for s in high_error_scenarios])}")
        
        slow_scenarios = [r for r in results if r.avg_response_time > 10]
        if slow_scenarios:
            recommendations.append(f"ì‘ë‹µ ì‹œê°„ì´ ëŠë¦° ì‹œë‚˜ë¦¬ì˜¤: {', '.join([s.test_type for s in slow_scenarios])}")
        
        return recommendations
    
    async def _get_system_performance_summary(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        try:
            performance_report = performance_optimizer.get_performance_report()
            monitoring_metrics = await langgraph_monitor.get_realtime_metrics()
            
            return {
                "performance_score": performance_report.get("performance_score", 0),
                "performance_level": performance_report.get("performance_level", "unknown"),
                "monitoring_summary": {
                    "total_executions": monitoring_metrics.get("summary", {}).get("total_executions", 0),
                    "langgraph_adoption_rate": monitoring_metrics.get("summary", {}).get("langgraph_adoption_rate", 0)
                },
                "optimization_status": {
                    "total_optimizations": len(performance_optimizer.optimization_history),
                    "recent_optimizations": performance_optimizer.optimization_history[-3:] if performance_optimizer.optimization_history else []
                }
            }
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def run_single_agent_test(self, agent_name: str, test_type: str = "moderate_load") -> Dict[str, Any]:
        """ë‹¨ì¼ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
        if agent_name not in self.agents:
            return {"error": f"Agent '{agent_name}' not found"}
        
        scenario = next((s for s in self.test_scenarios if s.name == test_type), None)
        if not scenario:
            return {"error": f"Test type '{test_type}' not found"}
        
        # ì‹œë‚˜ë¦¬ì˜¤ì˜ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ë¥¼ ì§€ì •ëœ ì—ì´ì „íŠ¸ë¡œ ë³€ê²½
        scenario.agent_class = self.agents[agent_name].__class__
        
        result = await self._run_scenario(scenario)
        
        return {
            "agent_name": agent_name,
            "test_result": asdict(result),
            "analysis": self._analyze_test_results([result], result.test_duration)
        }


# ì „ì—­ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤í„° ì¸ìŠ¤í„´ìŠ¤
stress_tester = LangGraphStressTester()


async def run_quick_health_check() -> Dict[str, Any]:
    """ë¹ ë¥¸ ìƒíƒœ ì ê²€"""
    logger.info("ğŸ¥ LangGraph ë¹ ë¥¸ ìƒíƒœ ì ê²€ ì‹œì‘")
    
    try:
        # ê° ì—ì´ì „íŠ¸ë³„ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        agents = {
            "information_gap": LangGraphInformationGapAnalyzer(),
            "supervisor": LangGraphSupervisorAgent(),
            "multimodal_rag": LangGraphMultimodalRAGAgent()
        }
        
        health_results = {}
        
        for agent_name, agent in agents.items():
            try:
                start_time = time.time()
                
                test_input = AgentInput(
                    query="ì•ˆë…•í•˜ì„¸ìš”, ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                    user_id="health_check_user",
                    context={"health_check": True}
                )
                
                result = await asyncio.wait_for(agent.execute(test_input), timeout=30)
                
                response_time = time.time() - start_time
                
                health_results[agent_name] = {
                    "status": "healthy",
                    "response_time": round(response_time, 3),
                    "result_length": len(str(result.result)) if result.result else 0
                }
                
            except Exception as e:
                health_results[agent_name] = {
                    "status": "unhealthy",
                    "error": str(e),
                    "response_time": None
                }
        
        # ì „ì²´ ê±´ê°•ë„ í‰ê°€
        healthy_agents = [name for name, status in health_results.items() if status["status"] == "healthy"]
        overall_health = "healthy" if len(healthy_agents) == len(agents) else "degraded" if healthy_agents else "critical"
        
        return {
            "overall_health": overall_health,
            "healthy_agents": len(healthy_agents),
            "total_agents": len(agents),
            "agent_results": health_results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ ìƒíƒœ ì ê²€ ì‹¤íŒ¨: {e}")
        return {
            "overall_health": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }